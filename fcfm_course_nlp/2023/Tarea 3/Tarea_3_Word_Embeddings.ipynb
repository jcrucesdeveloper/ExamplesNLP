{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ckbt7VPDhBwb"},"source":["# **Tarea 3 - Word Embeddings üìö**\n","\n","**Integrantes:**\n","\n","**Fecha l√≠mite de entrega üìÜ:** 16 de mayo.\n","\n","**Tiempo estimado de dedicaci√≥n:**"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-19T18:30:18.109327Z","start_time":"2020-03-19T18:30:18.103344Z"},"id":"q5CSRY4oNCHK"},"source":["\n","**Instrucciones:**\n","- El ejercicio consiste en:\n","    - Responder preguntas relativas a los contenidos vistos en los v√≠deos y slides de las clases.\n","    - Implementar el m√©todo de la Word Context Matrix. \n","    - Entrenar Word2Vec y FastText sobre un peque√±o corpus.\n","    - Evaluar los embeddings obtenidos en una tarea de clasificaci√≥n.\n","- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n","- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n","- El formato de entrega es este mismo **Jupyter Notebook**.\n","- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n. \n","\n","\n","**Referencias**\n","\n","V√≠deos: \n","\n","- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n","- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n","- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"]},{"cell_type":"markdown","metadata":{"id":"G4wYf0vgnbTv"},"source":["## **Preguntas te√≥ricas üìï (3 puntos).** ##\n","Para estas preguntas no es necesario implementar c√≥digo, pero pueden utilizar pseudo c√≥digo."]},{"cell_type":"markdown","metadata":{"id":"B5hUG6-8ngoK"},"source":["### **Parte 1: Modelos Lineales (1.5 ptos)**"]},{"cell_type":"markdown","metadata":{"id":"5yRvZbhsoi8f"},"source":["Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categor√≠as: pol√≠tica, deporte, negocios y otros. "]},{"cell_type":"markdown","metadata":{"id":"irsqBVmCnx3M"},"source":["**Pregunta 1**: Dise√±e un modelo lineal capaz de clasificar un documento seg√∫n estas categor√≠as donde el output sea un vector con una distribuci√≥n de probabilidad con la pertenencia a cada clase. \n","\n","Especifique: representaci√≥n de los documentos de entrada, par√°metros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y funci√≥n de p√©rdida escogida. **(0.75 puntos)**\n","\n","**Respuesta**: \n"]},{"cell_type":"markdown","metadata":{"id":"G5FaWqBVvL90"},"source":["**Pregunta 2**: Explique c√≥mo funciona el proceso de entrenamiento en este tipo de modelos y su evaluaci√≥n. **(0.75 puntos)**\n","\n","**Respuesta**: "]},{"cell_type":"markdown","metadata":{"id":"XkK7pc54njZq"},"source":["### **Parte 2: Redes Neuronales (1.5 ptos)** "]},{"cell_type":"markdown","metadata":{"id":"VUbJjlj_9AFC"},"source":["Supongamos que tenemos la siguiente red neuronal."]},{"cell_type":"markdown","metadata":{"id":"obUfuOYB_TOC"},"source":["![image.png](https://drive.google.com/uc?export=view&id=1nV1G0dOeVGPn40qGcGF9l_pVEFNtLU-w)"]},{"cell_type":"markdown","metadata":{"id":"s2z-8zKW0_6q"},"source":["**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matem√°tica. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en funci√≥n del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n","\n","Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.75 Puntos)**\n","\n","**Respuesta**: \n","\n","Formula:\n","$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) =$\n","\n","Dimensiones: \n","\n","**Pregunta 2**: Explique qu√© es backpropagation. ¬øCuales ser√≠an los par√°metros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n","\n","**Respuesta**:\n","\n","**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.5 puntos)**\n","\n","**Respuesta**:"]},{"cell_type":"markdown","metadata":{"id":"ocS_vQhR1gcU"},"source":["## **Preguntas pr√°cticas üíª (3 puntos).** ##"]},{"cell_type":"markdown","source":["### Parte 3 A (1 Punto): Word Contex Matrix"],"metadata":{"id":"D0wk5GBkSE73"}},{"cell_type":"markdown","source":["\n","\n","En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template (para esta parte puede utilizar las librer√≠as ```numpy``` y/o ```scipy```). Hint: revise como utilizar matrices sparse de ```scipy```\n","\n","```python\n","class WordContextMatrix:\n","\n","  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n","    \"\"\"\n","    Utilice el constructor para definir los parametros.\n","    \"\"\"\n","\n","    # se sugiere agregar un una estructura de datos para guardar las\n","    # palabras del vocab y para guardar el conteo de coocurrencia\n","    # si lo necesita puede agregar m√°s parametros pero no puede cambiar el resto\n","    ...\n","    \n","  def build_vocab(self):\n","    \"\"\"\n","    Utilice este m√©todo para construir el vocabulario\n","    \"\"\"\n","    \n","\n","    # Le puede ser √∫til considerar un token unk al vocab\n","    # para palabras fuera del vocab\n","    ...\n","  \n","  def build_matrix(self):\n","    \"\"\"\n","    Utilice este m√©todo para crear la palabra contexto\n","    \"\"\"\n","    ...\n","\n","  def get_matrix(self):\n","    \"\"\"\n","    Utilice este m√©todo para obtener la matriz palabra contexto. \n","    \"\"\"\n","\n","    # se recomienda transformar la matrix a un diccionario de embedding.\n","    # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n","    ...\n","\n","```\n","\n","puede modificar los par√°metros o m√©todos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n","\n","```python\n","corpus = [\n","  \"I like deep learning.\",\n","  \"I like NLP.\",\n","  \"I enjoy flying.\"\n","]\n","```\n","\n","Obteniendo una matriz parecia a esta:\n","\n","***Resultado esperado***: \n","\n","| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n","|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n","| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n","| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n","| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n","| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n","| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n","| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n","| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n","| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n","\n","``\n","\n","Verifique si su matrix es igual a esta utilizando el corpus de ejemplo. Ojo que este es s√≥lo un ejemplo, su algoritmo debe **generalizar** a otros ejemplos."],"metadata":{"id":"e_mh12Z9SF-J"}},{"cell_type":"markdown","metadata":{"id":"Ol82nJ0FnmcP"},"source":["### **Parte 3 B (1 Punto): Word Embeddings**"]},{"cell_type":"markdown","metadata":{"id":"OgmeSFqKLpFL"},"source":["En la auxiliar 2 aprendieron como entrenar Word2Vec utilizando gensim. El objetivo de esta parte es comparar los embeddings obtenidos con dos modelos diferentes: Word2Vec y [FastText](https://radimrehurek.com/gensim/models/fasttext.html) (utilizen size=200 en FastText) entrenados en el mismo dataset de di√°logos de los Simpson. "]},{"cell_type":"code","metadata":{"id":"ecCvnryeQiG7"},"source":["import re  \n","import pandas as pd \n","from time import time  \n","from collections import defaultdict \n","import string \n","import multiprocessing\n","import os\n","import gensim\n","import sklearn\n","from sklearn import linear_model\n","from collections import Counter\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n","\n","# word2vec\n","from gensim.models import Word2Vec, KeyedVectors, FastText\n","from gensim.models.phrases import Phrases, Phraser\n","from sklearn.model_selection import train_test_split\n","import logging\n","\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","logger = logging.getLogger(__name__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tZgN06q4QPi3"},"source":["Utilizando el dataset adjunto con la tarea:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eY3kmg4onnsu","outputId":"d3525a54-0c10-401e-b3e2-9c6e9e714a2c"},"source":["data_file = \"dialogue-lines-of-the-simpsons.zip\"\n","df = pd.read_csv(data_file)\n","stopwords = pd.read_csv(\n","    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",").values\n","stopwords = Counter(stopwords.flatten().tolist())\n","df = df.dropna().reset_index(drop=True) # Quitar filas vacias"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-05-05 17:58:59,568 : INFO : NumExpr defaulting to 2 threads.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"VAg5a5bmWk3T"},"source":["**Pregunta 1**: Ayud√°ndose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec y FastText sobre el dataset anterior. **(1 punto)** (Hint, le puede servir explorar un poco los datos)"]},{"cell_type":"markdown","metadata":{"id":"MWw2fXFRXe5Y"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"Bvwplz7yTNcr"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Lr8U5wOTNcr"},"source":["**Pregunta 2**: Encuentre las palabras mas similares a las siguientes: Lisa, Bart, Homer, Marge. C√∫al es la diferencia entre ambos resultados? Por qu√© ocurre esto? Intente comparar ahora Liisa en ambos modelos (doble i). Cuando escoger√≠a uno vs el otro? **(0.5 puntos)**"]},{"cell_type":"markdown","metadata":{"id":"yMLyGffVTNcs"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"w6RvJGpbTNcs"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IRCB-jqgTNcs"},"source":["### **Parte 4 (1 Punto): Aplicar embeddings para clasificar**"]},{"cell_type":"markdown","metadata":{"id":"zlqzlJRSTNcs"},"source":["Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n","\n","Para esto ocuparemos el lexic√≥n AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotaci√≥n es positiva y un -1 si es negativa."]},{"cell_type":"code","metadata":{"id":"CMskFDmHTNcs"},"source":["AFINN = 'AFINN_full.csv'\n","df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uaKl8hsCTNcs"},"source":["Hint: Para w2v son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendr√°n una representaci√≥n en AFINN. Pueden utilizar esta funci√≥n auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."]},{"cell_type":"code","metadata":{"id":"tWSSuctiTNcs"},"source":["def try_apply(model,word):\n","    try:\n","        aux = model[word]\n","        return True\n","    except KeyError:\n","        #logger.error('Word {} not in dictionary'.format(word))\n","        return False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LrVPeEzgTNcs"},"source":["**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representaci√≥n en embedding que acabamos de calcular (con ambos modelos). \n","\n","Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n","\n","Para ambos modelos, separar train y test de acuerdo a la siguiente funci√≥n. **(0.75 puntos)**"]},{"cell_type":"code","metadata":{"id":"0Bkt26BwTNcs"},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iDcq5czXTNct"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"upAn_eT4TNct"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kDKe4gA3TNct"},"source":["**Pregunta 2**: Entrenar una regresi√≥n log√≠stica (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qu√© se obtienen estos resultados? C√≥mo los mejorar√≠as? **(0.75 puntos)**"]},{"cell_type":"markdown","metadata":{"id":"hJMzq_dETNct"},"source":["**Respuesta**:"]},{"cell_type":"markdown","metadata":{"id":"izppruGQTNct"},"source":["# Bonus: +0.25 puntos en cualquier pregunta"]},{"cell_type":"markdown","metadata":{"id":"YW0aeK2KTNct"},"source":["**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset m√°s grande y obtener mejores resultados. Les puede servir [√©sta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."]},{"cell_type":"markdown","metadata":{"id":"qvHcVS3sTNct"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"MSc8p-T8TNcu"},"source":[],"execution_count":null,"outputs":[]}]}