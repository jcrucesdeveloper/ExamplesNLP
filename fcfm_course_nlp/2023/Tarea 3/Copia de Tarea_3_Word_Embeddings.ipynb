{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ckbt7VPDhBwb"},"source":["# **Pauta Tarea 3 - Word Embeddings üìö**\n","\n","**Integrantes:**\n","\n","**Fecha l√≠mite de entrega üìÜ:** 16 de mayo.\n","\n","**Tiempo estimado de dedicaci√≥n:**"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-19T18:30:18.109327Z","start_time":"2020-03-19T18:30:18.103344Z"},"id":"q5CSRY4oNCHK"},"source":["\n","**Instrucciones:**\n","- El ejercicio consiste en:\n","    - Responder preguntas relativas a los contenidos vistos en los v√≠deos y slides de las clases.\n","    - Implementar el m√©todo de la Word Context Matrix. \n","    - Entrenar Word2Vec y FastText sobre un peque√±o corpus.\n","    - Evaluar los embeddings obtenidos en una tarea de clasificaci√≥n.\n","- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n","- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n","- El formato de entrega es este mismo **Jupyter Notebook**.\n","- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n. \n","\n","\n","**Referencias**\n","\n","V√≠deos: \n","\n","- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n","- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n","- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"]},{"cell_type":"markdown","metadata":{"id":"G4wYf0vgnbTv"},"source":["## **Preguntas te√≥ricas üìï (3 puntos).** ##\n","Para estas preguntas no es necesario implementar c√≥digo, pero pueden utilizar pseudo c√≥digo."]},{"cell_type":"markdown","metadata":{"id":"B5hUG6-8ngoK"},"source":["### **Parte 1: Modelos Lineales (1.5 ptos)**"]},{"cell_type":"markdown","metadata":{"id":"5yRvZbhsoi8f"},"source":["Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categor√≠as: pol√≠tica, deporte, negocios y otros. "]},{"cell_type":"markdown","metadata":{"id":"irsqBVmCnx3M"},"source":["**Pregunta 1**: Dise√±e un modelo lineal capaz de clasificar un documento seg√∫n estas categor√≠as donde el output sea un vector con una distribuci√≥n de probabilidad con la pertenencia a cada clase. \n","\n","Especifique: representaci√≥n de los documentos de entrada, par√°metros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y funci√≥n de p√©rdida escogida. **(0.75 puntos)**\n","\n","**Respuesta**: Representaci√≥n escogida del documento de entrada: Bag of words por ejemplo\n","\n","Par√°metros del modelo: Matriz de pesos W en donde la columna 1 tengan mayor importancia las palabras de pol√≠tica, columna 2 sobre deporte, columna 3 sobre negocios y columna 4 el resto\n","\n","Transformaciones necesarias: Aplicar softmax al vector de output del modelo.\n","\n","Funci√≥n de p√©rdida escogida: Cross-entropy es el ideal, pero pueden escoger cualquier funci√≥n de perdida multiclase que alcanze el m√≠nimo cuando las predicciones son correctas\n"]},{"cell_type":"markdown","metadata":{"id":"G5FaWqBVvL90"},"source":["**Pregunta 2**: Explique c√≥mo funciona el proceso de entrenamiento en este tipo de modelos y su evaluaci√≥n. **(0.75 puntos)**\n","\n","**Respuesta**: El objetivo del entrenamiento es minimizar la loss del modelo. Se van tuneando los par√°metros de la matriz W hasta encontrar uno que minimice la loss del modelo. La evaluaci√≥n se hace sobre datos que no se han observado para comprobar generalizaci√≥n del modelo."]},{"cell_type":"markdown","metadata":{"id":"XkK7pc54njZq"},"source":["### **Parte 2: Redes Neuronales (1.5 ptos)** "]},{"cell_type":"markdown","metadata":{"id":"VUbJjlj_9AFC"},"source":["Supongamos que tenemos la siguiente red neuronal."]},{"cell_type":"markdown","metadata":{"id":"obUfuOYB_TOC"},"source":["![image.png](https://drive.google.com/uc?export=view&id=1nV1G0dOeVGPn40qGcGF9l_pVEFNtLU-w)"]},{"cell_type":"markdown","metadata":{"id":"s2z-8zKW0_6q"},"source":["**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matem√°tica. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en funci√≥n del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n","\n","Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.75 Puntos)**\n","\n","\n","**Respuesta**: \n","\n","Formula:\n","$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x})= h(f(g(\\vec{x}W¬π+\\vec{b}¬π)W¬≤+\\vec{b}¬≤)W¬≥+\\vec{b}¬≥)W‚Å¥+\\vec{b}‚Å¥$ (1 punto)\n","\n","Dimensiones: (2 puntos)\n","- $\\vec{x}$: 3\n","- $W¬π$: 3x2\n","- $\\vec{b}¬π$: 2\n","- $W¬≤$: 2x3\n","- $\\vec{b}¬≤$: 3\n","- $W¬≥:$ 3x1\n","- $\\vec{b}¬≥:$ 1\n","- $W‚Å¥:$ 1x4\n","- $\\vec{b}‚Å¥:$ 4\n","\n","**Pregunta 2**: Explique qu√© es backpropagation. ¬øCuales ser√≠an los par√°metros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n","\n","**Respuesta**: \"Backpropagation es una t√©cnica eficiente para evaluar el gradiente de una loss function L en una red neuronal feed-forward con respecto a todos sus par√°metros\" (Cita de clases). Los par√°metros serian de $W¬π,\\vec{b}¬π$ a $W‚Å¥,\\vec{b}‚Å¥$\n","\n","**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.5 puntos)**\n","\n","**Respuesta**: Los 4 pasos son:\n","\n","- Aplicar el vector x y propagarlo por toda la red\n","- Evaluar $\\delta$ para todas las unidades ocultas\n","- Propagar los $\\delta$ desde el final al inicio de la red\n","- Ocupar la formula de $\\frac{\\partial L}{\\partial W}$\n","\n","En la red anterior se necesitan las derivadas: $f', g',h'$"]},{"cell_type":"markdown","metadata":{"id":"ocS_vQhR1gcU"},"source":["## **Preguntas pr√°cticas üíª (3 puntos).** ##"]},{"cell_type":"markdown","source":["### Parte 3 A (1 Punto): Word Contex Matrix"],"metadata":{"id":"D0wk5GBkSE73"}},{"cell_type":"markdown","source":["\n","\n","En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template (para esta parte puede utilizar las librer√≠as ```numpy``` y/o ```scipy```). Hint: revise como utilizar matrices sparse de ```scipy```\n","\n","```python\n","class WordContextMatrix:\n","\n","  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n","    \"\"\"\n","    Utilice el constructor para definir los parametros.\n","    \"\"\"\n","\n","    # se sugiere agregar un una estructura de datos para guardar las\n","    # palabras del vocab y para guardar el conteo de coocurrencia\n","    # si lo necesita puede agregar m√°s parametros pero no puede cambiar el resto\n","    ...\n","    \n","  def build_vocab(self):\n","    \"\"\"\n","    Utilice este m√©todo para construir el vocabulario\n","    \"\"\"\n","    \n","\n","    # Le puede ser √∫til considerar un token unk al vocab\n","    # para palabras fuera del vocab\n","    ...\n","  \n","  def build_matrix(self):\n","    \"\"\"\n","    Utilice este m√©todo para crear la palabra contexto\n","    \"\"\"\n","    ...\n","\n","  def get_matrix(self):\n","    \"\"\"\n","    Utilice este m√©todo para obtener la matriz palabra contexto. \n","    \"\"\"\n","\n","    # se recomienda transformar la matrix a un diccionario de embedding.\n","    # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n","    ...\n","\n","```\n","\n","puede modificar los par√°metros o m√©todos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n","\n","```python\n","corpus = [\n","  \"I like deep learning.\",\n","  \"I like NLP.\",\n","  \"I enjoy flying.\"\n","]\n","```\n","\n","Obteniendo una matriz parecia a esta:\n","\n","***Resultado esperado***: \n","\n","| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n","|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n","| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n","| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n","| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n","| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n","| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n","| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n","| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n","| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n","\n","``\n","\n","Verifique si su matrix es igual a esta utilizando el corpus de ejemplo. Ojo que este es s√≥lo un ejemplo, su algoritmo debe **generalizar** a otros ejemplos."],"metadata":{"id":"e_mh12Z9SF-J"}},{"cell_type":"code","source":["import scipy\n","import numpy as np\n","\n","\n","class WordContextMatrix:\n","\n","  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n","    self.vocab_size = vocab_size\n","    self.window_size = window_size\n","    self.tokenizer = tokenizer\n","    self.dataset = dataset\n","    self.word2index = {}\n","    self.size = 0\n","    self.coor_matrix = scipy.sparse.lil_matrix((vocab_size + 1, vocab_size))\n","\n","    self.add_word_to_vocab('unk')\n","    \n","  def add_word_to_vocab(self, word):\n","    if self.size < self.vocab_size and word not in self.word2index:\n","      self.word2index[word] = self.size\n","      self.size += 1\n","\n","  def build_matrix(self):\n","    for text in self.dataset:\n","      tokens = self.tokenizer(text)\n","      for token in tokens:\n","        self.add_word_to_vocab(token)\n","      for token in tokens:\n","        ind_word = tokens.index(token)\n","        contexts = get_contexts(ind_word, self.window_size, tokens)\n","        for context in contexts:\n","          if token in self.word2index and context in self.word2index:\n","            ind_focus_word = self.word2index[token]\n","            ind_cont_word = self.word2index[context]\n","            self.coor_matrix[ind_focus_word, ind_cont_word] += 1.0\n","          elif token not in self.word2index and context in self.word2index:\n","            ind_cont_word = self.word2index[context]\n","            self.coor_matrix[0, ind_cont_word] += 1.0\n","\n","  def matrix2dict(self):\n","    embeddings = {}\n","    for word, idx in self.word2index.items():\n","      embeddings[word] = self.coor_matrix[idx, :].toarray()[0]\n","    return embeddings\n","\n","def get_contexts(ind_word, w_size, tokens):\n","    slice_start = ind_word - w_size if (ind_word - w_size >= 0) else 0\n","    slice_end = len(tokens) if (ind_word + w_size + 1 >= len(tokens)) else ind_word + w_size + 1\n","    first_part = tokens[slice_start: ind_word]\n","    last_part = tokens[ind_word + 1: slice_end]\n","    contexts = tuple(first_part + last_part)\n","    return contexts"],"metadata":{"id":"KV2Bk1tW-FrT","executionInfo":{"status":"ok","timestamp":1685321505084,"user_tz":240,"elapsed":499,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ol82nJ0FnmcP"},"source":["### **Parte 3 B (1 Punto): Word Embeddings**"]},{"cell_type":"markdown","metadata":{"id":"OgmeSFqKLpFL"},"source":["En la auxiliar 2 aprendieron como entrenar Word2Vec utilizando gensim. El objetivo de esta parte es comparar los embeddings obtenidos con dos modelos diferentes: Word2Vec y [FastText](https://radimrehurek.com/gensim/models/fasttext.html) (utilizen size=200 en FastText) entrenados en el mismo dataset de di√°logos de los Simpson. "]},{"cell_type":"code","metadata":{"id":"ecCvnryeQiG7","executionInfo":{"status":"ok","timestamp":1685321507923,"user_tz":240,"elapsed":2841,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"source":["import re  \n","import pandas as pd \n","from time import time  \n","from collections import defaultdict \n","import string \n","import multiprocessing\n","import os\n","import gensim\n","import sklearn\n","from sklearn import linear_model\n","from collections import Counter\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n","\n","# word2vec\n","from gensim.models import Word2Vec, KeyedVectors, FastText\n","from gensim.models.phrases import Phrases, Phraser\n","from sklearn.model_selection import train_test_split\n","import logging\n","\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","logger = logging.getLogger(__name__)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tZgN06q4QPi3"},"source":["Utilizando el dataset adjunto con la tarea:"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TXjtTxq6_tI_","executionInfo":{"status":"ok","timestamp":1685321640784,"user_tz":240,"elapsed":31982,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"baf47805-f13b-4af9-dbc4-2a9cf9e81c1e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"eY3kmg4onnsu","executionInfo":{"status":"ok","timestamp":1685321855848,"user_tz":240,"elapsed":2150,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"source":["data_file = \"/content/drive/MyDrive/dialogue-lines-of-the-simpsons.zip\"#\"dialogue-lines-of-the-simpsons.zip\"\n","df = pd.read_csv(data_file)\n","stopwords = pd.read_csv(\n","    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",").values\n","stopwords = Counter(stopwords.flatten().tolist())\n","df = df.dropna().reset_index(drop=True) # Quitar filas vacias"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VAg5a5bmWk3T"},"source":["**Pregunta 1**: Ayud√°ndose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec y FastText sobre el dataset anterior. **(1 punto)** (Hint, le puede servir explorar un poco los datos)"]},{"cell_type":"markdown","metadata":{"id":"MWw2fXFRXe5Y"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"Bvwplz7yTNcr","executionInfo":{"status":"ok","timestamp":1685321860151,"user_tz":240,"elapsed":1522,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"source":["punctuation = string.punctuation + \"¬´¬ª‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî\"\n","def simple_tokenizer(doc, lower=False):\n","    if lower:\n","        tokenized_doc = doc.translate(str.maketrans(\n","            '', '', punctuation)).lower().split()\n","\n","    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n","\n","    tokenized_doc = [\n","        token for token in tokenized_doc if token.lower() not in stopwords\n","    ]\n","    return tokenized_doc\n","content = df['spoken_words']\n","cleaned_content = [simple_tokenizer(doc) for doc in content.values]"],"execution_count":6,"outputs":[]},{"cell_type":"code","source":["w2v_model = Word2Vec(min_count=10,\n","                      window=4,\n","                      vector_size=200,\n","                      sample=6e-5,\n","                      alpha=0.03,\n","                      min_alpha=0.0007,\n","                      negative=20,\n","                      workers=multiprocessing.cpu_count())\n","ft_model = FastText(vector_size=200, window=3, min_count=1)"],"metadata":{"id":"Y3NZDg4s_I6w","executionInfo":{"status":"ok","timestamp":1685321902808,"user_tz":240,"elapsed":3,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["w2v_model.build_vocab(sentences.values, progress_per=10000)\n","ft_model.build_vocab(sentences.values, progress_per=10000)"],"metadata":{"id":"flGDIhIB_MXk","executionInfo":{"status":"ok","timestamp":1685322465297,"user_tz":240,"elapsed":2,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["t = time()\n","w2v_model.train(content, total_examples=w2v_model.corpus_count, epochs=15, report_delay=10)\n","print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n","if not os.path.exists('./pretrained_models'):\n","    os.mkdir('./pretrained_models')\n","w2v_model.save('./pretrained_models/w2v.model')"],"metadata":{"id":"-rDW058d_PPA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685322007199,"user_tz":240,"elapsed":36017,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"cb464d6f-f6af-43d4-e3c9-a8b48d416bba"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Time to train the model: 0.6 mins\n"]}]},{"cell_type":"code","source":["t = time()\n","ft_model.train(content, total_examples=ft_model.corpus_count, epochs=15, report_delay=10)\n","print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n","if not os.path.exists('./pretrained_models'):\n","    os.mkdir('./pretrained_models')\n","ft_model.save('./pretrained_models/ft.model')"],"metadata":{"id":"yqdZvE_n_Px4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685322075036,"user_tz":240,"elapsed":67839,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"ce0cb40b-cb2c-4cd4-ed44-6e92dc33b091"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Time to train the model: 0.99 mins\n"]}]},{"cell_type":"markdown","metadata":{"id":"-Lr8U5wOTNcr"},"source":["**Pregunta 2**: Encuentre las palabras mas similares a las siguientes: Lisa, Bart, Homer, Marge. C√∫al es la diferencia entre ambos resultados? Por qu√© ocurre esto? Intente comparar ahora Liisa en ambos modelos (doble i). Cuando escoger√≠a uno vs el otro? **(0.5 puntos)**"]},{"cell_type":"markdown","metadata":{"id":"yMLyGffVTNcs"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"w6RvJGpbTNcs","executionInfo":{"status":"ok","timestamp":1685322477615,"user_tz":240,"elapsed":2,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"source":["w2v_model.wv.most_similar(positive=[\"Lisa\"])"],"execution_count":37,"outputs":[]},{"cell_type":"code","source":["ft_model.wv.most_similar(positive=[\"Lisa\"])"],"metadata":{"id":"hPhrSMlc_YbO","executionInfo":{"status":"aborted","timestamp":1685322075038,"user_tz":240,"elapsed":13,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v_model.wv.most_similar(positive=[\"Bart\"])"],"metadata":{"id":"Zv0cp3ZU_anM","executionInfo":{"status":"aborted","timestamp":1685322075039,"user_tz":240,"elapsed":14,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ft_model.wv.most_similar(positive=[\"Bart\"])"],"metadata":{"id":"79RD2uQN_avr","executionInfo":{"status":"aborted","timestamp":1685322075040,"user_tz":240,"elapsed":15,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v_model.wv.most_similar(positive=[\"Homer\"])"],"metadata":{"id":"JfjIMoAQ_a3D","executionInfo":{"status":"aborted","timestamp":1685322075040,"user_tz":240,"elapsed":15,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ft_model.wv.most_similar(positive=['Homer'])"],"metadata":{"id":"auQTZou3_a9i","executionInfo":{"status":"aborted","timestamp":1685322075042,"user_tz":240,"elapsed":16,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v_model.wv.most_similar(positive=[\"Marge\"])"],"metadata":{"id":"_nD4f6Z__bn3","executionInfo":{"status":"aborted","timestamp":1685322075042,"user_tz":240,"elapsed":16,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ft_model.wv.most_similar(positive=[\"Marge\"])"],"metadata":{"id":"Tn0r6ANJ_bt6","executionInfo":{"status":"aborted","timestamp":1685322075043,"user_tz":240,"elapsed":16,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v_model.wv.most_similar(positive=[\"Liisa\"]) # Es normal el siguiente error!"],"metadata":{"id":"_1AZgdCU_ctM","executionInfo":{"status":"aborted","timestamp":1685322075043,"user_tz":240,"elapsed":16,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ft_model.wv.most_similar(positive=[\"Liisa\"])"],"metadata":{"id":"VlDx6Kq6_rBZ","executionInfo":{"status":"aborted","timestamp":1685322075044,"user_tz":240,"elapsed":15,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["W2V encuentra las palabras del vocabulario que est√°n mas cerca mientras que FastText busca los ngramas. FastText funciona mejor que W2V cuando buscamos palabras que est√°n fuera del vocabulario.\n"],"metadata":{"id":"FSxwzRLJ_u_0"}},{"cell_type":"markdown","metadata":{"id":"IRCB-jqgTNcs"},"source":["### **Parte 4 (1 Punto): Aplicar embeddings para clasificar**"]},{"cell_type":"markdown","metadata":{"id":"zlqzlJRSTNcs"},"source":["Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n","\n","Para esto ocuparemos el lexic√≥n AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotaci√≥n es positiva y un -1 si es negativa."]},{"cell_type":"code","metadata":{"id":"CMskFDmHTNcs","executionInfo":{"status":"aborted","timestamp":1685322075044,"user_tz":240,"elapsed":15,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"source":["AFINN = 'AFINN_full.csv'\n","df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uaKl8hsCTNcs"},"source":["Hint: Para w2v son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendr√°n una representaci√≥n en AFINN. Pueden utilizar esta funci√≥n auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."]},{"cell_type":"code","metadata":{"id":"tWSSuctiTNcs","executionInfo":{"status":"aborted","timestamp":1685322075044,"user_tz":240,"elapsed":15,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"source":["def try_apply(model,word):\n","    try:\n","        aux = model[word]\n","        return True\n","    except KeyError:\n","        #logger.error('Word {} not in dictionary'.format(word))\n","        return False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LrVPeEzgTNcs"},"source":["**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representaci√≥n en embedding que acabamos de calcular (con ambos modelos). \n","\n","Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n","\n","Para ambos modelos, separar train y test de acuerdo a la siguiente funci√≥n. **(0.75 puntos)**"]},{"cell_type":"code","metadata":{"id":"0Bkt26BwTNcs","executionInfo":{"status":"aborted","timestamp":1685322075045,"user_tz":240,"elapsed":16,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iDcq5czXTNct"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"upAn_eT4TNct","executionInfo":{"status":"aborted","timestamp":1685322075045,"user_tz":240,"elapsed":16,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"source":["df_afinn = df_afinn[df_afinn[0].apply(lambda x: try_apply(ft_model.wv,x))]\n","df_afinn[0] = df_afinn[0].apply(lambda x: ft_model.wv[x])\n","X = np.stack(df_afinn[0].values)\n","y = df_afinn[1].values\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kDKe4gA3TNct"},"source":["**Pregunta 2**: Entrenar una regresi√≥n log√≠stica (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qu√© se obtienen estos resultados? C√≥mo los mejorar√≠as? **(0.75 puntos)**"]},{"cell_type":"markdown","metadata":{"id":"hJMzq_dETNct"},"source":["**Respuesta**:"]},{"cell_type":"code","source":["reg = linear_model.LogisticRegression(penalty='l2', solver='liblinear', C=1)\n","reg.fit(X_train, y_train)\n","y_pred = reg.predict(X_test)\n","acc = accuracy_score(y_test, y_pred)\n","pre = precision_score(y_test, y_pred)\n","rec = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","conf = confusion_matrix(y_test, y_pred)"],"metadata":{"id":"Uz2iuJgy_4Id","executionInfo":{"status":"aborted","timestamp":1685322075045,"user_tz":240,"elapsed":16,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logger.info(\"The accuracy is {0}\".format(acc))\n","logger.info(\"The precision is {0}\".format(pre))\n","logger.info(\"The recall is {0}\".format(rec))\n","logger.info(\"The f1 score is {0}\".format(f1))\n","logger.info(\"The confusion matrix:\\n{0}\".format(conf))"],"metadata":{"id":"MwSWcHhI_4xz","executionInfo":{"status":"aborted","timestamp":1685322075045,"user_tz":240,"elapsed":16,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)\n","df_afinn = df_afinn[df_afinn[0].apply(lambda x: try_apply(w2v_model,x))]\n","df_afinn[0] = df_afinn[0].apply(lambda x: w2v_model[x])\n","X = np.stack(df_afinn[0].values)\n","y = df_afinn[1].values\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)\n","reg = linear_model.LogisticRegression(penalty='l2', solver='liblinear', C=1)\n","reg.fit(X_train, y_train)\n","y_pred = reg.predict(X_test)\n","acc = accuracy_score(y_test, y_pred)\n","pre = precision_score(y_test, y_pred)\n","rec = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","conf = confusion_matrix(y_test, y_pred)\n","logger.info(\"The accuracy is {0}\".format(acc))\n","logger.info(\"The precision is {0}\".format(pre))\n","logger.info(\"The recall is {0}\".format(rec))\n","logger.info(\"The f1 score is {0}\".format(f1))\n","logger.info(\"The confusion matrix:\\n{0}\".format(conf))"],"metadata":{"id":"K2zDV2aj_8W0","executionInfo":{"status":"aborted","timestamp":1685322075046,"user_tz":240,"elapsed":16,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"izppruGQTNct"},"source":["# Bonus: +0.25 puntos en cualquier pregunta"]},{"cell_type":"markdown","metadata":{"id":"YW0aeK2KTNct"},"source":["**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset m√°s grande y obtener mejores resultados. Les puede servir [√©sta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."]},{"cell_type":"markdown","metadata":{"id":"qvHcVS3sTNct"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"MSc8p-T8TNcu","executionInfo":{"status":"aborted","timestamp":1685322075046,"user_tz":240,"elapsed":16,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}}},"source":["df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)\n","import gensim.downloader as api\n","model = api.load(\"glove-wiki-gigaword-300\") \n","df_afinn = df_afinn[df_afinn[0].apply(lambda x: try_apply(model,x))]\n","df_afinn[0] = df_afinn[0].apply(lambda x: model[x])\n","X = np.stack(df_afinn[0].values)\n","y = df_afinn[1].values\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)\n","reg = linear_model.LogisticRegression(penalty='l2', solver='liblinear', C=1)\n","reg.fit(X_train, y_train)\n","y_pred = reg.predict(X_test)\n","acc = accuracy_score(y_test, y_pred)\n","pre = precision_score(y_test, y_pred)\n","rec = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","conf = confusion_matrix(y_test, y_pred)\n","logger.info(\"The accuracy is {0}\".format(acc))\n","logger.info(\"The precision is {0}\".format(pre))\n","logger.info(\"The recall is {0}\".format(rec))\n","logger.info(\"The f1 score is {0}\".format(f1))\n","logger.info(\"The ROC AUC score is {0}\".format(roc))\n","logger.info(\"The confusion matrix:\\n{0}\".format(conf))\n","logger.info(\"The kappa is {0}\".format(kappa))\n","logger.info(\"Summary report:\\n{0}\".format(class_rep))"],"execution_count":null,"outputs":[]}]}