{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ckbt7VPDhBwb"},"source":["# **Tarea 3 - Word Embeddings üìö**\n","\n","**Integrantes:**\n","\n","**Fecha l√≠mite de entrega üìÜ:** 3 de mayo.\n","\n","**Tiempo estimado de dedicaci√≥n:**"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-19T18:30:18.109327Z","start_time":"2020-03-19T18:30:18.103344Z"},"id":"q5CSRY4oNCHK"},"source":["\n","**Instrucciones:**\n","- El ejercicio consiste en:\n","    - Responder preguntas relativas a los contenidos vistos en los v√≠deos y slides de las clases. \n","    - Entrenar Word2Vec y Word Context Matrix sobre un peque√±o corpus.\n","    - Evaluar los embeddings obtenidos en una tarea de clasificaci√≥n.\n","- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n","- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n","- El formato de entrega es este mismo Jupyter Notebook.\n","- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n. \n","- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso. \n","\n","\n","**Referencias**\n","\n","V√≠deos: \n","\n","- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n","- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n","- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"]},{"cell_type":"markdown","metadata":{"id":"G4wYf0vgnbTv"},"source":["## **Preguntas te√≥ricas üìï (2 puntos).** ##\n","Para estas preguntas no es necesario implementar c√≥digo, pero pueden utilizar pseudo c√≥digo."]},{"cell_type":"markdown","metadata":{"id":"B5hUG6-8ngoK"},"source":["### **Parte 1: Modelos Lineales (1 ptos)**"]},{"cell_type":"markdown","metadata":{"id":"5yRvZbhsoi8f"},"source":["Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categor√≠as: pol√≠tica, deporte, negocios y otros. "]},{"cell_type":"markdown","metadata":{"id":"irsqBVmCnx3M"},"source":["**Pregunta 1**: Dise√±e un modelo lineal capaz de clasificar un documento seg√∫n estas categor√≠as donde el output sea un vector con una distribuci√≥n de probabilidad con la pertenencia a cada clase. \n","\n","Especifique: representaci√≥n de los documentos de entrada, par√°metros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y funci√≥n de p√©rdida escogida. **(0.5 puntos)**\n","\n","**Respuesta**: Representaci√≥n escogida del documento de entrada: Bag of words por ejemplo\n","\n","Par√°metros del modelo: Matriz de pesos W en donde la columna 1 tengan mayor importancia las palabras de pol√≠tica, columna 2 sobre deporte, columna 3 sobre negocios y columna 4 el resto\n","\n","Transformaciones necesarias: Aplicar softmax al vector de output del modelo.\n","\n","Funci√≥n de p√©rdida escogida: Cross-entropy es el ideal, pero pueden escoger cualquier funci√≥n de perdida multiclase que alcanze el m√≠nimo cuando las predicciones son correctas"]},{"cell_type":"markdown","metadata":{"id":"G5FaWqBVvL90"},"source":["**Pregunta 2**: Explique c√≥mo funciona el proceso de entrenamiento en este tipo de modelos y su evaluaci√≥n. **(0.5 puntos)**\n","\n","**Respuesta**: El objetivo del entrenamiento es minimizar la loss del modelo. Se van tuneando los par√°metros de la matriz W hasta encontrar uno que minimice la loss del modelo. La evaluaci√≥n se hace sobre datos que no se han observado para comprobar generalizaci√≥n del modelo. "]},{"cell_type":"markdown","metadata":{"id":"XkK7pc54njZq"},"source":["### **Parte 2: Redes Neuronales (1 ptos)** "]},{"cell_type":"markdown","metadata":{"id":"VUbJjlj_9AFC"},"source":["Supongamos que tenemos la siguiente red neuronal."]},{"cell_type":"markdown","metadata":{"id":"obUfuOYB_TOC"},"source":["![image.png](https://drive.google.com/uc?export=view&id=1fFTjtMvH6MY8o42_vj010y8eTuCVb5a3)"]},{"cell_type":"markdown","metadata":{"id":"s2z-8zKW0_6q"},"source":["**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matem√°tica. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en funci√≥n del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n","\n","Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.5 Puntos)**\n","\n","**Respuesta**: \n","\n","Formula:\n","$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) =$\n","\n","Dimensiones: \n","\n","- $\\vec{x}$: 3\n","- $W¬π$: 3x2\n","- $\\vec{b}¬π$: 2\n","- $W¬≤$: 2x3\n","- $\\vec{b}¬≤$: 3\n","- $W¬≥:$ 3x1\n","- $\\vec{b}¬≥:$ 1\n","- $W‚Å¥:$ 1x4\n","- $\\vec{b}‚Å¥:$ 4\n","\n","**Pregunta 2**: Explique qu√© es backpropagation. ¬øCuales ser√≠an los par√°metros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n","\n","**Respuesta**: \"Backpropagation es una t√©cnica eficiente para evaluar el gradiente de una loss function L en una red neuronal feed-forward con respecto a todos sus par√°metros\" (Cita de clases). Los par√°metros serian de $W¬π,\\vec{b}¬π$ a $W‚Å¥,\\vec{b}‚Å¥$\n","\n","**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.25 puntos)**\n","\n","**Respuesta**: Los 4 pasos son:\n","\n","- Aplicar el vector x y propagarlo por toda la red\n","- Evaluar $\\delta$ para todas las unidades ocultas\n","- Propagar los $\\delta$ desde el final al inicio de la red\n","- Ocupar la formula de $\\frac{\\partial L}{\\partial W}$\n","\n","En la red anterior se necesitan las derivadas: $f', g',h'$"]},{"cell_type":"markdown","metadata":{"id":"ocS_vQhR1gcU"},"source":["## **Preguntas pr√°cticas üíª (4 puntos).** ##"]},{"cell_type":"markdown","metadata":{"id":"Ol82nJ0FnmcP"},"source":["### **Parte 3: Word Embeddings**"]},{"cell_type":"markdown","source":["En la auxiliar 2 se nombraron dos formas de crear word vectors:\n","\n","-  Distributional Vectors.\n","-  Distributed Vectors.\n","\n","El objetivo de esta parte es comparar las dos embeddings obtenidos de estas dos estrategias en una tarea de clasificaci√≥n."],"metadata":{"id":"Daw7Ee5cdQTb"}},{"cell_type":"code","source":["import re  \n","import pandas as pd \n","from time import time  \n","from collections import defaultdict \n","import string \n","import multiprocessing\n","import os\n","import gensim\n","import sklearn\n","from sklearn import linear_model\n","from collections import Counter\n","import numpy as np\n","import scipy\n","import nltk\n","from nltk import word_tokenize\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n","\n","# word2vec\n","from gensim.models import Word2Vec, KeyedVectors, FastText\n","from gensim.models.phrases import Phrases, Phraser\n","from sklearn.model_selection import train_test_split\n","import logging\n","\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","logger = logging.getLogger(__name__)"],"metadata":{"id":"5_kDPY9IAjvS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Parte A (1 punto)** \n","\n","En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template:\n","\n","```python\n","class WordContextMatrix:\n","\n","  def __init__(self, vocab_size):\n","    # se sugiere agregar un una estructura de datos para guardar las\n","    # palabras del vocab y para guardar el conteo de coocurrencia\n","    ...\n","    \n","  def add_word_to_vocab(self, word):\n","    ...\n","  \n","  def build_matrix(self):\n","    ...\n","\n","  def matrix2dict(self):\n","    # se recomienda transformar la matrix a un diccionario de embedding.\n","    ...\n","\n","```\n","\n","puede modificar los par√°metros o m√©todos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n","\n","```python\n","corpus = [\n","  \"I like deep learning.\",\n","  \"I like NLP.\",\n","  \"I enjoy flying.\"\n","]\n","```\n","\n","Obteniendo una matriz parecia a esta:\n","\n","***Resultado esperado***: \n","\n","| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n","|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n","| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n","| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n","| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n","| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n","| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n","| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n","| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n","| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n","\n","``"],"metadata":{"id":"AuEAv-whdMCG"}},{"cell_type":"markdown","source":["**Respuesta:**"],"metadata":{"id":"ur16vkyO37B5"}},{"cell_type":"code","source":["import scipy\n","import numpy as np\n","\n","\n","class WordContextMatrix:\n","\n","  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n","    self.vocab_size = vocab_size\n","    self.window_size = window_size\n","    self.tokenizer = tokenizer\n","    self.dataset = dataset\n","    self.word2index = {}\n","    self.size = 0\n","    self.coor_matrix = scipy.sparse.lil_matrix((vocab_size + 1, vocab_size))\n","\n","    self.add_word_to_vocab('unk')\n","    \n","  def add_word_to_vocab(self, word):\n","    if self.size < self.vocab_size and word not in self.word2index:\n","      self.word2index[word] = self.size\n","      self.size += 1\n","\n","  def build_matrix(self):\n","    for text in self.dataset:\n","      tokens = self.tokenizer(text)\n","      for token in tokens:\n","        self.add_word_to_vocab(token)\n","      for token in tokens:\n","        ind_word = tokens.index(token)\n","        contexts = get_contexts(ind_word, self.window_size, tokens)\n","        for context in contexts:\n","          if token in self.word2index and context in self.word2index:\n","            ind_focus_word = self.word2index[token]\n","            ind_cont_word = self.word2index[context]\n","            self.coor_matrix[ind_focus_word, ind_cont_word] += 1.0\n","          elif token not in self.word2index and context in self.word2index:\n","            ind_cont_word = self.word2index[context]\n","            self.coor_matrix[0, ind_cont_word] += 1.0\n","\n","  def matrix2dict(self):\n","    embeddings = {}\n","    for word, idx in self.word2index.items():\n","      embeddings[word] = self.coor_matrix[idx, :].toarray()[0]\n","    return embeddings\n","\n","def get_contexts(ind_word, w_size, tokens):\n","    slice_start = ind_word - w_size if (ind_word - w_size >= 0) else 0\n","    slice_end = len(tokens) if (ind_word + w_size + 1 >= len(tokens)) else ind_word + w_size + 1\n","    first_part = tokens[slice_start: ind_word]\n","    last_part = tokens[ind_word + 1: slice_end]\n","    contexts = tuple(first_part + last_part)\n","    return contexts"],"metadata":{"id":"gOI1FL8MlGZB","executionInfo":{"status":"ok","timestamp":1684683599650,"user_tz":240,"elapsed":4,"user":{"displayName":"Gabriel Iturra-Bocaz","userId":"02319919045117626989"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["corpus = [\n","  \"I like deep learning.\",\n","  \"I like NLP.\",\n","  \"I enjoy flying.\"\n","]\n","\n"],"metadata":{"id":"EAupvBjyCykY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eQSq7FW_DV9f","outputId":"41303530-6165-4764-ec6c-4ca0dd423031","executionInfo":{"status":"ok","timestamp":1650580602602,"user_tz":240,"elapsed":432,"user":{"displayName":"Gabriel Iturra","userId":"02319919045117626989"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["wcm = WordContextMatrix(7, 1, corpus, word_tokenize)\n","wcm.build_matrix()"],"metadata":{"id":"zBt6uQolC2zO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wcm.coor_matrix.toarray()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Gdt-MbcDc9U","outputId":"e19aa7dd-22d1-4075-c163-1fb0989167cb","executionInfo":{"status":"ok","timestamp":1650580602605,"user_tz":240,"elapsed":11,"user":{"displayName":"Gabriel Iturra","userId":"02319919045117626989"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 0., 0., 0., 1., 0.],\n","       [0., 0., 2., 0., 0., 0., 0.],\n","       [0., 2., 0., 1., 0., 0., 1.],\n","       [0., 0., 1., 0., 1., 0., 0.],\n","       [0., 0., 0., 1., 0., 1., 0.],\n","       [0., 0., 0., 0., 1., 0., 1.],\n","       [0., 0., 1., 0., 0., 1., 0.],\n","       [0., 0., 0., 0., 0., 0., 0.]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["wcm.matrix2dict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jsBOIBTHDlUp","outputId":"bedbb625-d9ab-45e8-bd0c-4b6c96e5981a","executionInfo":{"status":"ok","timestamp":1650580602608,"user_tz":240,"elapsed":12,"user":{"displayName":"Gabriel Iturra","userId":"02319919045117626989"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'.': array([0., 0., 0., 0., 1., 0., 1.]),\n"," 'I': array([0., 0., 2., 0., 0., 0., 0.]),\n"," 'NLP': array([0., 0., 1., 0., 0., 1., 0.]),\n"," 'deep': array([0., 0., 1., 0., 1., 0., 0.]),\n"," 'learning': array([0., 0., 0., 1., 0., 1., 0.]),\n"," 'like': array([0., 2., 0., 1., 0., 0., 1.]),\n"," 'unk': array([0., 1., 0., 0., 0., 1., 0.])}"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"OgmeSFqKLpFL"},"source":["#### **Parte B (1.5 puntos)**\n","\n","En esta parte es debe entrenar Word2Vec de gensim y construir la matriz palabra contexto utilizando el dataset de di√°logos de los Simpson. "]},{"cell_type":"markdown","metadata":{"id":"tZgN06q4QPi3"},"source":["Utilizando el dataset adjunto con la tarea:"]},{"cell_type":"code","metadata":{"id":"eY3kmg4onnsu"},"source":["data_file = \"drive/MyDrive/dialogue-lines-of-the-simpsons.zip\"\n","df = pd.read_csv(data_file)\n","stopwords = pd.read_csv(\n","    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",").values\n","stopwords = Counter(stopwords.flatten().tolist())\n","df = df.dropna().reset_index(drop=True) # Quitar filas vacias"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"VP2FjXyLw7Lm","executionInfo":{"status":"ok","timestamp":1650580603229,"user_tz":240,"elapsed":11,"user":{"displayName":"Gabriel Iturra","userId":"02319919045117626989"}},"outputId":"ad46f6b6-600b-4d39-da04-22fe8a557797"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        raw_character_text                                       spoken_words\n","0              Miss Hoover  No, actually, it was a little of both. Sometim...\n","1             Lisa Simpson                             Where's Mr. Bergstrom?\n","2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n","3             Lisa Simpson                         That life is worth living.\n","4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."],"text/html":["\n","  <div id=\"df-9f57b250-3c0b-4f43-aa20-14458c31ccc6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>raw_character_text</th>\n","      <th>spoken_words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Miss Hoover</td>\n","      <td>No, actually, it was a little of both. Sometim...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Lisa Simpson</td>\n","      <td>Where's Mr. Bergstrom?</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Miss Hoover</td>\n","      <td>I don't know. Although I'd sure like to talk t...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Lisa Simpson</td>\n","      <td>That life is worth living.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Edna Krabappel-Flanders</td>\n","      <td>The polls will be open from now until the end ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f57b250-3c0b-4f43-aa20-14458c31ccc6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9f57b250-3c0b-4f43-aa20-14458c31ccc6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9f57b250-3c0b-4f43-aa20-14458c31ccc6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["punctuation = string.punctuation + \"¬´¬ª‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî\"\n","def simple_tokenizer(doc, lower=False):\n","    if lower:\n","        tokenized_doc = doc.translate(str.maketrans(\n","            '', '', punctuation)).lower().split()\n","\n","    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n","\n","    tokenized_doc = [\n","        token for token in tokenized_doc if token.lower() not in stopwords\n","    ]\n","    return tokenized_doc\n","content = df['spoken_words']\n","sentences = [simple_tokenizer(doc) for doc in content.values]"],"metadata":{"id":"KLOMH6zrHLe6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VAg5a5bmWk3T"},"source":["**Pregunta 1**: Ayud√°ndose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec. **(0.75 punto)** (Hint, le puede servir explorar un poco los datos)"]},{"cell_type":"markdown","metadata":{"id":"MWw2fXFRXe5Y"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"Bvwplz7yTNcr"},"source":["w2v_model = Word2Vec(min_count=10,\n","                      window=4,\n","                      size=200,\n","                      sample=6e-5,\n","                      alpha=0.03,\n","                      min_alpha=0.0007,\n","                      negative=20,\n","                      workers=multiprocessing.cpu_count())"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v_model.build_vocab(sentences, progress_per=10000)"],"metadata":{"id":"-HWerQtHHvfk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650580608942,"user_tz":240,"elapsed":4306,"user":{"displayName":"Gabriel Iturra","userId":"02319919045117626989"}},"outputId":"09613c1c-15f9-4832-f25c-aa81b3378f0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2022-04-21 22:36:44,774 : INFO : collecting all words and their counts\n","2022-04-21 22:36:44,778 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","2022-04-21 22:36:44,802 : INFO : PROGRESS: at sentence #10000, processed 33320 words, keeping 10655 word types\n","2022-04-21 22:36:44,836 : INFO : PROGRESS: at sentence #20000, processed 67371 words, keeping 17128 word types\n","2022-04-21 22:36:44,869 : INFO : PROGRESS: at sentence #30000, processed 104569 words, keeping 23061 word types\n","2022-04-21 22:36:44,900 : INFO : PROGRESS: at sentence #40000, processed 138266 words, keeping 27246 word types\n","2022-04-21 22:36:44,929 : INFO : PROGRESS: at sentence #50000, processed 170239 words, keeping 31169 word types\n","2022-04-21 22:36:44,958 : INFO : PROGRESS: at sentence #60000, processed 200105 words, keeping 34573 word types\n","2022-04-21 22:36:44,988 : INFO : PROGRESS: at sentence #70000, processed 233512 words, keeping 38228 word types\n","2022-04-21 22:36:45,067 : INFO : PROGRESS: at sentence #80000, processed 268907 words, keeping 41807 word types\n","2022-04-21 22:36:45,104 : INFO : PROGRESS: at sentence #90000, processed 303348 words, keeping 45061 word types\n","2022-04-21 22:36:45,138 : INFO : PROGRESS: at sentence #100000, processed 337260 words, keeping 48055 word types\n","2022-04-21 22:36:45,182 : INFO : PROGRESS: at sentence #110000, processed 371777 words, keeping 51327 word types\n","2022-04-21 22:36:45,218 : INFO : PROGRESS: at sentence #120000, processed 405287 words, keeping 54155 word types\n","2022-04-21 22:36:45,249 : INFO : PROGRESS: at sentence #130000, processed 438262 words, keeping 56276 word types\n","2022-04-21 22:36:45,259 : INFO : collected 56628 word types from a corpus of 444264 raw words and 131853 sentences\n","2022-04-21 22:36:45,265 : INFO : Loading a fresh vocabulary\n","2022-04-21 22:36:45,342 : INFO : effective_min_count=10 retains 6492 unique words (11% of original 56628, drops 50136)\n","2022-04-21 22:36:45,348 : INFO : effective_min_count=10 leaves 338988 word corpus (76% of original 444264, drops 105276)\n","2022-04-21 22:36:45,389 : INFO : deleting the raw counts dictionary of 56628 items\n","2022-04-21 22:36:45,397 : INFO : sample=6e-05 downsamples 1253 most-common words\n","2022-04-21 22:36:45,403 : INFO : downsampling leaves estimated 203325 word corpus (60.0% of prior 338988)\n","2022-04-21 22:36:45,442 : INFO : estimated required memory for 6492 words and 200 dimensions: 13633200 bytes\n","2022-04-21 22:36:45,448 : INFO : resetting layer weights\n"]}]},{"cell_type":"code","source":["t = time()\n","w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=15, report_delay=10)\n","print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n","if not os.path.exists('./pretrained_models'):\n","    os.mkdir('./pretrained_models')\n","w2v_model.save('./pretrained_models/w2v.model')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yVD-vV2wxI0C","executionInfo":{"status":"ok","timestamp":1650580646483,"user_tz":240,"elapsed":37546,"user":{"displayName":"Gabriel Iturra","userId":"02319919045117626989"}},"outputId":"f9db702b-560d-4a7f-f534-03195e17aeba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2022-04-21 22:36:48,922 : INFO : training model with 2 workers on 6492 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4\n","2022-04-21 22:36:50,002 : INFO : EPOCH 1 - PROGRESS: at 40.24% examples, 77362 words/s, in_qsize 3, out_qsize 0\n","2022-04-21 22:36:51,318 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-04-21 22:36:51,342 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-04-21 22:36:51,349 : INFO : EPOCH - 1 : training on 444264 raw words (202914 effective words) took 2.4s, 84389 effective words/s\n","2022-04-21 22:36:52,376 : INFO : EPOCH 2 - PROGRESS: at 45.47% examples, 89940 words/s, in_qsize 3, out_qsize 0\n","2022-04-21 22:36:53,473 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-04-21 22:36:53,503 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-04-21 22:36:53,509 : INFO : EPOCH - 2 : training on 444264 raw words (203257 effective words) took 2.1s, 94689 effective words/s\n","2022-04-21 22:36:54,572 : INFO : EPOCH 3 - PROGRESS: at 50.19% examples, 95991 words/s, in_qsize 3, out_qsize 0\n","2022-04-21 22:36:55,538 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-04-21 22:36:55,612 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-04-21 22:36:55,615 : INFO : EPOCH - 3 : training on 444264 raw words (203199 effective words) took 2.1s, 97249 effective words/s\n","2022-04-21 22:36:56,658 : INFO : EPOCH 4 - PROGRESS: at 45.47% examples, 88336 words/s, in_qsize 3, out_qsize 0\n","2022-04-21 22:36:57,830 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-04-21 22:36:57,850 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-04-21 22:36:57,852 : INFO : EPOCH - 4 : training on 444264 raw words (203656 effective words) took 2.2s, 91426 effective words/s\n","2022-04-21 22:36:58,945 : INFO : EPOCH 5 - PROGRESS: at 42.84% examples, 81137 words/s, in_qsize 3, out_qsize 0\n","2022-04-21 22:37:00,258 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-04-21 22:37:00,281 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-04-21 22:37:00,285 : INFO : EPOCH - 5 : training on 444264 raw words (203149 effective words) took 2.4s, 84423 effective words/s\n","2022-04-21 22:37:01,345 : INFO : EPOCH 6 - PROGRESS: at 30.77% examples, 61074 words/s, in_qsize 3, out_qsize 0\n","2022-04-21 22:37:03,051 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-04-21 22:37:03,072 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-04-21 22:37:03,074 : INFO : EPOCH - 6 : training on 444264 raw words (203036 effective words) took 2.8s, 73283 effective words/s\n","2022-04-21 22:37:04,123 : INFO : EPOCH 7 - PROGRESS: at 30.77% examples, 62769 words/s, in_qsize 3, out_qsize 0\n","2022-04-21 22:37:05,942 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-04-21 22:37:05,953 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-04-21 22:37:05,960 : INFO : EPOCH - 7 : training on 444264 raw words (203244 effective words) took 2.8s, 71320 effective words/s\n","2022-04-21 22:37:07,004 : INFO : EPOCH 8 - PROGRESS: at 40.24% examples, 80087 words/s, in_qsize 3, out_qsize 0\n","2022-04-21 22:37:08,688 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-04-21 22:37:08,705 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-04-21 22:37:08,711 : INFO : EPOCH - 8 : training on 444264 raw words (203267 effective words) took 2.7s, 74394 effective words/s\n","2022-04-21 22:37:09,756 : INFO : EPOCH 9 - PROGRESS: at 40.24% examples, 80859 words/s, in_qsize 3, out_qsize 0\n","2022-04-21 22:37:11,124 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-04-21 22:37:11,130 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-04-21 22:37:11,132 : INFO : EPOCH - 9 : training on 444264 raw words (203025 effective words) took 2.4s, 84969 effective words/s\n","2022-04-21 22:37:12,202 : INFO : EPOCH 10 - PROGRESS: at 35.40% examples, 70947 words/s, in_qsize 3, out_qsize 0\n","2022-04-21 22:37:13,633 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-04-21 22:37:13,642 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-04-21 22:37:13,644 : INFO : EPOCH - 10 : training on 444264 raw words (203694 effective words) took 2.5s, 82270 effective words/s\n","2022-04-21 22:37:14,828 : INFO : EPOCH 11 - PROGRESS: at 30.77% examples, 56276 words/s, in_qsize 2, out_qsize 1\n","2022-04-21 22:37:16,797 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-04-21 22:37:16,837 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-04-21 22:37:16,840 : INFO : EPOCH - 11 : training on 444264 raw words (203058 effective words) took 3.1s, 64585 effective words/s\n","2022-04-21 22:37:17,854 : INFO : EPOCH 12 - PROGRESS: at 42.88% examples, 86793 words/s, in_qsize 3, out_qsize 0\n","2022-04-21 22:37:19,193 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-04-21 22:37:19,246 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-04-21 22:37:19,258 : INFO : EPOCH - 12 : training on 444264 raw words (203343 effective words) took 2.4s, 84510 effective words/s\n","2022-04-21 22:37:20,284 : INFO : EPOCH 13 - PROGRESS: at 30.77% examples, 63501 words/s, in_qsize 2, out_qsize 1\n","2022-04-21 22:37:21,732 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-04-21 22:37:21,758 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-04-21 22:37:21,765 : INFO : EPOCH - 13 : training on 444264 raw words (203146 effective words) took 2.5s, 81768 effective words/s\n","2022-04-21 22:37:22,837 : INFO : EPOCH 14 - PROGRESS: at 42.88% examples, 84010 words/s, in_qsize 3, out_qsize 0\n","2022-04-21 22:37:24,023 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-04-21 22:37:24,043 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-04-21 22:37:24,050 : INFO : EPOCH - 14 : training on 444264 raw words (203429 effective words) took 2.2s, 90535 effective words/s\n","2022-04-21 22:37:25,104 : INFO : EPOCH 15 - PROGRESS: at 47.93% examples, 92884 words/s, in_qsize 4, out_qsize 0\n","2022-04-21 22:37:26,214 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-04-21 22:37:26,240 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-04-21 22:37:26,242 : INFO : EPOCH - 15 : training on 444264 raw words (203560 effective words) took 2.2s, 93818 effective words/s\n","2022-04-21 22:37:26,243 : INFO : training on a 6663960 raw words (3048977 effective words) took 37.3s, 81732 effective words/s\n","2022-04-21 22:37:26,252 : INFO : saving Word2Vec object under ./pretrained_models/w2v.model, separately None\n","2022-04-21 22:37:26,266 : INFO : not storing attribute vectors_norm\n","2022-04-21 22:37:26,280 : INFO : not storing attribute cum_table\n"]},{"output_type":"stream","name":"stdout","text":["Time to train the model: 0.62 mins\n"]},{"output_type":"stream","name":"stderr","text":["2022-04-21 22:37:26,498 : INFO : saved ./pretrained_models/w2v.model\n"]}]},{"cell_type":"markdown","source":["**Pregunta 2**: Cree una matriz palabra contexto usando el mismo dataset. Configure el largo del vocabulario 7028. Puede que esto tarde un poco. **(0.75 punto)** "],"metadata":{"id":"3vBkF3hreGjg"}},{"cell_type":"markdown","source":["**Respuesta:**"],"metadata":{"id":"zzLuH6MneWIY"}},{"cell_type":"code","source":["wcm = WordContextMatrix(1000, 4, content.values, word_tokenize)"],"metadata":{"id":"9gPyW8fMeXNX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wcm.build_matrix()"],"metadata":{"id":"8XgIOej6xy0V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" "],"metadata":{"id":"UrHNMpkr_OM0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embs = wcm.matrix2dict()"],"metadata":{"id":"7a_dubuByJ0K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IRCB-jqgTNcs"},"source":["#### **Parte C (1.5 puntos): Aplicar embeddings para clasificar**"]},{"cell_type":"markdown","metadata":{"id":"zlqzlJRSTNcs"},"source":["Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n","\n","Para esto ocuparemos el lexic√≥n AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotaci√≥n es positiva y un -1 si es negativa."]},{"cell_type":"code","metadata":{"id":"CMskFDmHTNcs"},"source":["AFINN = 'drive/MyDrive/AFINN_full.csv'\n","df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uaKl8hsCTNcs"},"source":["Hint: Para w2v y la wcm son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendr√°n una representaci√≥n en AFINN. Para el caso de la matriz palabra contexto se recomienda convertir su matrix a un diccionario. Pueden utilizar esta funci√≥n auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."]},{"cell_type":"code","metadata":{"id":"tWSSuctiTNcs"},"source":["def try_apply(model,word):\n","    try:\n","        aux = model[word]\n","        return True\n","    except KeyError:\n","        #logger.error('Word {} not in dictionary'.format(word))\n","        return False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LrVPeEzgTNcs"},"source":["**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representaci√≥n en embedding que acabamos de calcular (con ambos modelos). \n","\n","Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n","\n","Para ambos modelos, separar train y test de acuerdo a la siguiente funci√≥n. **(0.5 puntos)**"]},{"cell_type":"markdown","source":["```python \n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)\n","```"],"metadata":{"id":"mn3WNPU5SQrF"}},{"cell_type":"markdown","metadata":{"id":"iDcq5czXTNct"},"source":["**Respuesta**:"]},{"cell_type":"markdown","source":["- Word Context Matrix"],"metadata":{"id":"81sI8akdO7jP"}},{"cell_type":"code","source":["df_afinn = df_afinn[df_afinn[0].apply(lambda x: try_apply(embs,x))]\n","df_afinn[0] = df_afinn[0].apply(lambda x: embs[x])\n","X = np.stack(df_afinn[0].values)\n","y = df_afinn[1].values\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ef0YS_C3PX-Y","executionInfo":{"status":"ok","timestamp":1650580752980,"user_tz":240,"elapsed":241,"user":{"displayName":"Gabriel Iturra","userId":"02319919045117626989"}},"outputId":"d22ac9f4-9887-4fe3-f03b-39a68948caaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n"]}]},{"cell_type":"markdown","metadata":{"id":"kDKe4gA3TNct"},"source":["**Pregunta 2**: Entrenar una regresi√≥n log√≠stica (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qu√© se obtienen estos resultados? C√≥mo los mejorar√≠as? Como podr√≠as mejorar los resultados de la matriz palabra contexto? es equivalente al modelo word2vec? **(1 punto)**"]},{"cell_type":"markdown","metadata":{"id":"hJMzq_dETNct"},"source":["**Respuesta**:"]},{"cell_type":"code","source":["reg = linear_model.LogisticRegression(penalty='l2', solver='liblinear', C=1)\n","reg.fit(X_train, y_train)\n","y_pred = reg.predict(X_test)\n","acc = accuracy_score(y_test, y_pred)\n","pre = precision_score(y_test, y_pred)\n","rec = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","conf = confusion_matrix(y_test, y_pred)"],"metadata":{"id":"bj1r_BnKn_7L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logger.info(\"The accuracy is {0}\".format(acc))\n","logger.info(\"The precision is {0}\".format(pre))\n","logger.info(\"The recall is {0}\".format(rec))\n","logger.info(\"The f1 score is {0}\".format(f1))\n","logger.info(\"The confusion matrix:\\n{0}\".format(conf))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v4v2oQcEQVqp","executionInfo":{"status":"ok","timestamp":1650580758349,"user_tz":240,"elapsed":246,"user":{"displayName":"Gabriel Iturra","userId":"02319919045117626989"}},"outputId":"e05c9c7e-f5c9-4c8a-c5cd-49e05ba1f818"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2022-04-21 22:39:18,382 : INFO : The accuracy is 1.0\n","2022-04-21 22:39:18,385 : INFO : The precision is 1.0\n","2022-04-21 22:39:18,390 : INFO : The recall is 1.0\n","2022-04-21 22:39:18,394 : INFO : The f1 score is 1.0\n","2022-04-21 22:39:18,398 : INFO : The confusion matrix:\n","[[6 0]\n"," [0 5]]\n"]}]},{"cell_type":"code","source":["df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)\n","df_afinn = df_afinn[df_afinn[0].apply(lambda x: try_apply(w2v_model,x))]\n","df_afinn[0] = df_afinn[0].apply(lambda x: w2v_model[x])\n","X = np.stack(df_afinn[0].values)\n","y = df_afinn[1].values\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)\n","reg = linear_model.LogisticRegression(penalty='l2', solver='liblinear', C=1)\n","reg.fit(X_train, y_train)\n","y_pred = reg.predict(X_test)\n","acc = accuracy_score(y_test, y_pred)\n","pre = precision_score(y_test, y_pred)\n","rec = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","conf = confusion_matrix(y_test, y_pred)\n","logger.info(\"The accuracy is {0}\".format(acc))\n","logger.info(\"The precision is {0}\".format(pre))\n","logger.info(\"The recall is {0}\".format(rec))\n","logger.info(\"The f1 score is {0}\".format(f1))\n","logger.info(\"The confusion matrix:\\n{0}\".format(conf))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24TIQvv-QZGH","executionInfo":{"status":"ok","timestamp":1650580770036,"user_tz":240,"elapsed":236,"user":{"displayName":"Gabriel Iturra","userId":"02319919045117626989"}},"outputId":"865c647c-274a-471c-f046-45c647ec0ffa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","2022-04-21 22:39:30,129 : INFO : The accuracy is 0.5813953488372093\n","2022-04-21 22:39:30,130 : INFO : The precision is 0.0\n","2022-04-21 22:39:30,133 : INFO : The recall is 0.0\n","2022-04-21 22:39:30,135 : INFO : The f1 score is 0.0\n","2022-04-21 22:39:30,137 : INFO : The confusion matrix:\n","[[50  0]\n"," [36  0]]\n"]}]},{"cell_type":"markdown","source":["Estos resultados son p√©simos. Hay 2 razones posibles: El dataset es muy peque√±o c√≥mo para que estos modelos logren aprender bien las relaciones entre las palabras o puede que con los di√°logos de los Simpson no se obtengan buenos embeddings para clasificar palabras por sentimiento. Se podr√≠a mejorar a√±adiendo mas datos, por ejemplo los subt√≠tulos de las pel√≠culas o simplemente buscar otro dataset m√°s grande."],"metadata":{"id":"ofRAHdoCSJMg"}},{"cell_type":"markdown","metadata":{"id":"izppruGQTNct"},"source":["# Bonus: +0.25 puntos en cualquier pregunta"]},{"cell_type":"markdown","metadata":{"id":"YW0aeK2KTNct"},"source":["**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset m√°s grande y obtener mejores resultados. Les puede servir [√©sta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."]},{"cell_type":"markdown","metadata":{"id":"qvHcVS3sTNct"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"MSc8p-T8TNcu"},"source":[],"execution_count":null,"outputs":[]}]}