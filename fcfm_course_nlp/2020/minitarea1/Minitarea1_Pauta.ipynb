{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"Minitarea1_Pauta.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-19T18:30:18.109327Z","start_time":"2020-03-19T18:30:18.103344Z"},"id":"q5CSRY4oNCHK"},"source":["# Minitarea 1\n","\n","\n","-----------------\n","\n","Nombre: \n","\n","Fecha de Entrega: Lunes 6 de Abril\n","\n","\n","## Objetivos de la minitarea\n","\n","Este ejericio cuenta con varios objetivos principales: \n","\n","- Evaluar los contenidos de las primeras semanas de clases. En particular Information Retrieval (IR) y Vector Space Models. \n","\n","- Introducirlos a la programación en python enfocada en NLP.\n","\n","- Implementar un modelo básico de IR: *Term Frequency-Inverse Document Frequency (TF-IDF)*.\n","\n","\n","## Instrucciones\n","\n","- El ejercicio consiste en:\n","\n","    - Responder preguntas relativas a los contenidos vistos en los videos y slides de las clases. \n","    \n","    - Implementar el modelo TF-IFD utilizando solo python, pandas y numpy. Está **PROHIBIDO** usar cualquier paquete que implemente dicho modelo (NLTK, spacy, scikit, etc...).\n","\n","- La minitarea es INDIVIDUAL.\n","\n","- Está demás decir que no se admiten copias, ni de código, ni de respuestas escritas. \n","\n","- La entrega debe ser por ucursos a mas tardar el día estipulado arriba. No se aceptan atrasos.\n","\n","- En el horario de auxiliar se abriran horarios de consulta en donde podrán preguntar acerca del ejercicio y en general, de todo el curso. \n","\n","- Cada punto equivale a 0.5 décimas de la nota de la minitarea.\n","\n","- Al revisar, tu código será ejecutado. Verifica que tu entrega no tenga errores.\n","\n","\n","## Referencias\n"," \n","Slides:\n","    \n","- [Introducción al curso](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-introduction.pdf)\n","- [Vector Space Model / Information Retrieval](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-IR.pdf)    \n","\n","Videos: \n","\n","- [CC6205 - Procesamiento de Lenguaje Natural: Vector Space Model and Information Retrieval parte 1](https://youtu.be/FXIVClF370w)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Vector Space Model and Information Retrieval parte 2](https://youtu.be/f8nG1EMmPZk)"]},{"cell_type":"markdown","metadata":{"id":"SKjrzckdNCHL"},"source":["## Parte 1. \n","\n","La siguientes celdas contendrán preguntas acerca del contenido visto en clases y en el material del curso. La idea es contestar cada pregunta en su celda correspondiente. Las respuestas deben ser breves: máximo 3 lineas (salvo para la p3)."]},{"cell_type":"markdown","metadata":{"id":"qemse4YdNCHM"},"source":["**P1) Son Natural Language Processing y Computational Linguistics lo mismo? (1 Punto)**\n","\n","Respuesta: \n","    \n","    No. NLP desarrolla métodos y modelos para resolver problemas prácticos (tasks) del lenguaje. CL estudia los procesos y formalismos computaciones que fundamentan el lenguaje humano. Sin embargo, muchas de las tareas de ambos se solapan."]},{"cell_type":"markdown","metadata":{"id":"bMcCS7_aNCHN"},"source":["**P2) Por qué estudiar el lenguaje humano es difícil? (1 Punto)**\n","\n","Respuesta: \n","    \n","    Porque es extremadamente complejo entender formalmente y describir las reglas que rigen el lenguaje."]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-19T18:45:25.544502Z","start_time":"2020-03-19T18:45:25.537548Z"},"id":"nSThg8CWNCHO"},"source":["\n","**P3) Para el siguiente corpus:**\n","\n","    d1) I like human languages\n","\n","    d2) I like programming languages\n","\n","    d3) Spanish is my favorite language\n","\n","\n","**Extraiga el vocabulario:**\n","\n","1. **Solamente usando tokenization (1.5 puntos)**\n","\n","Respuesta: \n","\n","        Vocabulario: ...\n","\n","        Vocabulario: I, like, human, languages, programming, Spanish is, my favorite, language\n","\n","\n","2. **Usando stemming (proponga sus propias reglas de stemming) y borrado de stopwords (indique cuales son sus stopwords) (1.5 puntos)**\n","\n","Respuesta: \n","\n","        Stopwords: ...\n","        \n","        Reglas de Stemming: ...\n","        \n","        Vocabulario: ...\n","        \n","        Stopwords: I, is, my\n","\n","        Reglas de Stemming: \n","\n","        - ...s -> nada\n","        - ...ing -> nada\n","    \n","        Vocabulario: like, human, language, program, favorite.\n","\n"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-19T18:46:24.015666Z","start_time":"2020-03-19T18:46:24.010706Z"},"id":"YRx1E7AqNCHP"},"source":["**P4) Conceptualmente cuál es la diferencia entre usar machine learning clásico (empirismo) y deep learning para un problema de NLP (Puede usar el análisis de sentimientos como ejemplo) (1 punto)**\n","\n","    Respuesta: En machine learning clásico el desarrollador debía crear por el mismo las características/representaciones. En deep learning, estas representaciones son aprendidas por los mismos algoritmos.\n"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-19T18:47:04.360754Z","start_time":"2020-03-19T18:47:04.357763Z"},"id":"wJDhGpc3NCHQ"},"source":["----------\n","\n","## Parte 2"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-19T18:57:05.868104Z","start_time":"2020-03-19T18:57:05.863117Z"},"id":"brr-0WyWNCHQ"},"source":["Impementar TF-IDF. \n","\n","Esta parte, cada celda representa una función distinta por implementar. Se usará pandas para representar las matrices y arreglos que vayamos calculando. Siguiente a cada celda con una función por implementar habrá un ejemplo de como debería ser el output que tienen que lograr. No vale hardcodeo..."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-25T17:24:35.689898Z","start_time":"2020-03-25T17:24:34.889604Z"},"id":"UHA4g3LBNCHR"},"source":["import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hgg2MLpWNCHV"},"source":["**Corpus**"]},{"cell_type":"markdown","metadata":{"id":"9BxubGj5NCHW"},"source":["En el dataset, cada string representa un documento."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-25T17:26:51.639800Z","start_time":"2020-03-25T17:26:51.634834Z"},"id":"H5DZlJHYNCHW"},"source":["dataset = [\n","    't4 t3 t1 t4', 't5 t4 t2 t3 t5', 't2 t1 t4 t4', 't2 t3 t3 t1 t4',\n","    't1 t4 t2 t2', 't2 t3 t2 t3 t2 t3'\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lKUsHbK0NCHa"},"source":["**Obtener vocabulario (1 punto)** \n","\n","Implementar la función `get_vocab(dataset)` que recibe el dataset y retorna un arreglo con cada palabra que aparece en el dataset. (sin duplicar)"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-25T17:26:38.959217Z","start_time":"2020-03-25T17:26:38.954231Z"},"id":"BOOQYeO0NCHa"},"source":["def get_vocab(dataset):\n","    vocab = {}\n","    for document in dataset:\n","        for word in document.split(' '):\n","            if not word in vocab:\n","                vocab[word] = 1\n","    return list(vocab.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-25T17:26:53.088662Z","start_time":"2020-03-25T17:26:53.083676Z"},"id":"5OIiJtR6NCHd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615400499408,"user_tz":180,"elapsed":1335,"user":{"displayName":"Matias Rojas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM9SODS592KIjFIMn2zquUVG03kY2oETZyGYh7=s64","userId":"06611082509338078725"}},"outputId":"2c106df2-6fc1-48a7-ebd5-956fe23cfd23"},"source":["vocab = get_vocab(dataset)\n","vocab"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['t4', 't3', 't1', 't5', 't2']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"oZsLRuYUNCHh"},"source":["**Calcular Bag of Words (bow) (2 puntos)**\n","\n","implementar la función `calc_bow(dataset, vocab)` que toma el dataset y el vocabulario calculado en la parte anterior y retorna un pandas DataFrame en donde las columnas son el vocabulario y las filas representa las apariciones de cada una de las palabra los documento. En otras palabras, cada fila representa el bow de un determinado documento\n","\n","el bow de cada documento.\n","\n","Recordatorio - Bag of Words: Cuenta las apariciones de cada palabra en cada uno de los documentos. Por ejemplo:\n","\n","Por ejemplo, para los documentos: \n","\n","```\n","d_0 = 'El perro ladra'\n","d_1 = 'El perro come'\n","\n","```\n","\n","Deberíamos retornar:\n","\n","|   | el | perro | ladra | come |\n","|---|----|-------|------|-------|\n","| 0 | 1  | 1     | 1    | 0     |\n","| 1 | 1  | 1     | 0    | 1     |\n","\n","\n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-25T17:26:54.256241Z","start_time":"2020-03-25T17:26:54.251272Z"},"id":"z4UhnemlNCHi"},"source":["def calc_bow(dataset, vocab):\n","    all_bow = []\n","    \n","    for document in dataset:\n","        current_bow = {word: 0 for word in vocab}\n","        for word in document.split(' '):\n","            current_bow[word] += 1\n","            \n","        all_bow.append(current_bow)\n","        \n","    return pd.DataFrame(all_bow)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-25T17:26:55.054852Z","start_time":"2020-03-25T17:26:55.045894Z"},"id":"v00S7cGaNCHl","colab":{"base_uri":"https://localhost:8080/","height":233},"executionInfo":{"status":"ok","timestamp":1615400506434,"user_tz":180,"elapsed":625,"user":{"displayName":"Matias Rojas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM9SODS592KIjFIMn2zquUVG03kY2oETZyGYh7=s64","userId":"06611082509338078725"}},"outputId":"51d338d9-97e5-4d16-b3d7-2ffde32aed46"},"source":["dataset_bow = calc_bow(dataset, vocab)\n","dataset_bow"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>t4</th>\n","      <th>t3</th>\n","      <th>t1</th>\n","      <th>t5</th>\n","      <th>t2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   t4  t3  t1  t5  t2\n","0   2   1   1   0   0\n","1   1   1   0   2   1\n","2   2   0   1   0   1\n","3   1   2   1   0   1\n","4   1   0   1   0   2\n","5   0   3   0   0   3"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"5zmr4YeuW1T0","colab":{"base_uri":"https://localhost:8080/","height":233},"executionInfo":{"status":"ok","timestamp":1615400515084,"user_tz":180,"elapsed":573,"user":{"displayName":"Matias Rojas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM9SODS592KIjFIMn2zquUVG03kY2oETZyGYh7=s64","userId":"06611082509338078725"}},"outputId":"72608dad-0020-4d77-cf09-afa598261f35"},"source":["dataset_bow = pd.DataFrame([[2,1,1,0,0],[1,1,0,2,1],[2,0,1,0,1],[1,2,1,0,1],[1,0,1,0,2],[0,3,0,0,3]], columns=['t4', 't3', 't1', 't5', 't2'])\n","dataset_bow"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>t4</th>\n","      <th>t3</th>\n","      <th>t1</th>\n","      <th>t5</th>\n","      <th>t2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   t4  t3  t1  t5  t2\n","0   2   1   1   0   0\n","1   1   1   0   2   1\n","2   2   0   1   0   1\n","3   1   2   1   0   1\n","4   1   0   1   0   2\n","5   0   3   0   0   3"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-20T14:27:01.721767Z","start_time":"2020-03-20T14:27:01.716781Z"},"id":"tMGtXSJ6NCHo"},"source":["**Calcular TF (1 punto)**\n","\n","En esta sección debemos usar el dataframe calcular la normalización para obtener la matriz de TF. Es decir, dividir cada bow en la palabra que aparezca mas veces ese vector\n","\n","$$tf_{i,j} = \\frac{tf_{i,j}}{max_i({tf_{i,j})}}$$"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-25T17:26:58.199730Z","start_time":"2020-03-25T17:26:58.195728Z"},"id":"dLmVJ5pQNCHp"},"source":["def calc_tf(dataset_bow):\n","    return dataset_bow.divide(dataset_bow.max(axis=1), axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-25T17:26:58.605930Z","start_time":"2020-03-25T17:26:58.593962Z"},"id":"ZCnD6BcLNCHr","colab":{"base_uri":"https://localhost:8080/","height":233},"executionInfo":{"status":"ok","timestamp":1615400531832,"user_tz":180,"elapsed":569,"user":{"displayName":"Matias Rojas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM9SODS592KIjFIMn2zquUVG03kY2oETZyGYh7=s64","userId":"06611082509338078725"}},"outputId":"bba6ac7d-e8bb-4646-d3ff-da7d0bbe05f4"},"source":["tf = calc_tf(dataset_bow)\n","tf"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>t4</th>\n","      <th>t3</th>\n","      <th>t1</th>\n","      <th>t5</th>\n","      <th>t2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>0.5</td>\n","      <td>0.5</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.5</td>\n","      <td>0.5</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.5</td>\n","      <td>0.0</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.5</td>\n","      <td>1.0</td>\n","      <td>0.5</td>\n","      <td>0.0</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.5</td>\n","      <td>0.0</td>\n","      <td>0.5</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    t4   t3   t1   t5   t2\n","0  1.0  0.5  0.5  0.0  0.0\n","1  0.5  0.5  0.0  1.0  0.5\n","2  1.0  0.0  0.5  0.0  0.5\n","3  0.5  1.0  0.5  0.0  0.5\n","4  0.5  0.0  0.5  0.0  1.0\n","5  0.0  1.0  0.0  0.0  1.0"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"wHtuKNuANCHu"},"source":["**Calcular IDF (1 punto)**\n","\n","\n","Implementar `calc_idf(dataset_bow)`. Este debe retornar un diccionario en donde las llaves sean las palabras y los valores sean el calculo de cada idf por palabra.\n","\n","Recordar que $idf_{t_i} = log_{10}\\frac{N}{n_i}$ con $N = $ número de documentos y $n_i = $ Número de documentos que contienen la palabra $t_i$ \n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-25T17:27:01.388316Z","start_time":"2020-03-25T17:27:01.383330Z"},"id":"nzoDZzp8NCHv"},"source":["def calc_idf(dataset_bow):\n","    number_of_docs = len(dataset_bow)\n","    idf = {}\n","    for word in dataset_bow:\n","        n_i = (dataset_bow[[word]] != 0).sum()[0]\n","        idf[word] = np.log10(number_of_docs/n_i)\n","    return idf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-25T17:27:01.946573Z","start_time":"2020-03-25T17:27:01.929617Z"},"id":"XGStFkPqNCHy","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1588036421913,"user_tz":240,"elapsed":648,"user":{"displayName":"FeedTheTrees","photoUrl":"","userId":"16064772018428921502"}},"outputId":"75412fa1-19b4-4c30-c242-ebe219e4d8ed"},"source":["idf = calc_idf(dataset_bow)\n","idf"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'t1': 0.17609125905568124,\n"," 't2': 0.07918124604762482,\n"," 't3': 0.17609125905568124,\n"," 't4': 0.07918124604762482,\n"," 't5': 0.7781512503836436}"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"gJNa07NZNCH2"},"source":["**Calcular TF-IDF (1 punto)**\n","\n","Por último, implementar `calc_tf_idf(tf, idf)`. Esta debe cumplir que \n","\n","$$tf{-}idf = (0.5 + 0.5*tf_{i,j}) * idf_{i}$$"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-25T17:30:58.604427Z","start_time":"2020-03-25T17:30:58.600411Z"},"id":"L6QhMZl2NCH2"},"source":["def calc_tf_idf(tf, idf):\n","    return tf * idf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-25T17:30:59.501529Z","start_time":"2020-03-25T17:30:59.490530Z"},"id":"7pjDkfB4NCH5","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"ok","timestamp":1588036430419,"user_tz":240,"elapsed":612,"user":{"displayName":"FeedTheTrees","photoUrl":"","userId":"16064772018428921502"}},"outputId":"7006621e-9677-487d-db74-e6a8e3976444"},"source":["tf_idf = calc_tf_idf(tf, idf)\n","tf_idf"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>t4</th>\n","      <th>t3</th>\n","      <th>t1</th>\n","      <th>t5</th>\n","      <th>t2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.079181</td>\n","      <td>0.088046</td>\n","      <td>0.088046</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.039591</td>\n","      <td>0.088046</td>\n","      <td>0.000000</td>\n","      <td>0.778151</td>\n","      <td>0.039591</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.079181</td>\n","      <td>0.000000</td>\n","      <td>0.088046</td>\n","      <td>0.000000</td>\n","      <td>0.039591</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.039591</td>\n","      <td>0.176091</td>\n","      <td>0.088046</td>\n","      <td>0.000000</td>\n","      <td>0.039591</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.039591</td>\n","      <td>0.000000</td>\n","      <td>0.088046</td>\n","      <td>0.000000</td>\n","      <td>0.079181</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.000000</td>\n","      <td>0.176091</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.079181</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         t4        t3        t1        t5        t2\n","0  0.079181  0.088046  0.088046  0.000000  0.000000\n","1  0.039591  0.088046  0.000000  0.778151  0.039591\n","2  0.079181  0.000000  0.088046  0.000000  0.039591\n","3  0.039591  0.176091  0.088046  0.000000  0.039591\n","4  0.039591  0.000000  0.088046  0.000000  0.079181\n","5  0.000000  0.176091  0.000000  0.000000  0.079181"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"nNAh338lNCH7"},"source":["**Bonus: Hacer una función para realizar una Query (2 puntos extra)**\n","\n","`query(sentence, tf_idf)` función debe retornar un ranking con los documentos mas relevantes para la palabra consultada. \n","\n","Sugerencias:\n","\n","- Primero, transformar la oración a un documento del modelo tf_idf (ojo que solo funciona cuando todas las palabras de la oración existen en el modelo que acabas de crear).\n","- Luego, usar similitud coseno para comparar los documentos ya precalculados y la oración que acabas de consultar.\n","- A partir de eso, crear un ranking con los documentos mas similares.\n","\n","Deben reportar por lo menos 2 ejemplos.\n","\n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-25T17:24:08.566885Z","start_time":"2020-03-25T17:24:08.483Z"},"id":"rl1ewEGpNCH7"},"source":["def query(word, tf_idf):\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-20T15:20:26.831768Z","start_time":"2020-03-20T15:20:26.800521Z"},"id":"VJsnizXPNCH9"},"source":["**Usar dato reales (Opcional, sin puntaje)**\n","\n","Adicionalmente, existe la alternativa de utilizar un dataset de noticias:\n","\n","El modelo que creaste anteriormente no debería dejar de funcionar si cargas este dataset."]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-23T14:40:54.051703Z","start_time":"2020-03-23T14:40:53.993Z"},"id":"TCfXYkwONCH-"},"source":["import pandas as pd\n","import numpy as np\n","import re\n","\n","BASE_URL = 'https://github.com/dccuchile/CC6205/releases/download/Data/{}.json'\n","\n","dataset = pd.concat([\n","    pd.read_json(BASE_URL.format('biobio_nacional')),\n","    pd.read_json(BASE_URL.format('biobio_opinion')),\n","    pd.read_json(BASE_URL.format('biobio_internacional')),\n","    pd.read_json(BASE_URL.format('biobio_sociedad')),\n","    pd.read_json(BASE_URL.format('biobio_economia'))\n","])\n","\n","\n","def clean_doc(doc):\n","    return ' '.join(\n","        list(\n","            filter(\n","                lambda x: x != '',\n","                re.sub(\"\\?|\\¿|\\:|\\!|\\¡|\\(|\\)|\\t|\\n|“|”|\\\"|\\'|\\s\\s+|\\.|\\,|\\;\",\n","                       \" \", doc.lower()).split(' '))))\n","\n","\n","dataset = list(\n","    map(lambda x: clean_doc(x[0] + '.' + x[1]), dataset[['title',\n","                                                         'content']].values))"]}]}