{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"minitarea2_solucion.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2obO44rRIDIm","colab_type":"text"},"source":["**NOTA**: Las respuestas no necesariamente deberian ser literales\n","\n","# Minitarea 2\n","\n","Nombre:"]},{"cell_type":"markdown","metadata":{"id":"JstKx3TiKcIp","colab_type":"text"},"source":["---------------------------\n","## Language Models (3 pts)\n","Estas preguntas son teóricas y deben ser resueltas con desarrollo, pero sin código. Por favor usen $\\LaTeX$ para las fórmulas. El material del curso y los videos deberian ser suficientes para que puedan responder."]},{"cell_type":"markdown","metadata":{"id":"2hwW-07MrRpt","colab_type":"text"},"source":["\n","### Pregunta 1 (1 pt)\n","Asuma que tiene un modelo de lenguaje de trigramas (simple) entrenado sobre un corpus C. Muestre cómo el modelo le asigna una probabilidad a la oración `el perro le ladra al gato` usando los parámetros estimados a partir del corpus  $q(w_i | w_{i-2}, w_{i-1})$ ."]},{"cell_type":"markdown","metadata":{"id":"5VUjDx0NrYbg","colab_type":"text"},"source":["**Respuesta:**\n","\n","$ \n","\\begin{equation}\n","\\begin{split}\n","P(el\\: perro\\: le\\: ladra\\: al\\: gato\\: STOP) & = q(el|*, *) \\\\\n","                                                & \\times q(perro|*, el)\\\\\n","                                                & \\times q(le|el, perro)\\\\\n","                                                & \\times q(ladra|perro, le)\\\\\n","                                                & \\times q(al|le, ladra) \\\\\n","                                                & \\times q(gato|ladra, al)\\\\\n","                                                & \\times q(STOP|al, gato)\n","\\end{split}\n","\\end{equation}\n","$"]},{"cell_type":"markdown","metadata":{"id":"bwNkPIXure0C","colab_type":"text"},"source":["### Pregunta 2 (1 pt)\n","Muestre cómo se calcularía  $q(w_{i} | w_{i-2}, w_{i-1})$  usando un modelo que interpola un modelo de lenguajes de trigramas, bigramas, y unigrama.\n"]},{"cell_type":"markdown","metadata":{"id":"zeLZAd0Tr9ne","colab_type":"text"},"source":["**Respuesta:**\n","\n","$\n","\\begin{split}\n","q(w_{i} | w_{i-2}, w_{i-1}) & = \\lambda_1 \\times q_{ML}(w_{i} | w_{i-2}, w_{i-1})\\\\\n","                            & + \\lambda_2 \\times q_{ML}(w_{i} | w_{i-1}) \\\\\n","                            & + \\lambda_3 \\times q_{ML}(w_i)\n","\\end{split}\n","$\n","\n","Donde $\\lambda_1$, $\\lambda_2$ y $\\lambda_3$ son parametros calculados usando un set de validacion."]},{"cell_type":"markdown","metadata":{"id":"sHqcRJ7Vr_8x","colab_type":"text"},"source":["### Pregunta 3 (1 pt)\n","¿Qué ventajas tiene el modelo interpolado sobre el modelo de trigramas simple?"]},{"cell_type":"markdown","metadata":{"id":"6F5R3Ji7sHjt","colab_type":"text"},"source":["**Respuesta:** (esta respuesta es mas abierta)\n","\n","idea: los trigramas son muy escasos en un corpus de training porque hay demasiadas posibilidades, entonces muchos trigramas no aparecen en el corpus de entrenamiento y se vuelven 0 para estimaciones futuras. Por esto, se usan estimadores de maxima verosimilitud condicionados por menos contexto (o ninguno). Estos estimadores son mas robustos, en el sentido de que es menos probable que las ocurrecians de bigramas o unigramas sean cero en un corpus razonable, pero se ignora la informacion provista por el contexto para realizar la estimacion.\n","\n","Al usar un modelo interpolado, se obtienen ventajas de ambas aproximaciones, el modelo es capaz de condicionar con respecto al contexto, pero la probabilidad no se vuelve 0 cuando el trigrama no aparecio en el set de entrenamiento.\n"]},{"cell_type":"markdown","metadata":{"id":"rdmB-07ZKfaa","colab_type":"text"},"source":["-----------------------\n","## Naive Bayes (3 pts)\n","En esta parte de la minitarea deberan programar Naive Bayes Multinomial usando Laplace Smothing. Las referencias las pueden encontrar en el material del curso y los videos del profesor.\n","\n","A continuacion se presentan un conjunto de documentos de training divididos en 2 categorias distintas. Ustedes deberan clasificar los documentos del test set usando Naive Bayes con Laplace Smothing.\n","\n","Por favor, documenten su codigo. Escriban que hacen las funciones, que reciben, que entregan, etc. Si en algun lugar de su codigo hacen algo que creen que debe ser explicado, pongan comentarios, son bienvenidos. \n","\n","\n","**Underflow prevention:** En vez de hacer muchas multiplicacions de `float`s, reemplacenlas por sumas de logaritmos para prevenir errores de precision. Revisen la diapo 69 de las slides. \n","\n","NOTA: Sobre las `namedtuple`s. Es el tipo de los documentos. Son objetos inmutable que tienen dos atributos: `words` donde estan las palabras del documento y `class_` donde se guarda la clase de ese documento. Estos objetos son inmutables, lo que quiere decir que si quieren modificar un documento y cambiarle la clase, tienen que crear otro documento. Otra cosa es que son tuplas como cualquier otra, es decir se pueden acceder usando indices como `doc[0]` o `doc[1]`. Son libres de implementar su solucion como quieran, si no les gusta este tipo de dato usen otro.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"904e5b75-6a4a-4be8-ffa1-5a8ab18ca05a","id":"HLi8PxdV2VQX","executionInfo":{"status":"ok","timestamp":1590555426892,"user_tz":240,"elapsed":1079,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}},"colab":{"base_uri":"https://localhost:8080/","height":225}},"source":["from collections import namedtuple\n","from pprint import pprint\n","\n","\n","document = namedtuple(\n","    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",")\n","\n","train_set = (\n","    document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n","    document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n","    document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n","    document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n","    document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n","    document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=1),\n","    document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n","    document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1)\n",")\n","print(\"Train documents:\")\n","pprint(train_set)\n","\n","\n","test_set = (document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),)\n","print(\"\\nTest documents:\")\n","pprint(test_set)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Train documents:\n","(document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n"," document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n"," document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n"," document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n"," document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n"," document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=1),\n"," document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n"," document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1))\n","\n","Test documents:\n","(document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0edu0E7LA3U9","colab_type":"code","outputId":"12b3ef37-bea9-49da-ef34-3b207ee83253","executionInfo":{"status":"ok","timestamp":1590555431039,"user_tz":240,"elapsed":1373,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Tu respuesta\n","from itertools import chain\n","from functools import partial, lru_cache\n","from math import log\n","\n","\n","NUM_CLASSES = len({doc.class_ for doc in train_set})\n","\n","# Function to get de prior of a class\n","# Count the documents in a class divided by the total document count\n","@lru_cache()\n","def class_prior(documents, class_):\n","    return len([doc for doc in documents if doc.class_ == class_]) / len(\n","        documents\n","    )\n","\n","\n","# Function to get a word likelihood given a class\n","@lru_cache()\n","def word_likelihood(documents, word, class_):\n","    vocab = {word for doc in documents for word in doc.words}\n","    superdocument = list(\n","        chain.from_iterable(\n","            [doc.words for doc in documents if doc.class_ == class_]\n","        )\n","    )\n","    return (superdocument.count(word) + 1) / (len(superdocument) + len(vocab))\n","\n","\n","# Hago los calculos lazy\n","trained_class_prior = partial(class_prior, train_set)\n","trained_word_likelihood = partial(word_likelihood, train_set)\n","\n","\n","def predict(documents):\n","    predicted_documents = []\n","    for doc in documents:\n","        # implemented using log sum instead of multiplication\n","        class_probabilities = [\n","            log(trained_class_prior(class_))\n","            + sum(\n","                [\n","                    log(trained_word_likelihood(word, class_))\n","                    for word in doc.words\n","                ]\n","            )\n","            for class_ in range(NUM_CLASSES)\n","        ]\n","        predicted_documents.append(\n","            document(\n","                doc.words, class_probabilities.index(max(class_probabilities))\n","            )\n","        )\n","    return predicted_documents\n","\n","\n","predicted = predict(test_set)\n","for doc in predicted:\n","    print(f\"La clase predicha del documento {doc.words} es: {doc.class_}\")\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["La clase predicha del documento ('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03') es: 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tC-2qgVhfeZL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"0b9eca96-15b7-4664-dfbc-c4a089f11909","executionInfo":{"status":"ok","timestamp":1590562563513,"user_tz":240,"elapsed":1159,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}}},"source":["for word in test_set[0].words:\n","    print(word, trained_word_likelihood(word, 1))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["w02 0.08333333333333333\n","w09 0.125\n","w06 0.125\n","w01 0.08333333333333333\n","w05 0.16666666666666666\n","w04 0.08333333333333333\n","w03 0.020833333333333332\n","w03 0.020833333333333332\n"],"name":"stdout"}]}]}