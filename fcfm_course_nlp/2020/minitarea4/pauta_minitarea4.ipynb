{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pauta_minitarea4.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"X4lL5hGw07yP","colab_type":"text"},"source":["# Minitarea 4\n","\n","Por favor, en las respuestas de desarrollo expliquen lo que están haciendo. En las preguntas que son con desarrollo matemático pongan todos los pasos del cálculo (déjenlo bonito porfis :D).\n","\n","Usen $\\LaTeX$ para las fórmulas matemáticas.\n","\n","En la parte de programación pueden usar lo que quieran, pero la auxiliar 3 les puede servir de *inspiración*."]},{"cell_type":"markdown","metadata":{"id":"bWXD3D7RYKJ-","colab_type":"text"},"source":["# Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMM) and Conditional Random Field(CRF)\n","\n","### Pregunta 1 (1 pt)\n","Para un problema de POS tagging se define el conjunto de etiquetas $S = \\{ D, N, V, P \\}$ y se tiene un Hidden Markov Model con los siguientes parámetros estimados a partir de un corpus de entrenamiento:\n","\n","\\begin{equation}\n","\\begin{split}\n","q(D|N,P) &= 0.4 \\\\\n","q(D|w,P) &= 0 \\qquad \\forall w \\in S, w \\neq N \\\\\n","e(the|D) &= 0.6\n","\\end{split}\n","\\end{equation}\n","\n","Luego para la oración: `Ella walks to the red house`, se tiene una tabla de programación dinámica con los siguientes valores:\n","\\begin{equation}\n","\\begin{split}\n","\\pi(3,D,P)&=0.1\\\\\n","\\pi(3,N,P)&=0.2\\\\\n","\\pi(3,V,P)&=0.01\\\\\n","\\pi(3,P,P)&=0.5\n","\\end{split}\n","\\end{equation}\n","\n","Con esta información, calcule el valor de $\\pi(4,P,D)$. Puede dejar el resultado expresado como una fracción.\n"]},{"cell_type":"markdown","metadata":{"id":"5EzgysW9kGi-","colab_type":"text"},"source":["**Respuesta**\n","\n","Usamos la formula vista en clases\n","\\begin{equation}\n","\\pi(k,u,v) = \\max_{w \\in S_{k-2}} \\left ( \\pi(k-1,w,u) \\times q(v|w,u) \\times e(x_k|v) \\right) \\\\\n","\\forall k \\in \\{ 1 \\dots n \\}, \\forall u \\in S_{k-1}, \\forall v \\in S_k\n","\\end{equation}\n","\n","Reemplazando\n","\\begin{equation}\n","\\pi(4,P,D) = \\max_{w\\in S_2} \\left ( \\pi(3,w,P) \\times q(D|w,P) \\times e(the|D) \\right )\n","\\end{equation}\n","\n","Como $q(D|w, P) = 0, \\forall w \\neq N$, tenemos que $w = N$\n","\n","\\begin{equation}\n","\\begin{split}\n","\\pi(4,P,D) &= \\pi(3,N,P) \\times q(D|N,P) \\times e(the|D) \\\\\n","\\pi(4,P,D) &= 0.2 \\times 0.4 \\times 0.6 \\\\\n","\\pi(4,P,D) &= 0.048\n","\\end{split}\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"oiwJb_vmkKLZ","colab_type":"text"},"source":["### Pregunta 2 (0.5 pts)\n","Comente  sobre las similitudes o diferencias entre los HMMs, MEMMs y CRFs. Para esto, responda las siguientes preguntas.\n","* ¿Para qué tipo de tarea sirven? Dé dos ejemplo de este tipo de tarea y descríbalos brevemente.\n","* ¿Qué modelos usan features? ¿Qué ventajas conlleva esto?\n","* ¿Cómo maneja cada uno de los modelos las palabras con baja frecuencia en el set de train?\n","* ¿Qué le permite a los CRF realizar decisiones globales? ¿Qué diferencia con respecto a los MEMMs permite lograr esto? ¿Por qué los HMMs tampoco son capaces de tomar decisiones globales?"]},{"cell_type":"markdown","metadata":{"id":"P9h5ow8OWF7y","colab_type":"text"},"source":["**Respuesta**\n","\n","**NOTA:** Esta pregunta es un poco más abierta y puede que los alumnos incluyan cosas que no salen aquí. Siempre y cuando sean coherentes se le puede dar puntaje por eso.\n","\n","* Se ocupan para tareas de *sequence labeling*. Dos ejemplos son Named Entity Recognition (NER) y Part Of Speech tagging (POS). NER corresponde a encontrar \"entidades\" en el texto, como nombres de lugares, personas, etc. y POS corresponde a asignarle tags a las palabras como adjetivo, verbo, sustantivo, etc.\n","\n","* Los MEMMs y CRFs ocupan features, es decir un científico diseña un vector de *features* o características para la tarea específica que serán utilizados. Esto se diferencia de los HMM que son simples conteos, por lo que es posible generalizar mejor y evitar el problema de cadenas inexistentes en el dataset de training que tienen los modelos basados en conteo.\n","\n","* Para los HMM, es necesario mapear las palabras poco frecuentes a un conjunto pequeño de categorias escogidas a mano, basándose en propiedades de las palabras como la capitalización, si son dígitos, las terminaciones, etc. Por otro lado, la estrategia basada en *features* no tiene este problema, ya que siempre una palabra va a poder ser representada usando el vector de características.\n","\n","* Los CRF pueden tomar decisiones globales porque su vectores de features esta formado por la suma de los vectores de features de cada uno de los tokens de la frase, entonces el vectores de features de un CRF mapea la secuencia completa junto con la secuencia de tags a un vector. Por el contrario, el vector de features de un token utilizado en las MEMMs no ocupa información sobre toda la secuencia de tags, solo condiciona en base a la secuencia de palabras, la posición que se esta tagueando, el tag actual y el tag anterior, no toda la secuencia de tags. Los HMMs tampoco pueden tomar decisiones globales porque tampoco condicionan en base a toda la secuencia de tags."]},{"cell_type":"markdown","metadata":{"id":"ClRAHR95Y8aB","colab_type":"text"},"source":["# Convolutional Neural Networks\n","### Pregunta 3 (1 pt)\n","\n","Considere la frase $w_{1..7}=$ `El agua moja y el fuego quema` $=[El, agua, moja, y, el, fuego, quema]$.\n","\n","La siguiente matriz de embeddings, donde la i-ésima fila corresponde al vector de embedding de la i-ésima palabra, ordenadas segun aparecen en la frase. (vectores de largo 2).\n","\\begin{equation}\n","E = \\begin{pmatrix}\n","2 & 2\\\\\n","0 & -2\\\\\n","0 & 1\\\\\n","-2 & 1\\\\\n","-2 & 0\\\\\n","2 & -1\\\\\n","0 & 2\n","\\end{pmatrix}\n","\\end{equation}\n","\n","Los siguientes 3 filtros\n","\\begin{equation}\n","U = \\begin{pmatrix}\n","-1 & -1 & 0\\\\\n","1 & 1 & 0\\\\\n","0 & 0 & -1\\\\\n","-1 & -1 & -1\\\\\n","-1 & -1 & 1\\\\\n","1 & 0 & -1\n","\\end{pmatrix}\n","\\end{equation}\n","\n","Y la función de activación\n","\\begin{equation}\n","tanh = \\frac{e^{2x} - 1}{e^{2x} + 1}\n","\\end{equation}\n","\n","Usando estos paramátros calcule manualmente la representación (vector) resultante de aplicar la operación de convolución (sin padding) + max pooling.\n"]},{"cell_type":"markdown","metadata":{"id":"SlQ30Arkq0u4","colab_type":"text"},"source":["**Respuesta**\n","\n","\\begin{equation}\n","\\begin{pmatrix}\n","el & agua & moja \\\\\n","agua & moja & y \\\\\n","moja & y & el \\\\\n","y & el & fuego \\\\\n","el & fuego & quema\n","\\end{pmatrix}\n","\\rightarrow\n","\\begin{pmatrix}\n","2 & 2 & 0 & -2 & 0 & 1 \\\\\n","0 & -2 & 0 & 1 & -2 & 1 \\\\\n","0 & 1 & -2 & 1 & -2 & 0 \\\\\n","-2 & 1 & -2 & 0 & 2 & -1 \\\\\n","-2 & 0 & 2 & -1 & 0 & 2\n","\\end{pmatrix}\n","\\\\\n","\\vec{x}_i \\cdot U \\rightarrow\n","\\begin{pmatrix}\n","3 & 2 & 1 \\\\\n","0 & -1 & -4 \\\\\n","2 & 2 & -1 \\\\\n","0 & 1 & 5 \\\\\n","5 & 3 & -3\n","\\end{pmatrix}\n","\\rightarrow tanh(x) \\rightarrow\n","\\begin{pmatrix}\n","1.00 & 0.96 & 0.76 \\\\\n","0.00 & -0.76 & -1.00 \\\\\n","0.96 & 0.96 & -0.76 \\\\\n","0.00 & 0.76 & 1.00 \\\\\n","1.00 & 1.00 & -1.00\n","\\end{pmatrix} \\\\\n","maxpooling \\rightarrow\n","\\begin{pmatrix}\n","1.00 & 1.00 & 1.00\n","\\end{pmatrix}\n","\\end{equation}\n","\n","~qué fome jdkajsdkjq~"]},{"cell_type":"markdown","metadata":{"id":"tj1V_sAzZCHY","colab_type":"text"},"source":["# Recurrent Neural Networks\n","### Pregunta 4 (1 pt)\n","Sea la siguiente oración: `El perro ladra` representada como una secuencia de vectores $x1,x2,x3$. Donde cada vector corresponde al word embedding de cada palabra, que a la vez es un vector de dos dimensiones\n","\\begin{equation}\n","\\begin{split}\n","x1 &= Embed(El)    &= [1, 0] \\\\\n","x2 &= Embed(perro) &= [-1, 1] \\\\\n","x3 &= Embed(ladra) &= [1,1]\n","\\end{split}\n","\\end{equation}\n","\n","Se tiene una red recurrente Elman: \n","\\begin{equation}\n","\\begin{split}\n","\\vec{s}_i &= R_{SRNN}\\left (\\vec{x}_i, \\vec{s}_{i-1}\\right ) = g \\left (\\vec{s}_{i-1}W^s + \\vec{x}_i W^x + \\vec{b}\\right ) \\\\\n","\\vec{y}_i &= O_{SRNN}\\left(\\vec{s}_i\\right) = \\vec{s}_i \\\\\n","\\end{split}\n","\\end{equation}\n","Con\n","\\begin{equation}\n","\\vec{s}_i, \\vec{y}_i \\in \\mathbb{R}^{d_s}, \\quad \\vec{x}_i \\in \\mathbb{R}^{d_x}, \\quad W^x \\in \\mathbb{R}^{d_x \\times d_s}, \\quad W^s \\in \\mathbb{R}^{d_s \\times d_s}, \\quad \\vec{b} \\in \\mathbb{R}^{d_s}\n","\\end{equation}\n","y donde los vectores de estado $s_i$ son de tres dimensiones, $ds= 3$.\n","\n","Sea\n","\\begin{equation}\n","\\begin{split}\n","\\vec{s}_0 &= [0,0,0]\\\\\n","W^x &= \\begin{pmatrix}\n","0 &  0 & 1\\\\\n","1 & -1 & 0\n","\\end{pmatrix} \\\\\n","W^s &= \\begin{pmatrix}\n","1 & 0 &  1\\\\\n","0 & 1 & -1\\\\\n","1 & 1 &  1\n","\\end{pmatrix} \\\\\n","\\vec{b} &= [0, 0, 0] \\\\\n","g(x) &= ReLu(x) = max(0, x)\n","\\end{split}\n","\\end{equation}\n","\n","<br>\n","\n","Calcule manualmente los valores de $\\vec{s}_1, \\vec{s}_2,\\vec{s}_3$ y de $\\vec{y}_1, \\vec{y}_2,\\vec{y}_3$."]},{"cell_type":"markdown","metadata":{"id":"7M7sqIQV-Q3a","colab_type":"text"},"source":["**Respuesta**\n","\n","\\begin{equation}\n","\\vec{s}_1 = g \\left ( \\vec{s}_0 W^s + \\vec{x}_1 W^x + \\vec{b} \\right ) \\\\\n","\\vec{s}_1 = ReLu \\left (\n","\\begin{pmatrix}\n","0 & 0 & 0\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","1 & 0 &  1\\\\\n","0 & 1 & -1\\\\\n","1 & 1 &  1\n","\\end{pmatrix}\n","+\n","\\begin{pmatrix}\n","1 & 0\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","0 &  0 & 1\\\\\n","1 & -1 & 0\n","\\end{pmatrix}\n","+\n","\\begin{pmatrix}\n","0 & 0 & 0\n","\\end{pmatrix}\n","\\right ) \\\\\n","\\vec{s}_1 = ReLu \\left ( \\begin{pmatrix}0 & 0 & 0\\end{pmatrix} + \\begin{pmatrix}0 & 0 & 1\\end{pmatrix} + \\begin{pmatrix}0 & 0 & 0\\end{pmatrix} \\right ) \\\\\n","\\vec{s}_1 = ReLu \\left ( \\begin{pmatrix}0 & 0 & 1\\end{pmatrix} \\right ) \\\\\n","\\vec{s}_1 = \\begin{pmatrix}0 & 0 & 1\\end{pmatrix}\\\\\n","\\vec{y}_1 = \\vec{s}_1\n","\\end{equation}\n","<br>\n","\n","\\begin{equation}\n","\\vec{s}_2 = g \\left ( \\vec{s}_1 W^s + \\vec{x}_2 W^x + \\vec{b} \\right ) \\\\\n","\\vec{s}_2 = ReLu \\left (\n","\\begin{pmatrix}\n","0 & 0 & 1\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","1 & 0 &  1\\\\\n","0 & 1 & -1\\\\\n","1 & 1 &  1\n","\\end{pmatrix}\n","+\n","\\begin{pmatrix}\n","-1 & 1\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","0 &  0 & 1\\\\\n","1 & -1 & 0\n","\\end{pmatrix}\n","\\right ) \\\\\n","\\vec{s}_2 = ReLu \\left ( \\begin{pmatrix}1 & 1 & 1\\end{pmatrix} + \\begin{pmatrix}1 & -1 & -1\\end{pmatrix} \\right ) \\\\\n","\\vec{s}_2 = ReLu \\left ( \\begin{pmatrix}2 & 0 & 0\\end{pmatrix} \\right ) \\\\\n","\\vec{s}_2 = \\begin{pmatrix}2 & 0 & 0\\end{pmatrix}\\\\\n","\\vec{y}_2 = \\vec{s}_2\n","\\end{equation}\n","<br>\n","\n","\\begin{equation}\n","\\vec{s}_3 = g \\left ( \\vec{s}_2 W^s + \\vec{x}_3 W^x + \\vec{b} \\right ) \\\\\n","\\vec{s}_3 = ReLu \\left (\n","\\begin{pmatrix}\n","2 & 0 & 0\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","1 & 0 &  1\\\\\n","0 & 1 & -1\\\\\n","1 & 1 &  1\n","\\end{pmatrix}\n","+\n","\\begin{pmatrix}\n","1 & 1\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","0 &  0 & 1\\\\\n","1 & -1 & 0\n","\\end{pmatrix}\n","\\right ) \\\\\n","\\vec{s}_3 = ReLu \\left ( \\begin{pmatrix}2 & 0 & 2\\end{pmatrix} + \\begin{pmatrix}1 & -1 & 1\\end{pmatrix} \\right ) \\\\\n","\\vec{s}_3 = ReLu \\left ( \\begin{pmatrix}3 & -1 & 3\\end{pmatrix} \\right ) \\\\\n","\\vec{s}_3 = \\begin{pmatrix}3 & 0 & 3\\end{pmatrix}\\\\\n","\\vec{y}_3 = \\vec{s}_3\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"W4rAT6ELxRZW","colab_type":"text"},"source":["### Pregunta 5 (0.5 pts)\n","¿De qué forma las RNN y las CNN logran aprender representaciones específicas\n","para la tarea objetivo? Compare la forma en que las RNN y las CNN aprenden con los modelos que usan *features* diseñadas manualmente."]},{"cell_type":"markdown","metadata":{"id":"b6AXbQSgA_t8","colab_type":"text"},"source":["**Respuesta**\n","\n","**NOTA:** nuevamente una respuesta relativamente abierta, corregir con criterio.\n","\n","Como las redes son entrenadas desde 0 y dándoles toda la flexibilidad, los parámetros se ajustan a la tarea específica en la que están siendo entrenados. De esta forma una red convolucional \"aprende\" a darle más importancia a ciertos n-gramas dependiendo de la tarea específica, y una red recurrente es capaz de condicionar un resultado en base a toda la secuencia, dándole más importancia a ciertas partes de la frase dependiendo de la tarea. Además, con arquitecturas como la LSTM es posible incluso que la red aprenda a ignorar ciertas partes del input ya que no son importantes para la tarea.\n","\n","Por otro lado, cuando se diseña *features* de forma manual, la red está limitada a la expresividad que tienen las features que fueron escogidas manualmente por el experto, pero es perfectamente posible que esas características funcionen mejor para algunas tareas que para otras, o que la elección de features simplemente no sea la óptima."]},{"cell_type":"markdown","metadata":{"id":"qxQIuO8axTUa","colab_type":"text"},"source":["# Redes neuronales con PyTorch\n","### Pregunta 6 (2 pts)\n","En esta parte van a tener que implementar una red neuronal Feed Forward. Además, deberán entrenar el modelo usando uno de los datasets de TorchText. En la sección de la respuesta hay un esqueleto de lo que deben hacer, deberán completar los metodos del modelo y la parte asociada al entrenamiento la deben implementar ustedes. De todas formas, como les mencionamos en las auxiliares, el proceso de entrenamiento es bastante estándar, así que se pueden guiar en gran medida por los ejemplos que hemos visto y los que vamos a ver en las próximas auxiliares.\n","\n","### Bonus (0.5 pts)\n","Agregue a la arquitectura una capa convolucional, para esto debe registrar el parametro $U$ en la red y realizar el computo de la convolución en el metodo forward de la red.\n"]},{"cell_type":"code","metadata":{"id":"LVKEaQXZ3eGl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593755766534,"user_tz":240,"elapsed":7387,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}}},"source":["%%capture\n","# Nos aseguramos que torchtext este en la ultima version\n","!pip install --upgrade torchtext"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJ-wrzFO5mCC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1593755791045,"user_tz":240,"elapsed":31889,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}},"outputId":"b127f3e7-c400-4ea0-b0d2-3df263f74906"},"source":["# Trabajen con el siguiente dataset\n","import os\n","from random import choice\n","from torchtext.datasets import AG_NEWS\n","\n","os.makedirs(\".data\", exist_ok=True)\n","train_dataset, test_dataset = AG_NEWS(root=\".data\")\n","\n","print(\"\\nUn ejemplo aleatorio\\n\", choice(train_dataset))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["ag_news_csv.tar.gz: 11.8MB [00:00, 45.7MB/s]\n","120000lines [00:05, 22916.93lines/s]\n","120000lines [00:10, 11102.35lines/s]\n","7600lines [00:00, 11789.40lines/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Un ejemplo aleatorio\n"," (2, tensor([  850,  1341,   739, 25796,   149,  1628,  1575,  4382,   527,   226,\n","            7, 10049,   125,    20,   463,   100,   979,     4,   711,    93,\n","            9,   812,  1848,  3971,   862,     2,    25,  9600,   902,    13,\n","        37565,     2]))\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"wSHn-hYi8XN8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1593755791048,"user_tz":240,"elapsed":31883,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}},"outputId":"819becfe-6168-4290-850e-182eaa227bd0"},"source":["from pprint import pprint\n","# Informacion relevante del dataset\n","vocab = train_dataset.get_vocab()\n","labels = train_dataset.get_labels()\n","# El nombre de cada label lo pueden encontrar aqui\n","# https://pytorch.org/text/datasets.html#ag-news, aunque no es necesario para \n","# clasificar\n","print(labels)\n","# Un ejemplo convertido a texto\n","pprint(\" \".join(vocab.itos[token] for token in choice(train_dataset)[1]))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["{0, 1, 2, 3}\n","('hilfiger sales and net fall amid inquiry more bad news poured out of the '\n"," 'tommy hilfiger corporation yesterday , as the clothing company - facing an '\n"," 'investigation by the united states attorney #39 s office in manhattan - '\n"," 'reported disappointing sales and declining operating margins .')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IXngUm9HxKvA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593755791051,"user_tz":240,"elapsed":31878,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}}},"source":["# PAUTA: esto es literalmente la auxiliar 3, asique voy a copiar y pegar\n","\n","# De aca para abajo viene su respuesta, completen las funciones en la red\n","# y luego entrenen el modelo y evaluenlo usando los dataset que acaban de\n","# cargar\n","import torch\n","import torch.nn as nn\n","\n","\n","class ArgumentClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, num_class, pad_idx): # Reemplacen el *args por sus argumento\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim, pad_idx)\n","        self.fc = nn.Linear(embed_dim, num_class)\n","\n","    def forward(self, batch):\n","        return self.fc(self.embedding(batch).mean(dim=1))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPHyqgId96ra","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593755791052,"user_tz":240,"elapsed":31874,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}}},"source":["from itertools import zip_longest\n","\n","def generate_batch(batch):\n","    return (\n","        torch.tensor([item[0] for item in batch]),\n","        torch.tensor(\n","            list(\n","                zip(\n","                    *zip_longest(\n","                        *[item[1] for item in batch], fillvalue=vocab[\"<pad>\"]\n","                    )\n","                )\n","            )\n","        ),\n","    )"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"1SQiAy5lNaJ9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593755791055,"user_tz":240,"elapsed":31869,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}}},"source":["from torch.utils.data import DataLoader\n","\n","\n","def train_func(train_dataset):\n","    train_loss = 0\n","    train_acc = 0\n","    data = DataLoader(\n","        train_dataset,\n","        batch_size=BATCH_SIZE,\n","        shuffle=True,\n","        collate_fn=generate_batch,\n","    )\n","    for i, (cls, text) in enumerate(data):\n","        optimizer.zero_grad()\n","        cls, text = cls.to(device), text.to(device)\n","        output = model(text)\n","        loss = criterion(output, cls)\n","        train_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        train_acc += (output.argmax(1) == cls).sum().item()\n","\n","    return train_loss / len(train_dataset), train_acc / len(train_dataset)\n","\n","\n","def test(test_dataset):\n","    test_loss = 0\n","    acc = 0\n","    data = DataLoader(\n","        test_dataset, batch_size=BATCH_SIZE, collate_fn=generate_batch\n","    )\n","    for cls, text in data:\n","        cls, text = cls.to(device), text.to(device)\n","        with torch.no_grad():\n","            output = model(text)\n","            loss = criterion(output, cls)\n","            test_loss += loss.item()\n","            acc += (output.argmax(1) == cls).sum().item()\n","\n","    return test_loss / len(test_dataset), acc / len(test_dataset)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"15aReapkNt92","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":278},"executionInfo":{"status":"ok","timestamp":1593755937204,"user_tz":240,"elapsed":88262,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}},"outputId":"fdc7b0ce-257f-4ef0-dc42-b1e408544887"},"source":["import time\n","from torch.utils.data.dataset import random_split\n","\n","N_EPOCHS = 5\n","LEARN_RATE = 4.0\n","STEP_SIZE = 1\n","BATCH_SIZE = 128\n","EMBED_DIM = 100\n","\n","train_len = int(len(train_dataset) * 0.8)\n","train_dataset, validation_dataset = random_split(\n","    train_dataset, [train_len, len(train_dataset) - train_len]\n",")\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","model = ArgumentClassifier(\n","    vocab_size=len(vocab),\n","    embed_dim=EMBED_DIM,\n","    num_class=len(labels),\n","    pad_idx=vocab[\"<pad>\"],\n",").to(device)\n","\n","criterion = torch.nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARN_RATE)\n","\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    train_loss, train_acc = train_func(train_dataset)\n","    valid_loss, valid_acc = test(validation_dataset)\n","\n","    secs = int(time.time() - start_time)\n","    mins = secs // 60\n","    secs = secs % 60\n","\n","    print(\n","        f\"Epoch: {epoch + 1}\", f\" | time in {mins} minutes, {secs} seconds\",\n","    )\n","    print(\n","        f\"\\tLoss: {train_loss:.4f}(train)\\t|\"\n","        f\"\\tAcc: {train_acc * 100:.1f}%(train)\"\n","    )\n","    print(\n","        f\"\\tLoss: {valid_loss:.4f}(valid)\\t|\"\n","        f\"\\tAcc: {valid_acc * 100:.1f}%(valid)\"\n","    )\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch: 1  | time in 0 minutes, 17 seconds\n","\tLoss: 0.0074(train)\t|\tAcc: 61.8%(train)\n","\tLoss: 0.0055(valid)\t|\tAcc: 74.7%(valid)\n","Epoch: 2  | time in 0 minutes, 18 seconds\n","\tLoss: 0.0047(train)\t|\tAcc: 78.2%(train)\n","\tLoss: 0.0043(valid)\t|\tAcc: 81.4%(valid)\n","Epoch: 3  | time in 0 minutes, 17 seconds\n","\tLoss: 0.0039(train)\t|\tAcc: 82.7%(train)\n","\tLoss: 0.0037(valid)\t|\tAcc: 83.7%(valid)\n","Epoch: 4  | time in 0 minutes, 17 seconds\n","\tLoss: 0.0035(train)\t|\tAcc: 84.9%(train)\n","\tLoss: 0.0034(valid)\t|\tAcc: 85.2%(valid)\n","Epoch: 5  | time in 0 minutes, 17 seconds\n","\tLoss: 0.0032(train)\t|\tAcc: 86.3%(train)\n","\tLoss: 0.0032(valid)\t|\tAcc: 86.3%(valid)\n"],"name":"stdout"}]}]}