{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1XTS4XkCagRBqAc-9j4Dlya3EFwzhSXHM","timestamp":1714518009424},{"file_id":"1jvrpt6zgc-G5wWhtgcTqPmiKvl1wcvJ-","timestamp":1713283518140},{"file_id":"11hBKGIhHcrr2QYP9bxUsKVnXd5VBfrzg","timestamp":1712439760293}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Tarea 3: Sequence Labelling\n","**Procesamiento de Lenguaje Natural (CC6205-1 - Oto√±o 2024)**"],"metadata":{"id":"AxrkZWMLZB8z"}},{"cell_type":"markdown","source":["## Tarjeta de identificaci√≥n\n","\n","**Nombres:** ```<Completar aqu√≠>```\n","\n","**Fecha l√≠mite de entrega üìÜ:** 06/06.\n","\n","**Tiempo estimado de dedicaci√≥n:** 4 horas\n"],"metadata":{"id":"b97b4IJjZGxM"}},{"cell_type":"markdown","source":["## Instrucciones\n","Bienvenid@s a la tercera tarea en el curso de Natural Language Processing (NLP). Esta tarea tiene como objetivo evaluar los contenidos te√≥ricos de las √∫ltimas semanas de clases posteriores a la tarea 2, enfocado principalmente en **Word Embeddings**, **Sequence Labeling - HMM**, **Convolutional Neural Networks** y **Recurrent Neural Networks**. Si a√∫n no has visto las clases, se recomienda visitar los links de las referencias.\n","\n","La tarea consta de una una parte pr√°ctica con el f√≠n de introducirlos a la programaci√≥n en Python enfocada en NLP.\n","\n","* La tarea es en **grupo** (maximo hasta 3 personas).\n","* La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n","* El formato de entrega es este mismo Jupyter Notebook.\n","* Al momento de la revisi√≥n su c√≥digo ser√° ejecutado. Por favor verifiquen que su entrega no tenga errores de compilaci√≥n.\n","* Completar la tarjeta de identificaci√≥n. Sin ella no podr√° tener nota."],"metadata":{"id":"TKcZMFlmZ3b9"}},{"cell_type":"markdown","source":["## Material de referencia\n","\n","Diapositivas del curso üìÑ\n","    \n","- [Word Embeddings](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-wordvectors.pdf)\n","- [Sequence Labeling - HMM](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-HMM.pdf)\n","- [Convolutional Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-CNN.pdf)\n","- [Recurrent Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-RNN.pdf)\n","\n","Videos del curso üì∫\n","\n","- Word Embeddings: [Parte 1](https://www.youtube.com/watch?v=wtwUsJMC9CA), [Parte 2](https://www.youtube.com/watch?v=XDxzQ7JU95U), [Parte 3](https://www.youtube.com/watch?v=Ikyc3DRVodk)\n","\n","- Sequence Labeling - HMM: [Parte 1](https://www.youtube.com/watch?v=-ngfOZz8yK0), [Parte 2](https://www.youtube.com/watch?v=Tjgb-yQOg54), [Parte 3](https://www.youtube.com/watch?v=aaa5Qoi8Vco), [Parte 4](https://www.youtube.com/watch?v=4pKWIDkF_6Y)\n","\n","- [Convolutional Neural Networks](https://www.youtube.com/watch?v=lLZW5Fn40r8)\n","\n","- Recurrent Neural Networks: [Parte 1](https://www.youtube.com/watch?v=BmhjUkzz3nk), [Parte 2](https://www.youtube.com/watch?v=z43YFR1iIvk), [Parte 3](https://youtu.be/7L5JxQdwNJk)"],"metadata":{"id":"dnTrhOKraAw2"}},{"cell_type":"markdown","source":["## Objetivo\n","\n","El objetivo de esta tarea es resolver una de las tareas m√°s importantes en el √°rea del procesamiento de lenguage natural, relacionada con la extracci√≥n de informaci√≥n: [Named Entity Recognition (NER)](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf).\n","\n","En particular, deber√°n crear distintos modelos que apunten a resolver la tarea de NER en Espa√±ol. Para esto, les entregaremos un dataset real perteneciente a la lista de espera NO GES en Chile. Es importante destacar que existe una falta de trabajos realizados en el √°rea de NER en Espa√±ol y a√∫n m√°s en el contexto cl√≠nico, por ende puede ser considerado como una tarea bien desafiante y quiz√°s les interesa trabajar en el √°rea m√°s adelante en sus carreras.\n","\n","En este notebook les entregaremos un baseline como referencia de los resultados que esperamos puedan obtener. Recuerden que el no superar a los baselines en alguna de las tres m√©tricas conlleva un descuento de 0.5 puntos hasta 1.5 puntos.\n","\n","Como hemos estado viendo redes neuronales tanto en c√°tedras, tareas y auxiliares (o pr√≥ximamente lo har√°n), esperamos que (por lo menos) utilicen Redes Neuronales Recurrentes (RNN) para resolverla.\n","\n","Nuevamente, hay total libertad para utilizar el software y los modelos que deseen, siempre y cuando estos no traigan los modelos ya implementados (de todas maneras, como es un corpus nuevo, es dif√≠cil que haya alg√∫n modelo ya implementado con √©stas entidades)."],"metadata":{"id":"025o3RYmm3oM"}},{"cell_type":"markdown","source":["## Explicaci√≥n de NER\n","\n","En esta tarea van a resolver **NER**, com√∫nmente abordada como un problema de Sequence Labeling.\n","\n","**¬øQu√© es Sequence Labeling?**\n","\n","En breves palabras, dada una secuencia de tokens (oraci√≥n) sequence labeling tiene por objetivo asignar una etiqueta a cada token de dicha secuencia. En pocas palabras, dada una lista de tokens esperamos encontrar la mejor secuencia de etiquetas asociadas a esa lista. Ahora veamos de qu√© se trata este problema.\n","\n","**Named Entity Recognition (NER)**\n","\n","NER es un ejemplo de un problema de Sequence Labeling. Pero antes de definir formalmente esta tarea, es necesario definir algunos conceptos claves para poder entenderla de la mejor manera:\n","\n","- *Token*: Un token es una secuencia de caracteres, puede ser una palabra, un n√∫mero o un s√≠mbolo.\n","\n","- *Entidad*: No es m√°s que un trozo de texto (uno o m√°s tokens) asociado a una categor√≠a predefinida. Originalmente se sol√≠an utilizar categor√≠as como nombres de personas, organizaciones, ubicaciones, pero actualmente se ha extendido a diferentes dominios.\n","\n","- *L√≠mites de una entidad*: Son los √≠ndices de los tokens de inicio y f√≠n dentro de una entidad.\n","\n","- *Tipo de entidad*: Es la categor√≠a predefinida asociada a la entidad.\n","\n","Dicho esto, definimos formalmente una entidad como una tupla: $(s, e, t)$, donde $s, t$ son los l√≠mites de la entidad (√≠ndices de los tokens de inicio y fin, respectivamente) y corresponde al tipo de entidad o categor√≠a. Ya veremos m√°s ejemplos luego de describir el Dataset.\n","\n","**Corpus de la Lista de espera**\n","\n","Trabajaran con un conjunto de datos reales correspondiente a interconsultas de la lista de espera NO GES en Chile. Si quieren saber m√°s sobre c√≥mo fueron generados los datos pueden revisar el paper publicado hace unos meses atr√°s en el workshop de EMNLP, una de las conferencias m√°s importantes de NLP: [https://www.aclweb.org/anthology/2020.clinicalnlp-1.32/](https://www.aclweb.org/anthology/2020.clinicalnlp-1.32/).\n","\n","Este corpus Chileno est√° constituido originalmente por 7 tipos de entidades pero por simplicidad en esta tarea trabajar√°n con las siguientes:\n","\n","- **Disease**\n","- **Body_Part**\n","- **Medication**\n","- **Procedures**\n","- **Family_Member**\n","\n","Si quieren obtener m√°s informaci√≥n sobre estas entidades pueden consultar la [gu√≠a de anotaci√≥n](https://plncmm.github.io/annodoc/). Adem√°s, mencionar que este corpus est√° restringido bajo una licencia que permite solamente su uso acad√©mico, as√≠ que no puede ser compartido m√°s all√° de este curso o sin permisos por parte de los autores en caso que quieran utilizarlo fuera. Si este √∫ltimo es el caso entonces pueden escribir directamente al correo: pln@cmm.uchile.cl. Al aceptar los t√©rminos y condiciones de la tarea est√°n de acuerdo con los puntos descritos anteriormente.\n","\n","\n","**Formato ConLL**\n","\n","Los archivos que ser√°n entregados a ustedes vienen en un formato est√°ndar utilizado en NER, llamado ConLL. No es m√°s que un archivo de texto, que cumple las siguientes propiedades.\n","\n","- Un salto de linea corresponde a la separaci√≥n entre oraciones. Esto es importante ya que al entrenar una red neuronal ustedes pasaran una lista de oraciones como input, m√°s conocidos como batches.\n","\n","- La primera columna del archivo contiene todos los tokens de la partici√≥n.\n","\n","- La segunda columna del archivo contiene el tipo de entidad asociado al token de la primera columna.\n","\n","- Los tipos de entidades siguen un formato cl√°sico en NER denominado *IOB2*. Si un tipo de entidad comienza con el prefijo \"B-\" (Beginning) significa que es el token de inicio de una entidad, si comienza con \"I-\" (Inside) es un token distinto al de inicio y si un token est√° asociado a la categor√≠a O (Outside) significa que no pertenece a ninguna entidad.\n","\n","Aqu√≠ va un ejemplo:\n","\n","```\n","PACIENTE O\n","PRESENTA O\n","FRACTURA B-Disease\n","CORONARIA I-Disease\n","COMPLICADA I-Disease\n","EN O\n","PIE B-Body_Part\n","IZQUIERDO I-Body_Part\n",". O\n","SE O\n","REALIZA O\n","INSTRUMENTACION B-Procedure\n","INTRACONDUCTO I-Procedure\n",". O\n","```\n","\n","Seg√∫n nuestra definici√≥n tenemos las siguientes tres entidades (enumerando desde 0):\n","\n","- $(2, 4, Disease)$\n","- $(6, 7, Body Part)$\n","- $(11, 12, Procedure)$\n","\n","Repasen un par de veces todos estos conceptos antes de pasar a la siguiente secci√≥n del notebook.\n","Es importante entender bien este formato ya que al medir el rendimiento de sus modelos, consideraremos una **m√©trica estricta**. Esta m√©trica se llama as√≠ ya que considera correcta una predicci√≥n de su modelo, s√≥lo si al compararlo con las entidades reales **coinciden tanto los l√≠mites de la entidad como el tipo.**\n","\n","Para ejemplificar, tomando el caso anterior, si el modelo es capaz de encontrar la siguiente entidad: $(2, 3, Disease)$, entonces se considera incorrecto ya que pudo predecir dos de los tres tokens de dicha enfermedad. Es decir, buscamos una m√©trica que sea alta a nivel de entidad y no a nivel de token.\n","\n","Antes de pasar a explicar las reglas, se recomienda visitar los siguientes links para entender bien el baseline de la tarea:\n","\n","-  [Recurrent Neural Networks](slides/NLP-RNN.pdf): [Parte 1](https://youtu.be/BmhjUkzz3nk), [Parte 2](https://youtu.be/z43YFR1iIvk), [Parte 3](https://youtu.be/7L5JxQdwNJk)\n","\n","\n","Recuerden que todo el material se encuentra disponible en el [github del curso](https://github.com/dccuchile/CC6205)."],"metadata":{"id":"KbBq7gcHnPTf"}},{"cell_type":"markdown","source":["## Baseline\n","\n","En este punto esperamos que tengan conocimiento sobre redes neuronales y en particular redes neuronales recurrentes (RNN), si no siempre pueden escribirnos por el canal de Discord para aclarar dudas. La RNN del baseline adjunto a este notebook est√° programado en la librer√≠a [`pytorch`](https://pytorch.org/) pero ustedes pueden utilizar keras, tensorflow si as√≠ lo desean. El c√≥digo contiene lo siguiente:\n","\n","- La carga de los datasets, creaci√≥n de batches de texto y padding (esto es importante ya que si utilizan redes neuronales tienen que tener el mismo largo los inputs).\n","\n","- La implementaci√≥n b√°sica de una red `LSTM` simple de solo un nivel y sin bi-direccionalidad.\n","\n","- La construcci√≥n del formato del output requerido para que lo puedan probar en la tarea en codalab."],"metadata":{"id":"NXgaywGdo0bh"}},{"cell_type":"markdown","source":["### **Sugerencias**\n","Se espera que ustedes puedan experimentar con el baseline utilizando (pero no limit√°ndose) estas sugerencias:\n","\n","*   Probar la t√©cnica de early stopping.\n","*   Variar la cantidad de par√°metros de la capa de embeddings.\n","*   Variar la cantidad de capas RNN.\n","*   Variar la cantidad de par√°metros de las capas de RNN.\n","*   Inicializar la capa de embeddings con modelos pre-entrenados. (word2vec, glove, conceptnet, etc...). [Embeddings en espa√±ol aqu√≠](https://github.com/dccuchile/spanish-word-embeddings). Tambi√©n aqu√≠ pueden encontrar unos embeddings cl√≠nicos en Espa√±ol: [https://zenodo.org/record/3924799](https://zenodo.org/record/3924799)\n","*   Variar la cantidad de √©pocas de entrenamiento.\n","*   Variar el optimizador, learning rate, batch size, etc.\n","*   Probar bi-direccionalidad.\n","*   Incluir dropout.\n","*   Probar modelos de tipo GRU.\n","*   Probar usando capas de atenci√≥n.\n","*   Probar Embedding Contextuales (les puede ser de utilidad [flair](https://github.com/flairNLP/flair))\n","*   Probar modelos de transformers en espa√±ol usando [Huggingface](https://github.com/huggingface/transformers) o el framework Flair."],"metadata":{"id":"TfMlGTCgIq8o"}},{"cell_type":"markdown","source":["## Experimento base\n","\n","El c√≥digo que les entregaremos servir√° de baseline para luego implementar mejores modelos.\n","En general, el c√≥digo asociado a la carga de los datos, las funciones de entrenamiento, de evaluaci√≥n y la predicci√≥n de los datos de la tarea no deber√≠an cambiar.\n","Solo deben preocuparse de cambiar la arquitectura del modelo, sus hiperpar√°metros y reportar, lo cual lo pueden hacer en las subsecciones *modelos*."],"metadata":{"id":"YWEFCu4_sWwm"}},{"cell_type":"markdown","source":["###  **Carga de datos y Preprocesamiento**\n","\n","Para cargar los datos y preprocesarlos usaremos la librer√≠a [`torchtext`](https://github.com/pytorch/text). Tener cuidado ya que hace algunos meses esta librer√≠a tuvo cambios radicales, quedando las funcionalidades pasadas depreciadas de la librer√≠a ```legacy```. Esto ya que si quieren usar m√°s funciones de la librer√≠a entonces vean los cambios en la documentaci√≥n debe usar la versi√≥n antigua con python 3.8\n","\n","El proceso ser√° el siguiente:\n","\n","1. Descargar los datos desde github y examinarlos.\n","2. Cargar los datasets con la clase ```TaggingDataset``` de m√°s abajo.\n","3. Crear el vocabulario."],"metadata":{"id":"_v9-YCA9sg-A"}},{"cell_type":"code","source":["!pip install -U torchtext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YLZkrIVQsqBB","executionInfo":{"status":"ok","timestamp":1715012492696,"user_tz":240,"elapsed":181957,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"214640e6-1558-42b3-9af2-deabf65a9cd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n","Collecting torchtext\n","  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n","Collecting torch>=2.3.0 (from torchtext)\n","  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.3.0->torchtext)\n","  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Collecting triton==2.3.0 (from torch>=2.3.0->torchtext)\n","  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n","Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchtext\n","  Attempting uninstall: triton\n","    Found existing installation: triton 2.2.0\n","    Uninstalling triton-2.2.0:\n","      Successfully uninstalled triton-2.2.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.2.1+cu121\n","    Uninstalling torch-2.2.1+cu121:\n","      Successfully uninstalled torch-2.2.1+cu121\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.17.1\n","    Uninstalling torchtext-0.17.1:\n","      Successfully uninstalled torchtext-0.17.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n","torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 torchtext-0.18.0 triton-2.3.0\n"]}]},{"cell_type":"code","source":["import torch\n","import torchtext\n","from torchtext import data, datasets\n","\n","# Garantizar reproducibilidad de los experimentos\n","SEED = 1234\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-4DCjj5bsxNJ","executionInfo":{"status":"ok","timestamp":1715012495033,"user_tz":240,"elapsed":2348,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"e2142a01-7499-4a5f-d641-d1d8b5d77175"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/usr/local/lib/python3.10/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"]}]},{"cell_type":"markdown","source":["#### **Obtener datos**\n","\n","Descargamos los datos de entrenamiento, validaci√≥n y prueba en nuestro directorio de trabajo"],"metadata":{"id":"TDCqI-1is2j_"}},{"cell_type":"code","source":["%%capture\n","\n","!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt -nc\n","!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt -nc"],"metadata":{"id":"OIMlUsp3s7J-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NUEVO DATALOADER Y OTRAS COSAS NECESARIAS\n","from collections import Counter, OrderedDict\n","\n","import torch\n","\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","from torchtext.vocab import vocab\n","\n","class TaggingDataset(Dataset):\n","    def __init__(self, paths, lower=False, separator=\" \", encoding=\"utf-8\"):\n","\n","        data = []\n","        for path in paths:\n","          with open(path, 'r', encoding=encoding) as file:\n","            text, tag = [], []\n","            for line in file:\n","                line = line.strip()\n","                if line == \"\":\n","                    data.append(dict({'text':text, 'nertags':tag}))\n","                    text, tag = [], []\n","                else:\n","                    line_content = line.split(separator) # .rstrip('\\n')\n","                    if lower:\n","                      text.append(line_content[0].lower())\n","                    else:\n","                      text.append(line_content[0])\n","                    tag.append(line_content[1])\n","          data.append(dict({'text':text, 'nertags':tag}))\n","\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        item = self.data[index]\n","        text = item[\"text\"]\n","        nertags = item[\"nertags\"]\n","        return nertags, text\n","\n","def fit_vocab(data_iter):\n","\n","  def update_counter(counter_obj):\n","    sorted_by_freq_tuples = sorted(counter_obj.items(),\n","                                  key=lambda x: x[1],\n","                                  reverse=True)\n","    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n","    return ordered_dict\n","\n","  counter_1 = Counter()\n","  counter_2 = Counter()\n","  for _nertags, _text in data_iter:\n","    counter_1.update(_text)\n","    counter_2.update(_nertags)\n","\n","  od1 = update_counter(counter_1)\n","  od2 = update_counter(counter_2)\n","\n","  v1 = vocab(od1, specials=['<PAD>', '<unk>'])\n","  v1.set_default_index(v1[\"<unk>\"])\n","  v2 = vocab(od2, specials=['<PAD>'])\n","\n","  text_pipeline = lambda x: v1(x)\n","  nertags_pipeline = lambda x: v2(x)\n","\n","  return text_pipeline, nertags_pipeline, v1, v2\n","\n","def collate_batch(batch, nertags_pipeline, text_pipeline, device):\n","  nertags_list, text_list = [], []\n","  for _nertags, _text in batch:\n","    processed_nertags = torch.tensor(nertags_pipeline(_nertags),\n","                                     dtype=torch.int64)\n","    nertags_list.append(processed_nertags)\n","    processed_text = torch.tensor(text_pipeline(_text),\n","                                  dtype=torch.int64)\n","    text_list.append(processed_text)\n","  nertags_list = pad_sequence(nertags_list, batch_first=True).T\n","  text_list = pad_sequence(text_list, batch_first=True).T\n","  return nertags_list.to(device), text_list.to(device)"],"metadata":{"id":"kn_YMpyetNhn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_iter = TaggingDataset([\"train.txt\", \"dev.txt\"])\n","\n","data_length = len(data_iter)\n","\n","train_length = int(data_length * 0.8)\n","dev_length = int(data_length * 0.1)\n","test_length = data_length - train_length - dev_length\n","\n","train_iter, dev_iter, test_iter = torch.utils.data.random_split(\n","    data_iter,\n","     (train_length, dev_length, test_length),\n","    torch.Generator().manual_seed(42))\n","\n","text_pipeline, nertags_pipeline, TEXT, NER_TAGS = fit_vocab(train_iter)"],"metadata":{"id":"ctd5gNmzxnnE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_iter), len(dev_iter), len(test_iter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-MPvr5LDkq5","executionInfo":{"status":"ok","timestamp":1715019847735,"user_tz":240,"elapsed":4,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"0895ea5d-18a7-46d5-c117-1137358ccb98"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7132, 891, 893)"]},"metadata":{},"execution_count":86}]},{"cell_type":"code","source":["# seteamos algunos valores de interes\n","UNK_IDX = TEXT.vocab.get_stoi()['<unk>']\n","PAD_IDX = TEXT.vocab.get_stoi()['<PAD>']\n","\n","PAD_TAG_IDX = NER_TAGS.get_stoi()['<PAD>']\n","O_TAG_IDX = NER_TAGS.vocab.get_stoi()['O']"],"metadata":{"id":"h-BjEO1exsuq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 22\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using', device)\n","\n","fix_collocate_batch = lambda x: collate_batch(x, nertags_pipeline, text_pipeline, device)\n","\n","dataloader_train = DataLoader(\n","    train_iter, batch_size=BATCH_SIZE, shuffle=False, collate_fn=fix_collocate_batch\n",")\n","dataloader_dev = DataLoader(\n","    dev_iter, batch_size=BATCH_SIZE, shuffle=False, collate_fn=fix_collocate_batch\n",")\n","dataloader_test = DataLoader(\n","    test_iter, batch_size=BATCH_SIZE, shuffle=False, collate_fn=fix_collocate_batch\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sD3AmVkOxsE8","executionInfo":{"status":"ok","timestamp":1715019853335,"user_tz":240,"elapsed":238,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"ee0dd657-5629-4958-f2fc-c5378e1e55be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda\n"]}]},{"cell_type":"code","source":["example = next(iter(dataloader_train))\n","ner_tags_example = example[0]\n","text_example = example[1]\n","\n","# revisamos el primer ejemplo\n","[NER_TAGS.vocab.get_itos()[j] for j in ner_tags_example[:, 1]], [TEXT.vocab.get_itos()[j] for j in text_example[:, 1]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_3XrV_YyyBWB","executionInfo":{"status":"ok","timestamp":1715020124681,"user_tz":240,"elapsed":268,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"7d7fa4db-54fb-4648-e73b-7a78bb04fa9e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['O',\n","  'O',\n","  'O',\n","  'O',\n","  'B-Disease',\n","  'I-Disease',\n","  'I-Disease',\n","  'I-Disease',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>'],\n"," ['SOLICITO',\n","  'EVALUACION',\n","  'Y',\n","  'TTO',\n","  'SUBLUXACION',\n","  'ATM',\n","  'IZQ',\n","  '.',\n","  ',',\n","  'DOLOR',\n","  'APERTURA',\n","  '.',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>'])"]},"metadata":{},"execution_count":96}]},{"cell_type":"code","source":["!pip install seqeval"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9D81QD5Ky2_4","executionInfo":{"status":"ok","timestamp":1715020140033,"user_tz":240,"elapsed":4945,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"ec168808-cf0a-42c6-8bd2-3eb1d3fc5ad3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.25.2)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n"]}]},{"cell_type":"code","source":["# Definimos las m√©tricas\n","\n","from seqeval.metrics import f1_score, precision_score, recall_score\n","\n","def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n","    \"\"\"\n","    Calcula precision, recall y f1 de cada batch.\n","    \"\"\"\n","\n","    # Obtener el indice de la clase con probabilidad mayor. (clases)\n","    y_pred = preds.argmax(dim=1, keepdim=True)\n","\n","    # filtramos <pad> para calcular los scores.\n","    mask = [(y_true != pad_idx)]\n","    y_pred = y_pred[mask]\n","    y_true = y_true[mask]\n","\n","    # traemos a la cpu\n","    y_pred = y_pred.view(-1).to('cpu').numpy()\n","    y_true = y_true.to('cpu').numpy()\n","    y_pred = [[NER_TAGS.vocab.get_itos()[v] for v in y_pred]]\n","    y_true = [[NER_TAGS.vocab.get_itos()[v] for v in y_true]]\n","\n","    # calcular scores\n","    f1 = f1_score(y_true, y_pred, mode='strict')\n","    precision = precision_score(y_true, y_pred, mode='strict')\n","    recall = recall_score(y_true, y_pred, mode='strict')\n","\n","    return precision, recall, f1"],"metadata":{"id":"YdWPWTMTy8Cm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Modelo Baseline**\n","\n","Teniendo ya cargado los datos, toca definir nuestro modelo. Este baseline tendr√° una capa de embedding, unas cuantas LSTM y una capa de salida y usar√° dropout en el entrenamiento.\n","\n","Este constar√° de los siguientes pasos:\n","\n","1. Definir la clase que contendr√° la red.\n","2. Definir los hiperpar√°metros e inicializar la red.\n","3. Definir el n√∫mero de √©pocas de entrenamiento\n","4. Definir la funci√≥n de loss.\n","\n","\n","\n","Recomendamos que para experimentar, encapsules los modelos en una sola variable y luego la fijes en model para entrenarla"],"metadata":{"id":"eN984Z0XzHkP"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","\n","# Definir la red\n","class NER_RNN(nn.Module):\n","    def __init__(self,\n","                 input_dim,\n","                 embedding_dim,\n","                 hidden_dim,\n","                 output_dim,\n","                 n_layers,\n","                 bidirectional,\n","                 dropout,\n","                 pad_idx):\n","\n","        super().__init__()\n","\n","        # Capa de embedding\n","        self.embedding = nn.Embedding(input_dim,\n","                                      embedding_dim,\n","                                      padding_idx=pad_idx,\n","                                      )\n","\n","        # Capa LSTM\n","        self.lstm = nn.LSTM(embedding_dim,\n","                           hidden_dim,\n","                           num_layers=n_layers,\n","                           bidirectional=bidirectional,\n","                           dropout = dropout if n_layers > 1 else 0)\n","\n","        # Capa de salida\n","        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n","                            output_dim)\n","\n","        # Dropout\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, text):\n","\n","        #text = [sent len, batch size]\n","\n","        # Convertir lo enviado a embedding\n","        embedded = self.dropout(self.embedding(text))\n","\n","        outputs, (hidden, cell) = self.lstm(embedded)\n","        #embedded = [sent len, batch size, emb dim]\n","\n","        # Pasar los embeddings por la rnn (LSTM)\n","\n","        #output = [sent len, batch size, hid dim * n directions]\n","        #hidden/cell = [n layers * n directions, batch size, hid dim]\n","\n","        # Predecir usando la capa de salida.\n","        predictions = self.fc(self.dropout(outputs))\n","        #predictions = [sent len, batch size, output dim]\n","\n","        return predictions"],"metadata":{"id":"G3vNCpAyy7_L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hiperpar√°metros de la red"],"metadata":{"id":"3Ofso-aozeSL"}},{"cell_type":"code","source":["# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n","INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 300  # dimensi√≥n de los embeddings.\n","HIDDEN_DIM = 256  # dimensi√≥n de la capas LSTM\n","OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n","\n","N_LAYERS = 3  # n√∫mero de capas.\n","DROPOUT = 0.6\n","BIDIRECTIONAL = True\n","\n","# Creamos nuestro modelo.\n","baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n","                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n","\n","baseline_model_name = 'baseline'  # nombre que tendr√° el modelo guardado...\n","\n","baseline_n_epochs = 10"],"metadata":{"id":"9ryN9aKzy773"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Definimos la funci√≥n de loss"],"metadata":{"id":"TfGxRLBwzhsg"}},{"cell_type":"code","source":["# Loss: Cross Entropy\n","TAG_PAD_IDX = NER_TAGS.vocab.get_stoi()['<PAD>']\n","baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"],"metadata":{"id":"LtfG9RUQy7vx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Entrenamos y evaluamos**\n","\n","\n","**Importante** : Fijen el modelo, el n√∫mero de √©pocas de entrenamiento, la loss y el optimizador que usar√°n para entrenar y evaluar en las siguientes variables!!!"],"metadata":{"id":"Dg2Lm8Yszu11"}},{"cell_type":"code","source":["model = baseline_model\n","model_name = baseline_model_name\n","criterion = baseline_criterion\n","n_epochs = baseline_n_epochs"],"metadata":{"id":"1-TnXy2LzvXO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Inicializamos la red**\n","\n","Iniciamos los pesos de la red de forma aleatoria (Usando una distribuci√≥n normal)."],"metadata":{"id":"x3wQq18i0Ehc"}},{"cell_type":"code","source":["def init_weights(m):\n","    # Inicializamos los pesos como aleatorios\n","    for name, param in m.named_parameters():\n","        nn.init.normal_(param.data, mean=0, std=0.1)\n","\n","    # Seteamos como 0 los embeddings de UNK y PAD.\n","    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","\n","model.apply(init_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-kgtnu2L0E-B","executionInfo":{"status":"ok","timestamp":1715020153269,"user_tz":240,"elapsed":282,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"2e308dba-6a68-4cda-d05a-c1e7b5295850"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NER_RNN(\n","  (embedding): Embedding(16496, 300, padding_idx=0)\n","  (lstm): LSTM(300, 256, num_layers=3, dropout=0.6, bidirectional=True)\n","  (fc): Linear(in_features=512, out_features=12, bias=True)\n","  (dropout): Dropout(p=0.6, inplace=False)\n",")"]},"metadata":{},"execution_count":104}]},{"cell_type":"code","source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f'El modelo actual tiene {count_parameters(model):,} par√°metros entrenables.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O2TB3-J80aBB","executionInfo":{"status":"ok","timestamp":1715020155319,"user_tz":240,"elapsed":386,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"9ea079f4-8585-41a5-d894-5ab6175f7f0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["El modelo actual tiene 9,251,660 par√°metros entrenables.\n"]}]},{"cell_type":"markdown","source":["Notar que definimos los embeddings que representan a \\<unk\\> y \\<pad\\>  como [0, 0, ..., 0]"],"metadata":{"id":"Gnuk_Hs10djW"}},{"cell_type":"markdown","source":["Definimos el optimizador"],"metadata":{"id":"7qJhs1OX0gls"}},{"cell_type":"code","source":["# Optimizador\n","optimizer = optim.Adam(model.parameters())"],"metadata":{"id":"xBAYockO0mE7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Enviamos el modelo a cuda"],"metadata":{"id":"RGj6tfdd0qJb"}},{"cell_type":"code","source":["# Enviamos el modelo y la loss a cuda (en el caso en que est√© disponible)\n","model = model.to(device)\n","criterion = criterion.to(device)"],"metadata":{"id":"_68iNmcN0qkm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Definimos el entrenamiento de la red**\n","\n","Algunos conceptos previos:\n","\n","- `epoch` : una pasada de entrenamiento completa de una dataset.\n","- `batch`: una fracci√≥n de la √©poca. Se utilizan para entrenar mas r√°pidamente la red. (mas eficiente pasar n datos que uno en cada ejecuci√≥n del backpropagation)\n","\n","Esta funci√≥n est√° encargada de entrenar la red en una √©poca. Para esto, por cada batch de la √©poca actual, predice los tags del texto, calcula su loss y luego hace backpropagation para actualizar los pesos de la red.\n","\n","Observaci√≥n: En algunos comentarios aparecer√° el tama√±o de los tensores entre corchetes"],"metadata":{"id":"BuKvsbrw0wHw"}},{"cell_type":"code","source":["def train(model, iterator, optimizer, criterion):\n","\n","    epoch_loss = 0\n","    epoch_precision = 0\n","    epoch_recall = 0\n","    epoch_f1 = 0\n","\n","    model.train()\n","\n","    # Por cada batch del iterador de la √©poca:\n","    for tags, text in iterator:\n","        # Reiniciamos los gradientes calculados en la iteraci√≥n anterior\n","        optimizer.zero_grad()\n","\n","        #text = [sent len, batch size]\n","\n","        # Predecimos los tags del texto del batch.\n","        predictions = model(text.to(device))\n","\n","        #predictions = [sent len, batch size, output dim]\n","        #tags = [sent len, batch size]\n","\n","        # Reordenamos los datos para calcular la loss\n","        predictions = predictions.view(-1, predictions.shape[-1])\n","        #ipdb.set_trace()\n","        tags = torch.reshape(tags, (-1,)).to(device)\n","\n","        #predictions = [sent len * batch size, output dim]\n","\n","\n","\n","        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n","        loss = criterion(predictions, tags)\n","\n","        # Calculamos el accuracy\n","        precision, recall, f1 = calculate_metrics(predictions, tags)\n","\n","        # Calculamos los gradientes\n","        loss.backward()\n","\n","        # Actualizamos los par√°metros de la red\n","        optimizer.step()\n","\n","        # Actualizamos el loss y las m√©tricas\n","        epoch_loss += loss.item()\n","        epoch_precision += precision\n","        epoch_recall += recall\n","        epoch_f1 += f1\n","\n","    return ( epoch_loss / len(iterator), epoch_precision / len(iterator),\n","              epoch_recall / len(iterator), epoch_f1 / len(iterator) )"],"metadata":{"id":"ShIJbx2v0wlf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Definimos la funci√≥n de evaluaci√≥n**\n","\n","Evalua el rendimiento actual de la red usando los datos de validaci√≥n.\n","\n","Por cada batch de estos datos, calcula y reporta el loss y las m√©tricas asociadas al conjunto de validaci√≥n.\n","Ya que las m√©tricas son calculadas por cada batch, estas son retornadas promediadas por el n√∫mero de batches entregados. (ver linea del return)"],"metadata":{"id":"IWSosAit0xCJ"}},{"cell_type":"code","source":["def evaluate(model, iterator, criterion):\n","\n","    epoch_loss = 0\n","    epoch_precision = 0\n","    epoch_recall = 0\n","    epoch_f1 = 0\n","\n","    model.eval()\n","\n","    # Indicamos que ahora no guardaremos los gradientes\n","    with torch.no_grad():\n","        # Por cada batch\n","        for tags, text in iterator:\n","            # Predecimos\n","            predictions = model(text.to(device))\n","\n","            predictions = predictions.view(-1, predictions.shape[-1])\n","            tags = torch.reshape(tags, (-1,)).to(device)\n","\n","            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n","            loss = criterion(predictions, tags)\n","\n","            # Calculamos las m√©tricas\n","            precision, recall, f1 = calculate_metrics(predictions, tags)\n","\n","            # Actualizamos el loss y las m√©tricas\n","            epoch_loss += loss.item()\n","            epoch_precision += precision\n","            epoch_recall += recall\n","            epoch_f1 += f1\n","\n","    return epoch_loss / len(iterator), epoch_precision / len(\n","        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"],"metadata":{"id":"Z3fRycaX0xbA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"metadata":{"id":"tA8nad2A1HUm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Entrenamiento de la red**\n","\n","En este cuadro de c√≥digo ejecutaremos el entrenamiento de la red.\n","Para esto, primero definiremos el n√∫mero de √©pocas y luego por cada √©poca, ejecutaremos `train` y `evaluate`.\n","\n","**Importante: Reiniciar los pesos del modelo**\n","\n","Si ejecutas nuevamente esta celda, se seguira entrenando el mismo modelo una y otra vez.\n","Para reiniciar el modelo se debe ejecutar nuevamente la celda que contiene la funci√≥n `init_weights`."],"metadata":{"id":"2twwfpmz1E7K"}},{"cell_type":"code","source":["best_valid_loss = float('inf')\n","\n","for epoch in range(n_epochs):\n","\n","    start_time = time.time()\n","\n","    # Recuerdo: dataloader_train y valid_iterator contienen el dataset dividido en batches.\n","\n","    # Entrenar\n","    train_loss, train_precision, train_recall, train_f1 = train(\n","        model, dataloader_train, optimizer, criterion)\n","\n","    # Evaluar (valid = validaci√≥n)\n","    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n","        model, dataloader_dev, criterion)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n","    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de c√≥digo.\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n","    # Si ya no mejoramos el loss de validaci√≥n, terminamos de entrenar.\n","\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(\n","        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n","    )\n","    print(\n","        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0Py5sCE1FTe","executionInfo":{"status":"ok","timestamp":1715020264671,"user_tz":240,"elapsed":96067,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"36fdecf3-512a-41fc-fe33-9d815d97807e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <PAD> seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.893 | Train f1: 0.36 | Train precision: 0.56 | Train recall: 0.28\n","\t Val. Loss: 0.579 |  Val. f1: 0.60 |  Val. precision: 0.74 | Val. recall: 0.52\n","Epoch: 02 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.531 | Train f1: 0.63 | Train precision: 0.73 | Train recall: 0.56\n","\t Val. Loss: 0.400 |  Val. f1: 0.72 |  Val. precision: 0.77 | Val. recall: 0.69\n","Epoch: 03 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.365 | Train f1: 0.74 | Train precision: 0.79 | Train recall: 0.70\n","\t Val. Loss: 0.356 |  Val. f1: 0.75 |  Val. precision: 0.78 | Val. recall: 0.73\n","Epoch: 04 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.277 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.78\n","\t Val. Loss: 0.352 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.75\n","Epoch: 05 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.222 | Train f1: 0.84 | Train precision: 0.85 | Train recall: 0.83\n","\t Val. Loss: 0.344 |  Val. f1: 0.78 |  Val. precision: 0.79 | Val. recall: 0.76\n","Epoch: 06 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.178 | Train f1: 0.87 | Train precision: 0.87 | Train recall: 0.87\n","\t Val. Loss: 0.360 |  Val. f1: 0.78 |  Val. precision: 0.80 | Val. recall: 0.76\n","Epoch: 07 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.152 | Train f1: 0.89 | Train precision: 0.89 | Train recall: 0.89\n","\t Val. Loss: 0.382 |  Val. f1: 0.78 |  Val. precision: 0.80 | Val. recall: 0.77\n","Epoch: 08 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.129 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.91\n","\t Val. Loss: 0.388 |  Val. f1: 0.78 |  Val. precision: 0.79 | Val. recall: 0.77\n","Epoch: 09 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.116 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n","\t Val. Loss: 0.379 |  Val. f1: 0.78 |  Val. precision: 0.77 | Val. recall: 0.79\n","Epoch: 10 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.101 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n","\t Val. Loss: 0.409 |  Val. f1: 0.78 |  Val. precision: 0.78 | Val. recall: 0.78\n"]}]},{"cell_type":"markdown","source":["**Importante**: Recuerden que el √∫ltimo modelo entrenado no es el mejor (probablemente est√© *overfitteado*), si no el que guardamos con la menor loss del conjunto de validaci√≥n. Este problema lo pueden solucionar con *early stopping*.\n","Para cargar el mejor modelo entrenado, ejecuten la siguiente celda."],"metadata":{"id":"zMU8i9eV-xYq"}},{"cell_type":"code","source":["# cargar el mejor modelo entrenado.\n","model.load_state_dict(torch.load('{}.pt'.format(model_name)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tqJU5AEY-bwK","executionInfo":{"status":"ok","timestamp":1715020264673,"user_tz":240,"elapsed":13,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"feb7cf6b-d543-4c4a-c05d-b1bec86525ca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":112}]},{"cell_type":"code","source":["# Limpiar ram de cuda\n","torch.cuda.empty_cache()"],"metadata":{"id":"JX84ETMg-1Qv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uVoujP7IjPM3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Evaluamos el set de validaci√≥n con el modelo final**\n","\n","Estos son los resultados de predecir el dataset de evaluaci√≥n con el *mejor* modelo entrenado."],"metadata":{"id":"_d9Yfz95_GIz"}},{"cell_type":"code","source":["valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n","    model, dataloader_dev, criterion)\n","\n","print(\n","    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6mMnQIC-2G2","executionInfo":{"status":"ok","timestamp":1715020266118,"user_tz":240,"elapsed":1453,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"726443ef-bfe6-4b1b-a687-b2e229859490"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Val. Loss: 0.344 |  Val. f1: 0.78 | Val. precision: 0.79 | Val. recall: 0.76\n"]}]},{"cell_type":"markdown","source":["#### **Evaluamos el set de prueba con el modelo final**\n","\n","Estos son los resultados de predecir el dataset de prueba con el *mejor* modelo entrenado."],"metadata":{"id":"K7FzQVxK-2kc"}},{"cell_type":"code","source":["prueba_loss, prueba_precision, prueba_recall, prueba_f1 = evaluate(\n","    model, dataloader_test, criterion)\n","\n","print(\n","    f'Val. Loss: {prueba_loss:.3f} |  Val. f1: {prueba_f1:.2f} | Val. precision: {prueba_precision:.2f} | Val. recall: {prueba_recall:.2f}'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"meu3CsgT-3PE","executionInfo":{"status":"ok","timestamp":1715020309098,"user_tz":240,"elapsed":699,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"d67789ea-7837-40c6-8ddb-f56a77e93873"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Val. Loss: 0.358 |  Val. f1: 0.76 | Val. precision: 0.78 | Val. recall: 0.75\n"]}]},{"cell_type":"markdown","source":["## Experimentos\n","\n","En esta secci√≥n deberan explicar, implementar y evaluar **TRES EXPERIMENTO DIFERENTES**. Puede revisar la [lista de sugerencias](#scrollTo=TfMlGTCgIq8o) al inicio del enunciado para sacar ideas de experimentos. Se espera que los experimentos sean relevantes y desafiantes.\n","\n","Cada experimento debe contener las siguientes cuatro subsecciones:\n","\n","- *Explicaci√≥n del experimento*: debe motivar y explicar en que consiste su experimento. La explicaci√≥n debe fundamentar porque es un experimento relevante y desafiantes, ademas de aclarar que estudiaran.\n","\n","- *Implementacion del experimento*: debe implementar su experimento con codigo debidamente comentado, con tal de que sea legible.\n","\n","- *Evaluar el experimento*: debe evaluar con las metricas propuestas para generar los resultados obtenidos con sus experimentos sobre el conjunto de prueba.\n","\n","- *Analizar el experimento*: debe hacer un breve analisis de los resultados obtenidos."],"metadata":{"id":"F2bRYNRnKRzK"}},{"cell_type":"markdown","source":["### **Experimento 1**"],"metadata":{"id":"QwdOcPfbL3pH"}},{"cell_type":"markdown","source":["#### Explicaci√≥n"],"metadata":{"id":"Gkn38sTsdpzR"}},{"cell_type":"markdown","source":["> Escribir explicaci√≥n aqu√≠"],"metadata":{"id":"r9Cr3jl1d0F3"}},{"cell_type":"markdown","source":["#### Implementaci√≥n"],"metadata":{"id":"Q5dlEgEqdtvp"}},{"cell_type":"code","source":["### Escribir implenetaci√≥n desde aqu√≠"],"metadata":{"id":"dhuuMKVAdvqt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluaci√≥n"],"metadata":{"id":"37IjYq-_d5Pv"}},{"cell_type":"code","source":["prueba_loss, prueba_precision, prueba_recall, prueba_f1 = evaluate(\n","    model_experimento_1, dataloader_test, criterion)\n","\n","print(\n","    f'Val. Loss: {prueba_loss:.3f} |  Val. f1: {prueba_f1:.2f} | Val. precision: {prueba_precision:.2f} | Val. recall: {prueba_recall:.2f}'\n",")"],"metadata":{"id":"kGz_8OB7eItO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### An√°lisis"],"metadata":{"id":"pURRQLkReNMY"}},{"cell_type":"markdown","source":["> Escribir an√°lisis aqu√≠"],"metadata":{"id":"wfTrOktPeSB7"}},{"cell_type":"markdown","source":["### **Experimento 2**"],"metadata":{"id":"JIFrDPaGej4y"}},{"cell_type":"markdown","source":["#### Explicaci√≥n"],"metadata":{"id":"M-dsNSEdej4z"}},{"cell_type":"markdown","source":["> Escribir explicaci√≥n aqu√≠"],"metadata":{"id":"c7JD-PzGej4z"}},{"cell_type":"markdown","source":["#### Implementaci√≥n"],"metadata":{"id":"PKZIGOUpej4z"}},{"cell_type":"code","source":["### Escribir implenetaci√≥n desde aqu√≠"],"metadata":{"id":"B_Uyw10-ej40"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluaci√≥n"],"metadata":{"id":"epaoMVyMej40"}},{"cell_type":"code","source":["prueba_loss, prueba_precision, prueba_recall, prueba_f1 = evaluate(\n","    model_experimento_2, dataloader_test, criterion)\n","\n","print(\n","    f'Val. Loss: {prueba_loss:.3f} |  Val. f1: {prueba_f1:.2f} | Val. precision: {prueba_precision:.2f} | Val. recall: {prueba_recall:.2f}'\n",")"],"metadata":{"id":"BF8M8iLGej40"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### An√°lisis"],"metadata":{"id":"bqymD_Nuej41"}},{"cell_type":"markdown","source":["> Escribir an√°lisis aqu√≠"],"metadata":{"id":"88tsCz4eej41"}},{"cell_type":"markdown","source":["### **Experimento 3**"],"metadata":{"id":"ZcPtyefAenbW"}},{"cell_type":"markdown","source":["#### Explicaci√≥n"],"metadata":{"id":"eYUFyLMJenbX"}},{"cell_type":"markdown","source":["> Escribir explicaci√≥n aqu√≠"],"metadata":{"id":"fuI3TaAhenbY"}},{"cell_type":"markdown","source":["#### Implementaci√≥n"],"metadata":{"id":"800wWbuGenbY"}},{"cell_type":"code","source":["### Escribir implenetaci√≥n desde aqu√≠"],"metadata":{"id":"zON_wa1penbY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluaci√≥n"],"metadata":{"id":"DYBzB3SlenbZ"}},{"cell_type":"code","source":["prueba_loss, prueba_precision, prueba_recall, prueba_f1 = evaluate(\n","    model_experimento_3, dataloader_test, criterion)\n","\n","print(\n","    f'Val. Loss: {prueba_loss:.3f} |  Val. f1: {prueba_f1:.2f} | Val. precision: {prueba_precision:.2f} | Val. recall: {prueba_recall:.2f}'\n",")"],"metadata":{"id":"0w69TMqoenba"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### An√°lisis"],"metadata":{"id":"YxQkQt1Benba"}},{"cell_type":"markdown","source":["> Escribir an√°lisis aqu√≠"],"metadata":{"id":"cT-giD-Tenbb"}}]}