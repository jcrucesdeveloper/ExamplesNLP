{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1XTS4XkCagRBqAc-9j4Dlya3EFwzhSXHM","timestamp":1714518009424},{"file_id":"1jvrpt6zgc-G5wWhtgcTqPmiKvl1wcvJ-","timestamp":1713283518140},{"file_id":"11hBKGIhHcrr2QYP9bxUsKVnXd5VBfrzg","timestamp":1712439760293}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Tarea 3: Sequence Labelling\n","**Procesamiento de Lenguaje Natural (CC6205-1 - Otoño 2024)**"],"metadata":{"id":"AxrkZWMLZB8z"}},{"cell_type":"markdown","source":["## Tarjeta de identificación\n","\n","**Nombres:** ```<Completar aquí>```\n","\n","**Fecha límite de entrega 📆:** 06/06.\n","\n","**Tiempo estimado de dedicación:** 4 horas\n"],"metadata":{"id":"b97b4IJjZGxM"}},{"cell_type":"markdown","source":["## Instrucciones\n","Bienvenid@s a la tercera tarea en el curso de Natural Language Processing (NLP). Esta tarea tiene como objetivo evaluar los contenidos teóricos de las últimas semanas de clases posteriores a la tarea 2, enfocado principalmente en **Word Embeddings**, **Sequence Labeling - HMM**, **Convolutional Neural Networks** y **Recurrent Neural Networks**. Si aún no has visto las clases, se recomienda visitar los links de las referencias.\n","\n","La tarea consta de una una parte práctica con el fín de introducirlos a la programación en Python enfocada en NLP.\n","\n","* La tarea es en **grupo** (maximo hasta 3 personas).\n","* La entrega es a través de u-cursos a más tardar el día estipulado arriba. No se aceptan atrasos.\n","* El formato de entrega es este mismo Jupyter Notebook.\n","* Al momento de la revisión su código será ejecutado. Por favor verifiquen que su entrega no tenga errores de compilación.\n","* Completar la tarjeta de identificación. Sin ella no podrá tener nota."],"metadata":{"id":"TKcZMFlmZ3b9"}},{"cell_type":"markdown","source":["## Material de referencia\n","\n","Diapositivas del curso 📄\n","    \n","- [Word Embeddings](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-wordvectors.pdf)\n","- [Sequence Labeling - HMM](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-HMM.pdf)\n","- [Convolutional Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-CNN.pdf)\n","- [Recurrent Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-RNN.pdf)\n","\n","Videos del curso 📺\n","\n","- Word Embeddings: [Parte 1](https://www.youtube.com/watch?v=wtwUsJMC9CA), [Parte 2](https://www.youtube.com/watch?v=XDxzQ7JU95U), [Parte 3](https://www.youtube.com/watch?v=Ikyc3DRVodk)\n","\n","- Sequence Labeling - HMM: [Parte 1](https://www.youtube.com/watch?v=-ngfOZz8yK0), [Parte 2](https://www.youtube.com/watch?v=Tjgb-yQOg54), [Parte 3](https://www.youtube.com/watch?v=aaa5Qoi8Vco), [Parte 4](https://www.youtube.com/watch?v=4pKWIDkF_6Y)\n","\n","- [Convolutional Neural Networks](https://www.youtube.com/watch?v=lLZW5Fn40r8)\n","\n","- Recurrent Neural Networks: [Parte 1](https://www.youtube.com/watch?v=BmhjUkzz3nk), [Parte 2](https://www.youtube.com/watch?v=z43YFR1iIvk), [Parte 3](https://youtu.be/7L5JxQdwNJk)"],"metadata":{"id":"dnTrhOKraAw2"}},{"cell_type":"markdown","source":["## Objetivo\n","\n","El objetivo de esta tarea es resolver una de las tareas más importantes en el área del procesamiento de lenguage natural, relacionada con la extracción de información: [Named Entity Recognition (NER)](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf).\n","\n","En particular, deberán crear distintos modelos que apunten a resolver la tarea de NER en Español. Para esto, les entregaremos un dataset real perteneciente a la lista de espera NO GES en Chile. Es importante destacar que existe una falta de trabajos realizados en el área de NER en Español y aún más en el contexto clínico, por ende puede ser considerado como una tarea bien desafiante y quizás les interesa trabajar en el área más adelante en sus carreras.\n","\n","En este notebook les entregaremos un baseline como referencia de los resultados que esperamos puedan obtener. Recuerden que el no superar a los baselines en alguna de las tres métricas conlleva un descuento de 0.5 puntos hasta 1.5 puntos.\n","\n","Como hemos estado viendo redes neuronales tanto en cátedras, tareas y auxiliares (o próximamente lo harán), esperamos que (por lo menos) utilicen Redes Neuronales Recurrentes (RNN) para resolverla.\n","\n","Nuevamente, hay total libertad para utilizar el software y los modelos que deseen, siempre y cuando estos no traigan los modelos ya implementados (de todas maneras, como es un corpus nuevo, es difícil que haya algún modelo ya implementado con éstas entidades)."],"metadata":{"id":"025o3RYmm3oM"}},{"cell_type":"markdown","source":["## Explicación de NER\n","\n","En esta tarea van a resolver **NER**, comúnmente abordada como un problema de Sequence Labeling.\n","\n","**¿Qué es Sequence Labeling?**\n","\n","En breves palabras, dada una secuencia de tokens (oración) sequence labeling tiene por objetivo asignar una etiqueta a cada token de dicha secuencia. En pocas palabras, dada una lista de tokens esperamos encontrar la mejor secuencia de etiquetas asociadas a esa lista. Ahora veamos de qué se trata este problema.\n","\n","**Named Entity Recognition (NER)**\n","\n","NER es un ejemplo de un problema de Sequence Labeling. Pero antes de definir formalmente esta tarea, es necesario definir algunos conceptos claves para poder entenderla de la mejor manera:\n","\n","- *Token*: Un token es una secuencia de caracteres, puede ser una palabra, un número o un símbolo.\n","\n","- *Entidad*: No es más que un trozo de texto (uno o más tokens) asociado a una categoría predefinida. Originalmente se solían utilizar categorías como nombres de personas, organizaciones, ubicaciones, pero actualmente se ha extendido a diferentes dominios.\n","\n","- *Límites de una entidad*: Son los índices de los tokens de inicio y fín dentro de una entidad.\n","\n","- *Tipo de entidad*: Es la categoría predefinida asociada a la entidad.\n","\n","Dicho esto, definimos formalmente una entidad como una tupla: $(s, e, t)$, donde $s, t$ son los límites de la entidad (índices de los tokens de inicio y fin, respectivamente) y corresponde al tipo de entidad o categoría. Ya veremos más ejemplos luego de describir el Dataset.\n","\n","**Corpus de la Lista de espera**\n","\n","Trabajaran con un conjunto de datos reales correspondiente a interconsultas de la lista de espera NO GES en Chile. Si quieren saber más sobre cómo fueron generados los datos pueden revisar el paper publicado hace unos meses atrás en el workshop de EMNLP, una de las conferencias más importantes de NLP: [https://www.aclweb.org/anthology/2020.clinicalnlp-1.32/](https://www.aclweb.org/anthology/2020.clinicalnlp-1.32/).\n","\n","Este corpus Chileno está constituido originalmente por 7 tipos de entidades pero por simplicidad en esta tarea trabajarán con las siguientes:\n","\n","- **Disease**\n","- **Body_Part**\n","- **Medication**\n","- **Procedures**\n","- **Family_Member**\n","\n","Si quieren obtener más información sobre estas entidades pueden consultar la [guía de anotación](https://plncmm.github.io/annodoc/). Además, mencionar que este corpus está restringido bajo una licencia que permite solamente su uso académico, así que no puede ser compartido más allá de este curso o sin permisos por parte de los autores en caso que quieran utilizarlo fuera. Si este último es el caso entonces pueden escribir directamente al correo: pln@cmm.uchile.cl. Al aceptar los términos y condiciones de la tarea están de acuerdo con los puntos descritos anteriormente.\n","\n","\n","**Formato ConLL**\n","\n","Los archivos que serán entregados a ustedes vienen en un formato estándar utilizado en NER, llamado ConLL. No es más que un archivo de texto, que cumple las siguientes propiedades.\n","\n","- Un salto de linea corresponde a la separación entre oraciones. Esto es importante ya que al entrenar una red neuronal ustedes pasaran una lista de oraciones como input, más conocidos como batches.\n","\n","- La primera columna del archivo contiene todos los tokens de la partición.\n","\n","- La segunda columna del archivo contiene el tipo de entidad asociado al token de la primera columna.\n","\n","- Los tipos de entidades siguen un formato clásico en NER denominado *IOB2*. Si un tipo de entidad comienza con el prefijo \"B-\" (Beginning) significa que es el token de inicio de una entidad, si comienza con \"I-\" (Inside) es un token distinto al de inicio y si un token está asociado a la categoría O (Outside) significa que no pertenece a ninguna entidad.\n","\n","Aquí va un ejemplo:\n","\n","```\n","PACIENTE O\n","PRESENTA O\n","FRACTURA B-Disease\n","CORONARIA I-Disease\n","COMPLICADA I-Disease\n","EN O\n","PIE B-Body_Part\n","IZQUIERDO I-Body_Part\n",". O\n","SE O\n","REALIZA O\n","INSTRUMENTACION B-Procedure\n","INTRACONDUCTO I-Procedure\n",". O\n","```\n","\n","Según nuestra definición tenemos las siguientes tres entidades (enumerando desde 0):\n","\n","- $(2, 4, Disease)$\n","- $(6, 7, Body Part)$\n","- $(11, 12, Procedure)$\n","\n","Repasen un par de veces todos estos conceptos antes de pasar a la siguiente sección del notebook.\n","Es importante entender bien este formato ya que al medir el rendimiento de sus modelos, consideraremos una **métrica estricta**. Esta métrica se llama así ya que considera correcta una predicción de su modelo, sólo si al compararlo con las entidades reales **coinciden tanto los límites de la entidad como el tipo.**\n","\n","Para ejemplificar, tomando el caso anterior, si el modelo es capaz de encontrar la siguiente entidad: $(2, 3, Disease)$, entonces se considera incorrecto ya que pudo predecir dos de los tres tokens de dicha enfermedad. Es decir, buscamos una métrica que sea alta a nivel de entidad y no a nivel de token.\n","\n","Antes de pasar a explicar las reglas, se recomienda visitar los siguientes links para entender bien el baseline de la tarea:\n","\n","-  [Recurrent Neural Networks](slides/NLP-RNN.pdf): [Parte 1](https://youtu.be/BmhjUkzz3nk), [Parte 2](https://youtu.be/z43YFR1iIvk), [Parte 3](https://youtu.be/7L5JxQdwNJk)\n","\n","\n","Recuerden que todo el material se encuentra disponible en el [github del curso](https://github.com/dccuchile/CC6205)."],"metadata":{"id":"KbBq7gcHnPTf"}},{"cell_type":"markdown","source":["## Baseline\n","\n","En este punto esperamos que tengan conocimiento sobre redes neuronales y en particular redes neuronales recurrentes (RNN), si no siempre pueden escribirnos por el canal de Discord para aclarar dudas. La RNN del baseline adjunto a este notebook está programado en la librería [`pytorch`](https://pytorch.org/) pero ustedes pueden utilizar keras, tensorflow si así lo desean. El código contiene lo siguiente:\n","\n","- La carga de los datasets, creación de batches de texto y padding (esto es importante ya que si utilizan redes neuronales tienen que tener el mismo largo los inputs).\n","\n","- La implementación básica de una red `LSTM` simple de solo un nivel y sin bi-direccionalidad.\n","\n","- La construcción del formato del output requerido para que lo puedan probar en la tarea en codalab."],"metadata":{"id":"NXgaywGdo0bh"}},{"cell_type":"markdown","source":["### **Sugerencias**\n","Se espera que ustedes puedan experimentar con el baseline utilizando (pero no limitándose) estas sugerencias:\n","\n","*   Probar la técnica de early stopping.\n","*   Variar la cantidad de parámetros de la capa de embeddings.\n","*   Variar la cantidad de capas RNN.\n","*   Variar la cantidad de parámetros de las capas de RNN.\n","*   Inicializar la capa de embeddings con modelos pre-entrenados. (word2vec, glove, conceptnet, etc...). [Embeddings en español aquí](https://github.com/dccuchile/spanish-word-embeddings). También aquí pueden encontrar unos embeddings clínicos en Español: [https://zenodo.org/record/3924799](https://zenodo.org/record/3924799)\n","*   Variar la cantidad de épocas de entrenamiento.\n","*   Variar el optimizador, learning rate, batch size, etc.\n","*   Probar bi-direccionalidad.\n","*   Incluir dropout.\n","*   Probar modelos de tipo GRU.\n","*   Probar usando capas de atención.\n","*   Probar Embedding Contextuales (les puede ser de utilidad [flair](https://github.com/flairNLP/flair))\n","*   Probar modelos de transformers en español usando [Huggingface](https://github.com/huggingface/transformers) o el framework Flair."],"metadata":{"id":"TfMlGTCgIq8o"}},{"cell_type":"markdown","source":["## Experimento base\n","\n","El código que les entregaremos servirá de baseline para luego implementar mejores modelos.\n","En general, el código asociado a la carga de los datos, las funciones de entrenamiento, de evaluación y la predicción de los datos de la tarea no deberían cambiar.\n","Solo deben preocuparse de cambiar la arquitectura del modelo, sus hiperparámetros y reportar, lo cual lo pueden hacer en las subsecciones *modelos*."],"metadata":{"id":"YWEFCu4_sWwm"}},{"cell_type":"markdown","source":["###  **Carga de datos y Preprocesamiento**\n","\n","Para cargar los datos y preprocesarlos usaremos la librería [`torchtext`](https://github.com/pytorch/text). Tener cuidado ya que hace algunos meses esta librería tuvo cambios radicales, quedando las funcionalidades pasadas depreciadas de la librería ```legacy```. Esto ya que si quieren usar más funciones de la librería entonces vean los cambios en la documentación debe usar la versión antigua con python 3.8\n","\n","El proceso será el siguiente:\n","\n","1. Descargar los datos desde github y examinarlos.\n","2. Cargar los datasets con la clase ```TaggingDataset``` de más abajo.\n","3. Crear el vocabulario."],"metadata":{"id":"_v9-YCA9sg-A"}},{"cell_type":"code","source":["!pip install -U torchtext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YLZkrIVQsqBB","executionInfo":{"status":"ok","timestamp":1715012492696,"user_tz":240,"elapsed":181957,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"214640e6-1558-42b3-9af2-deabf65a9cd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n","Collecting torchtext\n","  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n","Collecting torch>=2.3.0 (from torchtext)\n","  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.3.0->torchtext)\n","  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Collecting triton==2.3.0 (from torch>=2.3.0->torchtext)\n","  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n","Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchtext\n","  Attempting uninstall: triton\n","    Found existing installation: triton 2.2.0\n","    Uninstalling triton-2.2.0:\n","      Successfully uninstalled triton-2.2.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.2.1+cu121\n","    Uninstalling torch-2.2.1+cu121:\n","      Successfully uninstalled torch-2.2.1+cu121\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.17.1\n","    Uninstalling torchtext-0.17.1:\n","      Successfully uninstalled torchtext-0.17.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n","torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 torchtext-0.18.0 triton-2.3.0\n"]}]},{"cell_type":"code","source":["import torch\n","import torchtext\n","from torchtext import data, datasets\n","\n","# Garantizar reproducibilidad de los experimentos\n","SEED = 1234\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-4DCjj5bsxNJ","executionInfo":{"status":"ok","timestamp":1715012495033,"user_tz":240,"elapsed":2348,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"e2142a01-7499-4a5f-d641-d1d8b5d77175"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/usr/local/lib/python3.10/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"]}]},{"cell_type":"markdown","source":["#### **Obtener datos**\n","\n","Descargamos los datos de entrenamiento, validación y prueba en nuestro directorio de trabajo"],"metadata":{"id":"TDCqI-1is2j_"}},{"cell_type":"code","source":["%%capture\n","\n","!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt -nc\n","!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt -nc"],"metadata":{"id":"OIMlUsp3s7J-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NUEVO DATALOADER Y OTRAS COSAS NECESARIAS\n","from collections import Counter, OrderedDict\n","\n","import torch\n","\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","from torchtext.vocab import vocab\n","\n","class TaggingDataset(Dataset):\n","    def __init__(self, paths, lower=False, separator=\" \", encoding=\"utf-8\"):\n","\n","        data = []\n","        for path in paths:\n","          with open(path, 'r', encoding=encoding) as file:\n","            text, tag = [], []\n","            for line in file:\n","                line = line.strip()\n","                if line == \"\":\n","                    data.append(dict({'text':text, 'nertags':tag}))\n","                    text, tag = [], []\n","                else:\n","                    line_content = line.split(separator) # .rstrip('\\n')\n","                    if lower:\n","                      text.append(line_content[0].lower())\n","                    else:\n","                      text.append(line_content[0])\n","                    tag.append(line_content[1])\n","          data.append(dict({'text':text, 'nertags':tag}))\n","\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        item = self.data[index]\n","        text = item[\"text\"]\n","        nertags = item[\"nertags\"]\n","        return nertags, text\n","\n","def fit_vocab(data_iter):\n","\n","  def update_counter(counter_obj):\n","    sorted_by_freq_tuples = sorted(counter_obj.items(),\n","                                  key=lambda x: x[1],\n","                                  reverse=True)\n","    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n","    return ordered_dict\n","\n","  counter_1 = Counter()\n","  counter_2 = Counter()\n","  for _nertags, _text in data_iter:\n","    counter_1.update(_text)\n","    counter_2.update(_nertags)\n","\n","  od1 = update_counter(counter_1)\n","  od2 = update_counter(counter_2)\n","\n","  v1 = vocab(od1, specials=['<PAD>', '<unk>'])\n","  v1.set_default_index(v1[\"<unk>\"])\n","  v2 = vocab(od2, specials=['<PAD>'])\n","\n","  text_pipeline = lambda x: v1(x)\n","  nertags_pipeline = lambda x: v2(x)\n","\n","  return text_pipeline, nertags_pipeline, v1, v2\n","\n","def collate_batch(batch, nertags_pipeline, text_pipeline, device):\n","  nertags_list, text_list = [], []\n","  for _nertags, _text in batch:\n","    processed_nertags = torch.tensor(nertags_pipeline(_nertags),\n","                                     dtype=torch.int64)\n","    nertags_list.append(processed_nertags)\n","    processed_text = torch.tensor(text_pipeline(_text),\n","                                  dtype=torch.int64)\n","    text_list.append(processed_text)\n","  nertags_list = pad_sequence(nertags_list, batch_first=True).T\n","  text_list = pad_sequence(text_list, batch_first=True).T\n","  return nertags_list.to(device), text_list.to(device)"],"metadata":{"id":"kn_YMpyetNhn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_iter = TaggingDataset([\"train.txt\", \"dev.txt\"])\n","\n","data_length = len(data_iter)\n","\n","train_length = int(data_length * 0.8)\n","dev_length = int(data_length * 0.1)\n","test_length = data_length - train_length - dev_length\n","\n","train_iter, dev_iter, test_iter = torch.utils.data.random_split(\n","    data_iter,\n","     (train_length, dev_length, test_length),\n","    torch.Generator().manual_seed(42))\n","\n","text_pipeline, nertags_pipeline, TEXT, NER_TAGS = fit_vocab(train_iter)"],"metadata":{"id":"ctd5gNmzxnnE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_iter), len(dev_iter), len(test_iter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-MPvr5LDkq5","executionInfo":{"status":"ok","timestamp":1715019847735,"user_tz":240,"elapsed":4,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"0895ea5d-18a7-46d5-c117-1137358ccb98"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7132, 891, 893)"]},"metadata":{},"execution_count":86}]},{"cell_type":"code","source":["# seteamos algunos valores de interes\n","UNK_IDX = TEXT.vocab.get_stoi()['<unk>']\n","PAD_IDX = TEXT.vocab.get_stoi()['<PAD>']\n","\n","PAD_TAG_IDX = NER_TAGS.get_stoi()['<PAD>']\n","O_TAG_IDX = NER_TAGS.vocab.get_stoi()['O']"],"metadata":{"id":"h-BjEO1exsuq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 22\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using', device)\n","\n","fix_collocate_batch = lambda x: collate_batch(x, nertags_pipeline, text_pipeline, device)\n","\n","dataloader_train = DataLoader(\n","    train_iter, batch_size=BATCH_SIZE, shuffle=False, collate_fn=fix_collocate_batch\n",")\n","dataloader_dev = DataLoader(\n","    dev_iter, batch_size=BATCH_SIZE, shuffle=False, collate_fn=fix_collocate_batch\n",")\n","dataloader_test = DataLoader(\n","    test_iter, batch_size=BATCH_SIZE, shuffle=False, collate_fn=fix_collocate_batch\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sD3AmVkOxsE8","executionInfo":{"status":"ok","timestamp":1715019853335,"user_tz":240,"elapsed":238,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"ee0dd657-5629-4958-f2fc-c5378e1e55be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda\n"]}]},{"cell_type":"code","source":["example = next(iter(dataloader_train))\n","ner_tags_example = example[0]\n","text_example = example[1]\n","\n","# revisamos el primer ejemplo\n","[NER_TAGS.vocab.get_itos()[j] for j in ner_tags_example[:, 1]], [TEXT.vocab.get_itos()[j] for j in text_example[:, 1]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_3XrV_YyyBWB","executionInfo":{"status":"ok","timestamp":1715020124681,"user_tz":240,"elapsed":268,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"7d7fa4db-54fb-4648-e73b-7a78bb04fa9e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['O',\n","  'O',\n","  'O',\n","  'O',\n","  'B-Disease',\n","  'I-Disease',\n","  'I-Disease',\n","  'I-Disease',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>'],\n"," ['SOLICITO',\n","  'EVALUACION',\n","  'Y',\n","  'TTO',\n","  'SUBLUXACION',\n","  'ATM',\n","  'IZQ',\n","  '.',\n","  ',',\n","  'DOLOR',\n","  'APERTURA',\n","  '.',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>',\n","  '<PAD>'])"]},"metadata":{},"execution_count":96}]},{"cell_type":"code","source":["!pip install seqeval"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9D81QD5Ky2_4","executionInfo":{"status":"ok","timestamp":1715020140033,"user_tz":240,"elapsed":4945,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"ec168808-cf0a-42c6-8bd2-3eb1d3fc5ad3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.25.2)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n"]}]},{"cell_type":"code","source":["# Definimos las métricas\n","\n","from seqeval.metrics import f1_score, precision_score, recall_score\n","\n","def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n","    \"\"\"\n","    Calcula precision, recall y f1 de cada batch.\n","    \"\"\"\n","\n","    # Obtener el indice de la clase con probabilidad mayor. (clases)\n","    y_pred = preds.argmax(dim=1, keepdim=True)\n","\n","    # filtramos <pad> para calcular los scores.\n","    mask = [(y_true != pad_idx)]\n","    y_pred = y_pred[mask]\n","    y_true = y_true[mask]\n","\n","    # traemos a la cpu\n","    y_pred = y_pred.view(-1).to('cpu').numpy()\n","    y_true = y_true.to('cpu').numpy()\n","    y_pred = [[NER_TAGS.vocab.get_itos()[v] for v in y_pred]]\n","    y_true = [[NER_TAGS.vocab.get_itos()[v] for v in y_true]]\n","\n","    # calcular scores\n","    f1 = f1_score(y_true, y_pred, mode='strict')\n","    precision = precision_score(y_true, y_pred, mode='strict')\n","    recall = recall_score(y_true, y_pred, mode='strict')\n","\n","    return precision, recall, f1"],"metadata":{"id":"YdWPWTMTy8Cm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Modelo Baseline**\n","\n","Teniendo ya cargado los datos, toca definir nuestro modelo. Este baseline tendrá una capa de embedding, unas cuantas LSTM y una capa de salida y usará dropout en el entrenamiento.\n","\n","Este constará de los siguientes pasos:\n","\n","1. Definir la clase que contendrá la red.\n","2. Definir los hiperparámetros e inicializar la red.\n","3. Definir el número de épocas de entrenamiento\n","4. Definir la función de loss.\n","\n","\n","\n","Recomendamos que para experimentar, encapsules los modelos en una sola variable y luego la fijes en model para entrenarla"],"metadata":{"id":"eN984Z0XzHkP"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","\n","# Definir la red\n","class NER_RNN(nn.Module):\n","    def __init__(self,\n","                 input_dim,\n","                 embedding_dim,\n","                 hidden_dim,\n","                 output_dim,\n","                 n_layers,\n","                 bidirectional,\n","                 dropout,\n","                 pad_idx):\n","\n","        super().__init__()\n","\n","        # Capa de embedding\n","        self.embedding = nn.Embedding(input_dim,\n","                                      embedding_dim,\n","                                      padding_idx=pad_idx,\n","                                      )\n","\n","        # Capa LSTM\n","        self.lstm = nn.LSTM(embedding_dim,\n","                           hidden_dim,\n","                           num_layers=n_layers,\n","                           bidirectional=bidirectional,\n","                           dropout = dropout if n_layers > 1 else 0)\n","\n","        # Capa de salida\n","        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n","                            output_dim)\n","\n","        # Dropout\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, text):\n","\n","        #text = [sent len, batch size]\n","\n","        # Convertir lo enviado a embedding\n","        embedded = self.dropout(self.embedding(text))\n","\n","        outputs, (hidden, cell) = self.lstm(embedded)\n","        #embedded = [sent len, batch size, emb dim]\n","\n","        # Pasar los embeddings por la rnn (LSTM)\n","\n","        #output = [sent len, batch size, hid dim * n directions]\n","        #hidden/cell = [n layers * n directions, batch size, hid dim]\n","\n","        # Predecir usando la capa de salida.\n","        predictions = self.fc(self.dropout(outputs))\n","        #predictions = [sent len, batch size, output dim]\n","\n","        return predictions"],"metadata":{"id":"G3vNCpAyy7_L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hiperparámetros de la red"],"metadata":{"id":"3Ofso-aozeSL"}},{"cell_type":"code","source":["# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n","INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 300  # dimensión de los embeddings.\n","HIDDEN_DIM = 256  # dimensión de la capas LSTM\n","OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n","\n","N_LAYERS = 3  # número de capas.\n","DROPOUT = 0.6\n","BIDIRECTIONAL = True\n","\n","# Creamos nuestro modelo.\n","baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n","                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n","\n","baseline_model_name = 'baseline'  # nombre que tendrá el modelo guardado...\n","\n","baseline_n_epochs = 10"],"metadata":{"id":"9ryN9aKzy773"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Definimos la función de loss"],"metadata":{"id":"TfGxRLBwzhsg"}},{"cell_type":"code","source":["# Loss: Cross Entropy\n","TAG_PAD_IDX = NER_TAGS.vocab.get_stoi()['<PAD>']\n","baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"],"metadata":{"id":"LtfG9RUQy7vx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Entrenamos y evaluamos**\n","\n","\n","**Importante** : Fijen el modelo, el número de épocas de entrenamiento, la loss y el optimizador que usarán para entrenar y evaluar en las siguientes variables!!!"],"metadata":{"id":"Dg2Lm8Yszu11"}},{"cell_type":"code","source":["model = baseline_model\n","model_name = baseline_model_name\n","criterion = baseline_criterion\n","n_epochs = baseline_n_epochs"],"metadata":{"id":"1-TnXy2LzvXO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Inicializamos la red**\n","\n","Iniciamos los pesos de la red de forma aleatoria (Usando una distribución normal)."],"metadata":{"id":"x3wQq18i0Ehc"}},{"cell_type":"code","source":["def init_weights(m):\n","    # Inicializamos los pesos como aleatorios\n","    for name, param in m.named_parameters():\n","        nn.init.normal_(param.data, mean=0, std=0.1)\n","\n","    # Seteamos como 0 los embeddings de UNK y PAD.\n","    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","\n","model.apply(init_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-kgtnu2L0E-B","executionInfo":{"status":"ok","timestamp":1715020153269,"user_tz":240,"elapsed":282,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"2e308dba-6a68-4cda-d05a-c1e7b5295850"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NER_RNN(\n","  (embedding): Embedding(16496, 300, padding_idx=0)\n","  (lstm): LSTM(300, 256, num_layers=3, dropout=0.6, bidirectional=True)\n","  (fc): Linear(in_features=512, out_features=12, bias=True)\n","  (dropout): Dropout(p=0.6, inplace=False)\n",")"]},"metadata":{},"execution_count":104}]},{"cell_type":"code","source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O2TB3-J80aBB","executionInfo":{"status":"ok","timestamp":1715020155319,"user_tz":240,"elapsed":386,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"9ea079f4-8585-41a5-d894-5ab6175f7f0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["El modelo actual tiene 9,251,660 parámetros entrenables.\n"]}]},{"cell_type":"markdown","source":["Notar que definimos los embeddings que representan a \\<unk\\> y \\<pad\\>  como [0, 0, ..., 0]"],"metadata":{"id":"Gnuk_Hs10djW"}},{"cell_type":"markdown","source":["Definimos el optimizador"],"metadata":{"id":"7qJhs1OX0gls"}},{"cell_type":"code","source":["# Optimizador\n","optimizer = optim.Adam(model.parameters())"],"metadata":{"id":"xBAYockO0mE7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Enviamos el modelo a cuda"],"metadata":{"id":"RGj6tfdd0qJb"}},{"cell_type":"code","source":["# Enviamos el modelo y la loss a cuda (en el caso en que esté disponible)\n","model = model.to(device)\n","criterion = criterion.to(device)"],"metadata":{"id":"_68iNmcN0qkm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Definimos el entrenamiento de la red**\n","\n","Algunos conceptos previos:\n","\n","- `epoch` : una pasada de entrenamiento completa de una dataset.\n","- `batch`: una fracción de la época. Se utilizan para entrenar mas rápidamente la red. (mas eficiente pasar n datos que uno en cada ejecución del backpropagation)\n","\n","Esta función está encargada de entrenar la red en una época. Para esto, por cada batch de la época actual, predice los tags del texto, calcula su loss y luego hace backpropagation para actualizar los pesos de la red.\n","\n","Observación: En algunos comentarios aparecerá el tamaño de los tensores entre corchetes"],"metadata":{"id":"BuKvsbrw0wHw"}},{"cell_type":"code","source":["def train(model, iterator, optimizer, criterion):\n","\n","    epoch_loss = 0\n","    epoch_precision = 0\n","    epoch_recall = 0\n","    epoch_f1 = 0\n","\n","    model.train()\n","\n","    # Por cada batch del iterador de la época:\n","    for tags, text in iterator:\n","        # Reiniciamos los gradientes calculados en la iteración anterior\n","        optimizer.zero_grad()\n","\n","        #text = [sent len, batch size]\n","\n","        # Predecimos los tags del texto del batch.\n","        predictions = model(text.to(device))\n","\n","        #predictions = [sent len, batch size, output dim]\n","        #tags = [sent len, batch size]\n","\n","        # Reordenamos los datos para calcular la loss\n","        predictions = predictions.view(-1, predictions.shape[-1])\n","        #ipdb.set_trace()\n","        tags = torch.reshape(tags, (-1,)).to(device)\n","\n","        #predictions = [sent len * batch size, output dim]\n","\n","\n","\n","        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n","        loss = criterion(predictions, tags)\n","\n","        # Calculamos el accuracy\n","        precision, recall, f1 = calculate_metrics(predictions, tags)\n","\n","        # Calculamos los gradientes\n","        loss.backward()\n","\n","        # Actualizamos los parámetros de la red\n","        optimizer.step()\n","\n","        # Actualizamos el loss y las métricas\n","        epoch_loss += loss.item()\n","        epoch_precision += precision\n","        epoch_recall += recall\n","        epoch_f1 += f1\n","\n","    return ( epoch_loss / len(iterator), epoch_precision / len(iterator),\n","              epoch_recall / len(iterator), epoch_f1 / len(iterator) )"],"metadata":{"id":"ShIJbx2v0wlf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Definimos la función de evaluación**\n","\n","Evalua el rendimiento actual de la red usando los datos de validación.\n","\n","Por cada batch de estos datos, calcula y reporta el loss y las métricas asociadas al conjunto de validación.\n","Ya que las métricas son calculadas por cada batch, estas son retornadas promediadas por el número de batches entregados. (ver linea del return)"],"metadata":{"id":"IWSosAit0xCJ"}},{"cell_type":"code","source":["def evaluate(model, iterator, criterion):\n","\n","    epoch_loss = 0\n","    epoch_precision = 0\n","    epoch_recall = 0\n","    epoch_f1 = 0\n","\n","    model.eval()\n","\n","    # Indicamos que ahora no guardaremos los gradientes\n","    with torch.no_grad():\n","        # Por cada batch\n","        for tags, text in iterator:\n","            # Predecimos\n","            predictions = model(text.to(device))\n","\n","            predictions = predictions.view(-1, predictions.shape[-1])\n","            tags = torch.reshape(tags, (-1,)).to(device)\n","\n","            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n","            loss = criterion(predictions, tags)\n","\n","            # Calculamos las métricas\n","            precision, recall, f1 = calculate_metrics(predictions, tags)\n","\n","            # Actualizamos el loss y las métricas\n","            epoch_loss += loss.item()\n","            epoch_precision += precision\n","            epoch_recall += recall\n","            epoch_f1 += f1\n","\n","    return epoch_loss / len(iterator), epoch_precision / len(\n","        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"],"metadata":{"id":"Z3fRycaX0xbA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"metadata":{"id":"tA8nad2A1HUm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Entrenamiento de la red**\n","\n","En este cuadro de código ejecutaremos el entrenamiento de la red.\n","Para esto, primero definiremos el número de épocas y luego por cada época, ejecutaremos `train` y `evaluate`.\n","\n","**Importante: Reiniciar los pesos del modelo**\n","\n","Si ejecutas nuevamente esta celda, se seguira entrenando el mismo modelo una y otra vez.\n","Para reiniciar el modelo se debe ejecutar nuevamente la celda que contiene la función `init_weights`."],"metadata":{"id":"2twwfpmz1E7K"}},{"cell_type":"code","source":["best_valid_loss = float('inf')\n","\n","for epoch in range(n_epochs):\n","\n","    start_time = time.time()\n","\n","    # Recuerdo: dataloader_train y valid_iterator contienen el dataset dividido en batches.\n","\n","    # Entrenar\n","    train_loss, train_precision, train_recall, train_f1 = train(\n","        model, dataloader_train, optimizer, criterion)\n","\n","    # Evaluar (valid = validación)\n","    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n","        model, dataloader_dev, criterion)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n","    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de código.\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n","    # Si ya no mejoramos el loss de validación, terminamos de entrenar.\n","\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(\n","        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n","    )\n","    print(\n","        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0Py5sCE1FTe","executionInfo":{"status":"ok","timestamp":1715020264671,"user_tz":240,"elapsed":96067,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"36fdecf3-512a-41fc-fe33-9d815d97807e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <PAD> seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.893 | Train f1: 0.36 | Train precision: 0.56 | Train recall: 0.28\n","\t Val. Loss: 0.579 |  Val. f1: 0.60 |  Val. precision: 0.74 | Val. recall: 0.52\n","Epoch: 02 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.531 | Train f1: 0.63 | Train precision: 0.73 | Train recall: 0.56\n","\t Val. Loss: 0.400 |  Val. f1: 0.72 |  Val. precision: 0.77 | Val. recall: 0.69\n","Epoch: 03 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.365 | Train f1: 0.74 | Train precision: 0.79 | Train recall: 0.70\n","\t Val. Loss: 0.356 |  Val. f1: 0.75 |  Val. precision: 0.78 | Val. recall: 0.73\n","Epoch: 04 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.277 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.78\n","\t Val. Loss: 0.352 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.75\n","Epoch: 05 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.222 | Train f1: 0.84 | Train precision: 0.85 | Train recall: 0.83\n","\t Val. Loss: 0.344 |  Val. f1: 0.78 |  Val. precision: 0.79 | Val. recall: 0.76\n","Epoch: 06 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.178 | Train f1: 0.87 | Train precision: 0.87 | Train recall: 0.87\n","\t Val. Loss: 0.360 |  Val. f1: 0.78 |  Val. precision: 0.80 | Val. recall: 0.76\n","Epoch: 07 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.152 | Train f1: 0.89 | Train precision: 0.89 | Train recall: 0.89\n","\t Val. Loss: 0.382 |  Val. f1: 0.78 |  Val. precision: 0.80 | Val. recall: 0.77\n","Epoch: 08 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.129 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.91\n","\t Val. Loss: 0.388 |  Val. f1: 0.78 |  Val. precision: 0.79 | Val. recall: 0.77\n","Epoch: 09 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.116 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n","\t Val. Loss: 0.379 |  Val. f1: 0.78 |  Val. precision: 0.77 | Val. recall: 0.79\n","Epoch: 10 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.101 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n","\t Val. Loss: 0.409 |  Val. f1: 0.78 |  Val. precision: 0.78 | Val. recall: 0.78\n"]}]},{"cell_type":"markdown","source":["**Importante**: Recuerden que el último modelo entrenado no es el mejor (probablemente esté *overfitteado*), si no el que guardamos con la menor loss del conjunto de validación. Este problema lo pueden solucionar con *early stopping*.\n","Para cargar el mejor modelo entrenado, ejecuten la siguiente celda."],"metadata":{"id":"zMU8i9eV-xYq"}},{"cell_type":"code","source":["# cargar el mejor modelo entrenado.\n","model.load_state_dict(torch.load('{}.pt'.format(model_name)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tqJU5AEY-bwK","executionInfo":{"status":"ok","timestamp":1715020264673,"user_tz":240,"elapsed":13,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"feb7cf6b-d543-4c4a-c05d-b1bec86525ca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":112}]},{"cell_type":"code","source":["# Limpiar ram de cuda\n","torch.cuda.empty_cache()"],"metadata":{"id":"JX84ETMg-1Qv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uVoujP7IjPM3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Evaluamos el set de validación con el modelo final**\n","\n","Estos son los resultados de predecir el dataset de evaluación con el *mejor* modelo entrenado."],"metadata":{"id":"_d9Yfz95_GIz"}},{"cell_type":"code","source":["valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n","    model, dataloader_dev, criterion)\n","\n","print(\n","    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6mMnQIC-2G2","executionInfo":{"status":"ok","timestamp":1715020266118,"user_tz":240,"elapsed":1453,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"726443ef-bfe6-4b1b-a687-b2e229859490"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Val. Loss: 0.344 |  Val. f1: 0.78 | Val. precision: 0.79 | Val. recall: 0.76\n"]}]},{"cell_type":"markdown","source":["#### **Evaluamos el set de prueba con el modelo final**\n","\n","Estos son los resultados de predecir el dataset de prueba con el *mejor* modelo entrenado."],"metadata":{"id":"K7FzQVxK-2kc"}},{"cell_type":"code","source":["prueba_loss, prueba_precision, prueba_recall, prueba_f1 = evaluate(\n","    model, dataloader_test, criterion)\n","\n","print(\n","    f'Val. Loss: {prueba_loss:.3f} |  Val. f1: {prueba_f1:.2f} | Val. precision: {prueba_precision:.2f} | Val. recall: {prueba_recall:.2f}'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"meu3CsgT-3PE","executionInfo":{"status":"ok","timestamp":1715020309098,"user_tz":240,"elapsed":699,"user":{"displayName":"Felipe Urrutia","userId":"01225021230773564798"}},"outputId":"d67789ea-7837-40c6-8ddb-f56a77e93873"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Val. Loss: 0.358 |  Val. f1: 0.76 | Val. precision: 0.78 | Val. recall: 0.75\n"]}]},{"cell_type":"markdown","source":["## Experimentos\n","\n","En esta sección deberan explicar, implementar y evaluar **TRES EXPERIMENTO DIFERENTES**. Puede revisar la [lista de sugerencias](#scrollTo=TfMlGTCgIq8o) al inicio del enunciado para sacar ideas de experimentos. Se espera que los experimentos sean relevantes y desafiantes.\n","\n","Cada experimento debe contener las siguientes cuatro subsecciones:\n","\n","- *Explicación del experimento*: debe motivar y explicar en que consiste su experimento. La explicación debe fundamentar porque es un experimento relevante y desafiantes, ademas de aclarar que estudiaran.\n","\n","- *Implementacion del experimento*: debe implementar su experimento con codigo debidamente comentado, con tal de que sea legible.\n","\n","- *Evaluar el experimento*: debe evaluar con las metricas propuestas para generar los resultados obtenidos con sus experimentos sobre el conjunto de prueba.\n","\n","- *Analizar el experimento*: debe hacer un breve analisis de los resultados obtenidos."],"metadata":{"id":"F2bRYNRnKRzK"}},{"cell_type":"markdown","source":["### **Experimento 1**"],"metadata":{"id":"QwdOcPfbL3pH"}},{"cell_type":"markdown","source":["#### Explicación"],"metadata":{"id":"Gkn38sTsdpzR"}},{"cell_type":"markdown","source":["> Escribir explicación aquí"],"metadata":{"id":"r9Cr3jl1d0F3"}},{"cell_type":"markdown","source":["#### Implementación"],"metadata":{"id":"Q5dlEgEqdtvp"}},{"cell_type":"code","source":["### Escribir implenetación desde aquí"],"metadata":{"id":"dhuuMKVAdvqt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluación"],"metadata":{"id":"37IjYq-_d5Pv"}},{"cell_type":"code","source":["prueba_loss, prueba_precision, prueba_recall, prueba_f1 = evaluate(\n","    model_experimento_1, dataloader_test, criterion)\n","\n","print(\n","    f'Val. Loss: {prueba_loss:.3f} |  Val. f1: {prueba_f1:.2f} | Val. precision: {prueba_precision:.2f} | Val. recall: {prueba_recall:.2f}'\n",")"],"metadata":{"id":"kGz_8OB7eItO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Análisis"],"metadata":{"id":"pURRQLkReNMY"}},{"cell_type":"markdown","source":["> Escribir análisis aquí"],"metadata":{"id":"wfTrOktPeSB7"}},{"cell_type":"markdown","source":["### **Experimento 2**"],"metadata":{"id":"JIFrDPaGej4y"}},{"cell_type":"markdown","source":["#### Explicación"],"metadata":{"id":"M-dsNSEdej4z"}},{"cell_type":"markdown","source":["> Escribir explicación aquí"],"metadata":{"id":"c7JD-PzGej4z"}},{"cell_type":"markdown","source":["#### Implementación"],"metadata":{"id":"PKZIGOUpej4z"}},{"cell_type":"code","source":["### Escribir implenetación desde aquí"],"metadata":{"id":"B_Uyw10-ej40"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluación"],"metadata":{"id":"epaoMVyMej40"}},{"cell_type":"code","source":["prueba_loss, prueba_precision, prueba_recall, prueba_f1 = evaluate(\n","    model_experimento_2, dataloader_test, criterion)\n","\n","print(\n","    f'Val. Loss: {prueba_loss:.3f} |  Val. f1: {prueba_f1:.2f} | Val. precision: {prueba_precision:.2f} | Val. recall: {prueba_recall:.2f}'\n",")"],"metadata":{"id":"BF8M8iLGej40"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Análisis"],"metadata":{"id":"bqymD_Nuej41"}},{"cell_type":"markdown","source":["> Escribir análisis aquí"],"metadata":{"id":"88tsCz4eej41"}},{"cell_type":"markdown","source":["### **Experimento 3**"],"metadata":{"id":"ZcPtyefAenbW"}},{"cell_type":"markdown","source":["#### Explicación"],"metadata":{"id":"eYUFyLMJenbX"}},{"cell_type":"markdown","source":["> Escribir explicación aquí"],"metadata":{"id":"fuI3TaAhenbY"}},{"cell_type":"markdown","source":["#### Implementación"],"metadata":{"id":"800wWbuGenbY"}},{"cell_type":"code","source":["### Escribir implenetación desde aquí"],"metadata":{"id":"zON_wa1penbY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluación"],"metadata":{"id":"DYBzB3SlenbZ"}},{"cell_type":"code","source":["prueba_loss, prueba_precision, prueba_recall, prueba_f1 = evaluate(\n","    model_experimento_3, dataloader_test, criterion)\n","\n","print(\n","    f'Val. Loss: {prueba_loss:.3f} |  Val. f1: {prueba_f1:.2f} | Val. precision: {prueba_precision:.2f} | Val. recall: {prueba_recall:.2f}'\n",")"],"metadata":{"id":"0w69TMqoenba"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Análisis"],"metadata":{"id":"YxQkQt1Benba"}},{"cell_type":"markdown","source":["> Escribir análisis aquí"],"metadata":{"id":"cT-giD-Tenbb"}}]}