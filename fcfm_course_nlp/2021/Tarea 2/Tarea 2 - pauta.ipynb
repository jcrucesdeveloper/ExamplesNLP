{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tarea 2 - pauta.ipynb","provenance":[{"file_id":"1jZG4e_poENFYDeljHjx9goskyQ3Jy3GU","timestamp":1618284831252}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2obO44rRIDIm"},"source":["# **Tarea 2 - CC6205 Natural Language Processing 📚**\n","\n","**Integrantes:**\n","\n","**Fecha límite de entrega 📆:** Lunes 03 de mayo.\n","\n","**Tiempo estimado de dedicación:**"]},{"cell_type":"markdown","metadata":{"id":"Zpupcv6QW2fd"},"source":["Bienvenid@s a la segunda tarea del curso de Natural Language Processing (NLP). En esta tarea estaremos modelando probabilísticamente el lenguaje mediante **Languaje Modeling** y clasificando textos mediante el método **Näive Bayes**. Específicamente, la tarea consta de una parte teórica que busca evaluar conceptos vistos en clases sobre los **Language Models** y una parte práctica donde **programeran a mano** el método **Näive Bayes**. \n","\n","**Instrucciones:**\n","- La tarea se realiza en grupos de **máximo** 2 personas. Puede ser invidivual pero no es recomendable.\n","- La entrega es a través de u-cursos a más tardar el día estipulado arriba. No se aceptan atrasos.\n","- El formato de entrega es este mismo Jupyter Notebook.\n","- Al momento de la revisión tu código será ejecutado. Por favor verifica que tu entrega no tenga errores de compilación. \n","- Está **PROHIBIDO** usar cualquier librería que implemente los algoritmos pedidos (Spacy, scikit, etc). Sólo se podrán utilizar las librerías importadas al inicio de la sección de práctica.\n","- En el horario de auxiliar pueden realizar consultas acerca de la tarea a través del canal de Discord del curso.\n","\n","Si aún no han visto las clases, se recomienda visitar los links de las referencias.\n","\n","**Referencias:**\n","\n","Slides:\n","    \n","- [Language Models](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/lmslides.pdf) (slides by Michael Collins)\n","- [Text Classification and Naive Bayes](https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf) (slides by Dan Jurafsky)\n","\n","Videos: \n","\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 1](https://www.youtube.com/watch?v=9E2jJ6kcb4Y&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=4)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 2](https://www.youtube.com/watch?v=ZWqbEQXLra0&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=5)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 3](https://www.youtube.com/watch?v=tsumFqwFlaA&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=6)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 4](https://www.youtube.com/watch?v=s3TWdv4sqkg&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=7)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 1](https://www.youtube.com/watch?v=kG9BK9Oy1hU)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 2](https://www.youtube.com/watch?v=Iqte5kKHvzE)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 3](https://www.youtube.com/watch?v=TSJg0_X3Abk)\n","\n","` ` "]},{"cell_type":"markdown","metadata":{"id":"JstKx3TiKcIp"},"source":["---------------------------\n","## Parte 1. Language Modeling (3 pts)\n","\n","En esta parte responderán preguntas **teóricas** sobre los Lenguage Models. Para comprender como funcionan muchas de las técnicas que veremos posteriormente durante el curso será muy útil que dominen estos modelos y sus fundamentos.\n","\n","Recuerden que los **Lenguage Models** básicamente nos permiten, dado un corpus, estimar un modelo probabilista al que le podemos pasar una oración y determinar que tan probable es que esa oración haya sido generada. Para esto, tenemos que un modelo de $n$-gramas puede ser definido por una *cadena de Márkov* de orden $n-1$.\n","\n","En clases vimos los modelos basados en unigramas, bigramas y trigramas. En esta pregunta trabajaremos con modelos de lenguaje basados en 4-gramas (*cadena de Márkov* de tercer orden).\n","\n","**Nota:** Las preguntas deben ser resueltas con desarrollo, pero sin código. Por favor usen $\\LaTeX$ para las fórmulas."]},{"cell_type":"markdown","metadata":{"id":"2hwW-07MrRpt"},"source":["\n","### 1.1 (1 pt)\n","\n","Asuma que tenemos calculados los parámetros $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$ para todas las posibles secuencias de tamaño 4 que aparecen en un corpus $\\mathcal{C}$. Dado esto, muestre cómo un modelo le asignaría una probabilidad a la frase `una persona corriendo rápido`"]},{"cell_type":"markdown","metadata":{"id":"YzlQDAVqWNdX"},"source":["` ` \n","\n","**Respuesta:** \n","\n","\\begin{equation}\n","escriba\\ su\\ respuesta\\ aquí\n","\\end{equation}\n","\n","` `"]},{"cell_type":"markdown","metadata":{"id":"5VUjDx0NrYbg"},"source":["**Respuesta:**\n","\n","\\begin{align}\n","q(\\text{una persona corriendo rápido}) = &q(\\text{una}|*,*,*) \\times q(\\text{persona}|*,*,\\text{una}) \\\\\n","&\\times q(\\text{corriendo}|*,\\text{una},\\text{persona}) \\\\\n","&\\times q(\\text{rápido}|\\text{una},\\text{persona},\\text{corriendo}) \\\\\n","&\\times q(\\text{STOP}|\\text{persona},\\text{corriendo},\\text{rápido})\n","\\end{align}\n"]},{"cell_type":"markdown","metadata":{"id":"lAej_jqtVwm1"},"source":["### 1.2 Estimando las probabilidades (1 pt)"]},{"cell_type":"markdown","metadata":{"id":"gXSFlCIex8kq"},"source":["#### 1.2.1 Modelo simple (0.5 pt)\n","\n","Si hubieses tenido que estimar las probabilidades condicionales (parámetros del modelo) $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$ a partir de $\\mathcal{C}$, ¿cómo la definirías siguiendo el principio de máxima verosimilitud?"]},{"cell_type":"markdown","metadata":{"id":"WYLKSFjJXDvn"},"source":["` ` \n","\n","**Respuesta:** \n","\n","\\begin{equation}\n","escriba\\ su\\ respuesta\\ aquí\n","\\end{equation}\n","\n","donde ....\n","\n","` `"]},{"cell_type":"markdown","metadata":{"id":"Y8cT1bkOXF4u"},"source":["` `\n","\n","**Respuesta:** \n","\n","\\begin{equation}\n","q(w_i | w_{i-3}, w_{i-2}, w_{i-1}) = \\frac{Count(w_{i-3}, w_{i-2}, w_{i-1}, w_i)}{Count(w_{i-3}, w_{i-2},w_{i-1})},\n","\\end{equation}\n","\n","donde $Count(w_{i-3}, w_{i-2}, w_{i-1}, w_i)$ representa la frecuencia de apariciones de la secuencia de tamaño 4 $(w_{i-3}, w_{i-2}, w_{i-1}, w_i)$ y $Count(w_{i-3}, w_{i-2}, w_{i-1})$ representa la frecuancia de apariciones de la secuencia de tamaño 3 $(w_{i-3}, w_{i-2}, w_{i-1})$ dentro del corpus $\\mathcal{C}$.\n","\n","` `"]},{"cell_type":"markdown","metadata":{"id":"bwNkPIXure0C"},"source":["#### 1.2.2 Modelo interpolado (0.5 pt)\n","Muestre cómo se calcularía $q(w_{i} | w_{i-3}, w_{i-2}, w_{i-1})$ usando un modelo que interpola modelos de lenguajes basados en 4-grams, trigramas, bigramas y unigramas. Te fue necesario definir nuevos parámetros? En caso afirmativo, qué los diferencia de los parámetros del modelo simple y que propiedades deben cumplir?"]},{"cell_type":"markdown","metadata":{"id":"zeLZAd0Tr9ne"},"source":["` ` \n","\n","**Respuesta:** \n","\n","$escriba\\ su\\ respuesta\\ aquí$\n","\n","donde ...\n","\n","` ` \n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZeJI3-C2XAa1"},"source":["` ` \n","\n","**Respuesta:** \n","\n","\\begin{align}\n","q(w_{i} | w_{i-3}, w_{i-2}, w_{i-1}) &= \\lambda_1 \\times q_{ML}(w_{i} | w_{i-3}, w_{i-2}, w_{i-1}) \\times \\lambda_2 \\times q_{ML}(w_{i} | w_{i-2}, w_{i-1})+ \\lambda_3 \\times q_{ML}(w_{i} | w_{i-1}) + \\lambda_4 \\times q_{ML}(w_{i})\\\\\n","&=\\lambda_1 \\times \\frac{Count(w_{i-3}, w_{i-2}, w_{i-1}, w_i)}{Count(w_{i-3}, w_{i-2}, w_{i-1})} \\\\\n","&\\ \\ + \\lambda_2 \\times \\frac{Count(w_{i-2}, w_{i-1}, w_i)}{Count(w_{i-2}, w_{i-1})} \\\\\n","&\\ \\ + \\lambda_3 \\times \\frac{Count(w_{i-1}, w_i)}{Count(w_{i-1})} \\\\\n","&\\ \\ + \\lambda_4~\\times~\\frac{Count(w_i)}{Count()},\n","\\end{align}\n","\n","donde $\\lambda_1 + \\lambda_2 +\\lambda_3 +\\lambda_4 = 1$ con $\\lambda_1, \\lambda_2, \\lambda_3,\\lambda_4 \\geq 0$  y se diferencian de los otros parametros del modelo en que no necesitan estimarse en el entrenamiento del modelo, se pueden tratar como hiper-parámetros del modelo que debemos definir\n","\n","` ` "]},{"cell_type":"markdown","metadata":{"id":"sHqcRJ7Vr_8x"},"source":["### 1.3 (1 pt)\n","¿Qué ventajas tiene el modelo interpolado sobre el modelo de 4-gramas simple?"]},{"cell_type":"markdown","metadata":{"id":"6F5R3Ji7sHjt"},"source":["` ` \n","\n","**Respuesta:** \n","\n","escriba su respuesta aquí\n","\n","` ` "]},{"cell_type":"markdown","metadata":{"id":"wLedQWNJXLZF"},"source":["` ` \n","\n","**Respuesta:**\n","\n","En el modelo de 4-gramas uno debe extraer todos los 4-gramas y trigramas del corpus y contar cuantas veces aparecen. Esto se hace para determinar los parámetros $q$. Pero ocurre que muchos trigramas y bigramas no aparecen en el corpus, lo que hace que muchos parámetros $q$ sean cero. Por tanto, la productoria se nos va a cero también.\n","\n","Atendiendo a esto, tenemos que los modelos de trigramas se indefinen menos veces que los de 4-gramas, los de bigramas se indefinen menos veces que los de trigramas y los de unigramas se indefinen menos veces que los de bigramas. La idea de un modelo de interpolación es combinar los tres modelos en uno solo (mediante una ponderación lineal) para suavizar (disminuir) las inefiniciones. Con esto se aumenta la capacidad de generalización del modelo ya que puede entregarnos una probabilidad para un documento nuevo que puede contener 4-gramas nunca vistos.\n","\n","` ` "]},{"cell_type":"markdown","metadata":{"id":"mLPAel_RjFAO"},"source":["### 1.4 (otras posibles preguntas) (1 pt)\n","\n","¿Cómo podemos evaluar los Language Models?\n","\n","¿Cuándo una oración es plausible?\n","\n","¿Por qué no modelamos solo la probabilidad de las oraciones que existen en el curpus?\n","\n","¿Qué permiten los modelos de Markov de segundo orden?\n","\n","¿En qué consiste el proceso de padding y para que sirve?\n","\n","¿Qué significa $q(\\text{STOP}|\\text{el},\\text{perro})$?"]},{"cell_type":"markdown","metadata":{"id":"rdmB-07ZKfaa"},"source":["-----------------------\n","## Parte 2. Naive Bayes (3 pts)\n","En esta parte programaremos nuestro primer clasificador de documentos. Para esto implementaremos el método **Naive Bayes Multinomial** usando **Laplace Smothing**.\n","\n","Por favor, documenten su código. Escriban que hacen las funciones, que reciben, que entregan, etc. Si en algún lugar de su código hacen algo que creen que debe ser explicado, los comentarios son bienvenidos.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PpjjKnJUvRiA"},"source":["### 2.1 Clase para clasificador (0.5 pt)\n","\n","Programa una clase `MyMultinomialNB` que en su inicializador reciba el parametro `alpha` que representa ...\n","\n","```python\n","class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n","  def __init__(self, ...):\n","    ...\n","```\n","\n","Una llamada de ejemplo para crear un objeto de tu clase es:\n","```python\n","my_clf = MyMultinomialNB(alpha=1)\n","``` \n","lo que debiera crear un clasificador con parametro `alpha` igual a 1."]},{"cell_type":"markdown","metadata":{"id":"ROG50eH0xfxO"},"source":["### 2.2 Entrenamiento del clasificador (1 pt)\n","\n","Programa el entrenamiento de tu clasificador en el método `fit` de la clase `MyMultinomialNB`. La función debiera recibir el parámetro X que es un `DataFrame` de `pandas` con las columnas `words` y `class_`, donde `words` es una tupla con las palabras asociadas al cada documento y `class_` es el identificador de la clase a la que pertenece cada documento.\n","\n","Para computar el entrenamiento de nuestro clasificador debemos: \n","- extraer el vocabulario,\n","- determinar las probabilidades $p(c_j)$ para cada una de las clases posibles, \n","- determinar las probabilidades $p(w_i|c_j)$ para cada una de las palabras y cada una de las clases usando **Laplace Smothing**.\n","\n","El resultado del metodo `fit` será la misma instancia de nuestro clasificador `self`.\n","\n","```python\n","class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n","  def __init__(self, ...):\n","    ...\n","\n","  def fit(self, X):\n","    ...\n","    return self\n","```\n","\n","**Underflow prevention:** En vez de hacer muchas multiplicacions de `float`s, reemplácenlas por sumas de logaritmos para prevenir errores de precisión. Revisen la diapo 69 de las slides. "]},{"cell_type":"markdown","metadata":{"id":"FNouTCmR2FgY"},"source":["### 2.3 Predicción (1 pt)\n","\n","Programa la predicción de tu clasificador en el método `predict` de la clase `MyMultinomialNB`. Al igual que la función `fit`, `predict` debe recibir un `DataFrame` X con valores `None` en la columna `class_` y devolver una lista con las clases que maximizan la probabilidad de Bayes para cada uno de los elmentos de X (filas)."]},{"cell_type":"code","metadata":{"id":"DYFEgTyw2ELL","executionInfo":{"status":"ok","timestamp":1621281480544,"user_tz":240,"elapsed":958,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}}},"source":["# Acá implementarán las preguntas 2.1, 2.2 y 2.3,\n","# tu código debiera comenzar así\n","\n","# importamos algunos paquetes necesarios, puede que necesites otros\n","import numpy as np\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.utils.validation import check_is_fitted\n","\n","class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n","  def __init__(self, alpha=1.0):\n","    # acá tu código para inicializar el clasificador\n","    ...\n","\n","  def fit(self, X):\n","    # acá tu código para el entrenamiento del modelo\n","    ...\n","    \n","    return self\n","\n","  def predict(self, X):\n","    # Checkea que fit ha sido ejecutado anteriormente\n","    check_is_fitted(self)\n","\n","    # acá tu código para computar la predicción\n","    ...\n","    \n","    return prediction"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"0edu0E7LA3U9","executionInfo":{"status":"ok","timestamp":1621281481113,"user_tz":240,"elapsed":1524,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}}},"source":["import numpy as np\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.utils.validation import check_is_fitted\n","\n","# from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n","# from sklearn.utils.multiclass import unique_labels\n","\n","class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n","  def __init__(self, alpha=1.0):\n","    self.alpha_ = alpha\n","\n","  def fit(self, X):\n","    # Store the classes seen during fit\n","    self.groups = X.groupby('class_')\n","    self.classes_ = list(self.groups['class_'])\n","    self.n_classes = len(self.classes_)\n","\n","    # extract vocabulary\n","    self.vocab = list(set(X['words'].sum()))\n","    self.alpha_V = self.alpha_ * len(self.vocab)\n","    \n","    # compute the count of documents of class c_j \n","    docs_count = self.groups.count()\n","\n","    # the total count of documents in train set\n","    N_docs = X.shape[0]\n","\n","    # for each class (topic) concat all documents in a big doc c_j\n","    big_docs = self.groups.sum()\n","\n","    self.log_p_c, self.count_c_w, self.log_l_c_ls, self.log_p_c_w_ls = [], [], [], []\n","    for c in range(self.n_classes):\n","      # compute P(c_j) = docs_count(C==c_j) / N_docs\n","      self.log_p_c.append(np.log(docs_count['words'][c] / N_docs))\n","\n","      # compute the frecuency of word w_i in the big doc c_j\n","      count_w_ls = {w: self.alpha_ for w in self.vocab}\n","      for t in big_docs['words'][c]:\n","        count_w_ls[t] += 1\n","\n","      # compute the length of the big doc c_j (the sum of frecuency of all words)\n","      log_l_ls = np.log(len(big_docs['words'][c]) + self.alpha_V)\n","      self.log_l_c_ls.append(log_l_ls)\n","\n","      # compute P(w_i|c_j) = count(w_i,c_j) / sum_w(count(w,C_j))\n","      # considering Laplace (add alpha) smoothing (counters were initialized to alpha)\n","      # => P(w_i|c_j) = (count(w_i,c_j) + alpha) / (sum_w(count(w,C_j)) + alpha*|V|)\n","      log_p_w_ls = {w: np.log(count_w_ls[w]) - log_l_ls for w in self.vocab}\n","      self.log_p_c_w_ls.append(log_p_w_ls)\n","\n","    # Return the classifier\n","    return self\n","\n","  def __log_pi(self, c, d):\n","    # implementation of equation\n","    # log(pi_i(P(x_i|c_j) * P(c_j)) = log(pi_i(P(x_i|c_j)) + log(P(c_j)) = sum(log(P(x_i|c_j))) + log(P(c_j))\n","\n","    # considering the words that don't appear in the vocabulary\n","    # return np.sum([self.log_p_c_w_ls[c][w] if w in self.vocab else (np.log(self.alpha_) - self.log_l_c_ls[c]) for w in d]) + self.log_p_c[c]\n","\n","    # without considering the words that don't appear in the vocabulary\n","    return np.sum([self.log_p_c_w_ls[c][w] for w in d if w in self.vocab]) + self.log_p_c[c]\n","\n","  def predict_log_proba(self, X):\n","    # Check is fit had been called\n","    check_is_fitted(self)\n","\n","    log_probs = []\n","    for doc in X['words']:\n","      # compute probability of each doc by using mariginalization\n","      # P(d) = sum_j(P(d,c_j)) = sum_j(P(d|c_j) * P(c_j)) = sum_j(pi_i(P(x_i|c_j) * P(c_j))\n","      log_pi_c = [self.__log_pi(c, doc) for c in range(self.n_classes)]  # log(pi_i(P(x_i|c_j) * P(c_j))\n","      log_p_d = np.log(np.sum(np.exp(log_pi_c)))  # log(sum(exp(log(pi_i(P(x_i|c_j) * P(c_j)))))\n","\n","      # compute Bayes probabilites for each class c_j\n","      # P(d|c_j) * P(c_j) / P(d) = pi_i(P(x_i|c_j)) * P(c_j) / P(d)\n","      log_p_d_c = [log_pi_c[c] - log_p_d for c in range(self.n_classes)]\n","      log_probs.append(log_p_d_c)\n","\n","    return log_probs\n","\n","  def predict_proba(self, X):\n","    return np.exp(self.predict_log_proba(X))\n","\n","  def predict(self, X):\n","    # Check is fit had been called\n","    check_is_fitted(self)\n","\n","    # compute prediction, find the class c that maximize Bayes prob\n","    # it is not nedded to compute the exact probabilities\n","    # => argmax_c P(c) * pi_i(P(x_i|c))\n","    pred = []\n","    for doc in X['words']:\n","      max_log_p, max_c = float('-inf'), -1\n","      for c in range(self.n_classes):\n","        log_pi = self.__log_pi(c, doc)\n","        if log_pi > max_log_p:\n","          max_log_p, max_c = log_pi, c\n","      pred.append(self.classes_[max_c][0])\n","\n","    return pred"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_KOMJ-CS8PRP"},"source":["### 2.4 Probando el clasificador (0.5 pt)"]},{"cell_type":"markdown","metadata":{"id":"hucdz-R7xerG"},"source":["A continuación probarán el funcionamiento de su clasificador. Para esto, les presentamos un conjunto de documentos de entrenamiento `train_set` divididos en 2 categorias distintas. Ustedes deberán primero entrenar su clasificador usando el método `fit` de su clase y luego, clasificar los documentos del conjunto de prueva `test_set` usando el método `predict`.\n","\n","**NOTA:** Como pueden ver, los objetos `namedtuple`s tienen dos atributos: `words` donde están las palabras del documento y `class_` donde se guarda la clase de ese documento. Estos objetos son inmutables, lo que quiere decir que si quieren modificar un documento y cambiarle la clase, tienen que crear otro documento. Otra cosa es que son tuplas como cualquier otra, es decir se pueden acceder usando indices como `doc[0]` o `doc[1]`."]},{"cell_type":"code","metadata":{"id":"HLi8PxdV2VQX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621281481114,"user_tz":240,"elapsed":1520,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"14b6417e-ac6c-4cc6-d88d-e6b71c2441b8"},"source":["import pandas as pd\n","\n","from collections import namedtuple\n","document = namedtuple(\n","    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",")\n","\n","train_set = (\n","    document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n","    document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n","    document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n","    document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n","    document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n","    document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=1),\n","    document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n","    document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1)\n",")\n","X_train = pd.DataFrame(data=train_set)\n","print(\"Documentos de entrenamiento\")\n","print(X_train)\n","\n","test_set = (document(words=('w02', 'w09', 'w05', 'w01', 'w06', 'w05', 'w04', 'w00'), class_=None),\n","            document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),\n","            document(words=('w03', 'w03', 'w04', 'w05', 'w01', 'w06', 'w09', 'w02'), class_=None))\n","X_test = pd.DataFrame(data=test_set)\n","print(\"\\nDocumentos de prueba:\")\n","print(X_test)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Documentos de entrenamiento\n","                                           words  class_\n","0            (w03, w01, w02, w06, w02, w08, w07)       0\n","1  (w05, w04, w00, w06, w09, w07, w06, w09, w05)       1\n","2  (w07, w06, w00, w08, w01, w08, w08, w09, w02)       0\n","3            (w08, w09, w02, w06, w05, w08, w07)       1\n","4            (w09, w08, w05, w08, w05, w00, w08)       1\n","5            (w05, w05, w06, w01, w06, w08, w02)       1\n","6            (w04, w03, w07, w05, w04, w00, w02)       0\n","7       (w01, w00, w01, w04, w09, w02, w04, w07)       1\n","\n","Documentos de prueba:\n","                                      words class_\n","0  (w02, w09, w05, w01, w06, w05, w04, w00)   None\n","1  (w02, w09, w06, w01, w05, w04, w03, w03)   None\n","2  (w03, w03, w04, w05, w01, w06, w09, w02)   None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OXHwmOWB-4Aa","executionInfo":{"status":"ok","timestamp":1621281481114,"user_tz":240,"elapsed":1516,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"9b743664-4b11-4e66-a675-0e3d286b0b3c"},"source":["# Acá probarán su clasificador\n","\n","my_clf = MyMultinomialNB(alpha=1)\n","my_clf.fit(X_train)\n","\n","print('vocab: ', my_clf.vocab)\n","print('\\nTest probs:')\n","print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test)]))\n","print('\\nTest predictions:')\n","print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_clf.predict(X_test), X_test['words'])]))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["vocab:  ['w04', 'w03', 'w06', 'w07', 'w05', 'w00', 'w02', 'w08', 'w09', 'w01']\n","\n","Test probs:\n","[0.06194691 0.93805309]\n","[0.76018101 0.23981899]\n","[0.76018101 0.23981899]\n","\n","Test predictions:\n","1 <- w02 w09 w05 w01 w06 w05 w04 w00\n","0 <- w02 w09 w06 w01 w05 w04 w03 w03\n","0 <- w03 w03 w04 w05 w01 w06 w09 w02\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Js0W9LlXBgjk"},"source":["##### Comparación con sklearn"]},{"cell_type":"code","metadata":{"id":"u8Jmgl0KIYaV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621281481114,"user_tz":240,"elapsed":1512,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"cde8bfef-9906-4b60-c1c3-0c56342bfb56"},"source":["X_train = pd.DataFrame(data=train_set)\n","X_test = pd.DataFrame(data=test_set)\n","my_clf = MyMultinomialNB(alpha=1)\n","my_clf.fit(X_train)\n","print('vocab: ', my_clf.vocab)\n","print('\\nTest probs:')\n","print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test)]))\n","print('\\nTest predictions:')\n","print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_clf.predict(X_test), X_test['words'])]))\n","\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline\n","sk_clf = Pipeline([('vect', CountVectorizer()),\n","                    #  ('tfidf', TfidfTransformer()),\n","                     ('clf', MultinomialNB()),\n","                    ])\n","sk_clf.fit([' '.join(words) for words in X_train['words']], X_train['class_'])\n","print('\\nsklearn Test probs:')\n","X_test = [' '.join(words) for words in X_test['words']]\n","print('\\n'.join([str(l) for l in sk_clf.predict_proba(X_test)]))\n","print('\\nsklearn Test predictions:')\n","print('\\n'.join(['{} <- {}'.format(c, s) for c, s in zip(sk_clf.predict(X_test), X_test)]))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["vocab:  ['w04', 'w03', 'w06', 'w07', 'w05', 'w00', 'w02', 'w08', 'w09', 'w01']\n","\n","Test probs:\n","[0.06194691 0.93805309]\n","[0.76018101 0.23981899]\n","[0.76018101 0.23981899]\n","\n","Test predictions:\n","1 <- w02 w09 w05 w01 w06 w05 w04 w00\n","0 <- w02 w09 w06 w01 w05 w04 w03 w03\n","0 <- w03 w03 w04 w05 w01 w06 w09 w02\n","\n","sklearn Test probs:\n","[0.06194691 0.93805309]\n","[0.76018101 0.23981899]\n","[0.76018101 0.23981899]\n","\n","sklearn Test predictions:\n","1 <- w02 w09 w05 w01 w06 w05 w04 w00\n","0 <- w02 w09 w06 w01 w05 w04 w03 w03\n","0 <- w03 w03 w04 w05 w01 w06 w09 w02\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5tDZnmns_1dW"},"source":["#### (opcional) Oraciones reales\n","\n","Aquí intentaremos entrenar un clasificador para determinar cuando una oracion en inglés es interrogativa, afirmativa o negativa."]},{"cell_type":"code","metadata":{"id":"YCWi3oytd2nf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621281481115,"user_tz":240,"elapsed":1510,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"ff1dcf9d-df14-4b79-f48a-6ff00a5db03e"},"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","document = namedtuple(\n","    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",")\n","\n","train_set2 = [\n","              ['Do you have plenty of time?', '?'],\n","              ['Does she have enough money?','?'],\n","              ['Did they have any useful advice?','?'],\n","              ['What day is today?','?'],\n","              [\"I don't have much time\",'-'],\n","              [\"She doesn't have any money\",'-'],\n","              [\"They didn't have any advice to offer\",'-'],\n","              ['Have you plenty of time?','?'],\n","              ['Has she enough money?','?'],\n","              ['Had they any useful advice?','?'],\n","              [\"I haven't much time\",'-'],\n","              [\"She hasn't any money\",'-'],\n","              [\"He hadn't any advice to offer\",'-'],\n","              ['How are you?','?'],\n","              ['How do you make questions in English?','?'],\n","              ['How long have you lived here?','?'],\n","              ['How often do you go to the cinema?','?'],\n","              ['How much is this dress?','?'],\n","              ['How old are you?','?'],\n","              ['How many people came to the meeting?','?'],\n","              ['I’m from France','+'],\n","              ['I come from the UK','+'],\n","              ['My phone number is 61709832145','+'],\n","              ['I work as a tour guide for a local tour company','+'],\n","              ['I’m not dating anyone','-'],\n","              ['I live with my wife and children','+'],\n","              ['I often do morning exercises at 6am','+'],\n","              ['I run everyday','+'],\n","              ['She walks very slowly','+'],\n","              ['They eat a lot of meat daily','+'],\n","              ['We were in France that day', '+'],\n","              ['He speaks very fast', '+'],\n","              ['They told us they came back early', '+'],\n","              [\"I told her I'll be there\", '+']\n","]\n","train_set2 = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in train_set2]\n","X_train2 = pd.DataFrame(data=train_set2)\n","print(\"Documentos de entrenamiento:\")\n","print(X_train2)\n","\n","test_set2 = [\n","             ['Do you know who lives here?','?'],\n","             ['What time is it?','?'],\n","             ['Can you tell me where she comes from?','?'],\n","             ['How are you?','?'],\n","             ['I fill good today', '+'],\n","             ['There is a lot of history here','+'],\n","             ['I love programming','+'],\n","             ['He told us not to make so much noise','+'],  # interesing case\n","             ['We were asked not to park in front of the house','+'],  # interesing case\n","             [\"I don't have much time\",'-'],\n","             [\"She doesn't have any money\",'-'],\n","             [\"They didn't have any advice to offer\",'-'],\n","             ['I am not really sure','+']\n","]\n","test_set2 = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in test_set2]\n","X_test2 = pd.DataFrame(data=test_set2)\n","print(\"\\nDocumentos de prueba:\")\n","print(X_test2)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Documentos de entrenamiento:\n","                                                words class_\n","0                (Do, you, have, plenty, of, time, ?)      ?\n","1                 (Does, she, have, enough, money, ?)      ?\n","2           (Did, they, have, any, useful, advice, ?)      ?\n","3                           (What, day, is, today, ?)      ?\n","4                      (I, do, n't, have, much, time)      -\n","5                  (She, does, n't, have, any, money)      -\n","6      (They, did, n't, have, any, advice, to, offer)      -\n","7                    (Have, you, plenty, of, time, ?)      ?\n","8                        (Has, she, enough, money, ?)      ?\n","9                 (Had, they, any, useful, advice, ?)      ?\n","10                         (I, have, n't, much, time)      -\n","11                        (She, has, n't, any, money)      -\n","12             (He, had, n't, any, advice, to, offer)      -\n","13                                 (How, are, you, ?)      ?\n","14    (How, do, you, make, questions, in, English, ?)      ?\n","15             (How, long, have, you, lived, here, ?)      ?\n","16      (How, often, do, you, go, to, the, cinema, ?)      ?\n","17                    (How, much, is, this, dress, ?)      ?\n","18                            (How, old, are, you, ?)      ?\n","19     (How, many, people, came, to, the, meeting, ?)      ?\n","20                            (I, ’, m, from, France)      +\n","21                           (I, come, from, the, UK)      +\n","22               (My, phone, number, is, 61709832145)      +\n","23  (I, work, as, a, tour, guide, for, a, local, t...      +\n","24                     (I, ’, m, not, dating, anyone)      -\n","25           (I, live, with, my, wife, and, children)      +\n","26        (I, often, do, morning, exercises, at, 6am)      +\n","27                                 (I, run, everyday)      +\n","28                         (She, walks, very, slowly)      +\n","29               (They, eat, a, lot, of, meat, daily)      +\n","30                  (We, were, in, France, that, day)      +\n","31                           (He, speaks, very, fast)      +\n","32          (They, told, us, they, came, back, early)      +\n","33                  (I, told, her, I, 'll, be, there)      +\n","\n","Documentos de prueba:\n","                                                words class_\n","0                (Do, you, know, who, lives, here, ?)      ?\n","1                             (What, time, is, it, ?)      ?\n","2    (Can, you, tell, me, where, she, comes, from, ?)      ?\n","3                                  (How, are, you, ?)      ?\n","4                              (I, fill, good, today)      +\n","5              (There, is, a, lot, of, history, here)      +\n","6                              (I, love, programming)      +\n","7      (He, told, us, not, to, make, so, much, noise)      +\n","8   (We, were, asked, not, to, park, in, front, of...      +\n","9                      (I, do, n't, have, much, time)      -\n","10                 (She, does, n't, have, any, money)      -\n","11     (They, did, n't, have, any, advice, to, offer)      -\n","12                         (I, am, not, really, sure)      +\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Wdp22w2ArUl","executionInfo":{"status":"ok","timestamp":1621281481115,"user_tz":240,"elapsed":1506,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"4b265667-3738-4b8e-da63-c5e5003b79c4"},"source":["# Acá probarán su clasificador\n","\n","from sklearn.metrics import classification_report\n","\n","my_clf2 = MyMultinomialNB(alpha=1)\n","my_clf2.fit(X_train2)\n","print('vocab: ', len(my_clf2.vocab), my_clf2.vocab)\n","print('\\nTest probs:')\n","print('\\n'.join([str(l) for l in my_clf2.predict_proba(X_test2)]))\n","print('\\nTest predictions:')\n","my_y_preds = my_clf2.predict(X_test2)\n","print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_y_preds, X_test2['words'])]))\n","print(classification_report(y_true=X_test2['class_'], y_pred=my_y_preds, target_names=['?', '+', '-']))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["vocab:  109 ['fast', \"'ll\", 'told', 'had', 'my', 'back', 'not', '6am', 'Has', 'came', 'He', 'money', 'questions', 'for', 'children', 'Does', 'they', 'How', 'is', 'you', 'very', 'do', 'much', 'meeting', 'local', 'eat', 'live', 'day', 'make', 'her', 'English', 'the', 'company', 'slowly', 'exercises', 'morning', 'enough', 'dress', 'with', 'and', 'from', 'that', 'offer', \"n't\", 'tour', 'were', 'dating', 'does', '’', '?', 'us', 'useful', 'Have', '61709832145', 'm', 'come', 'France', 'Did', 'phone', 'walks', 'speaks', 'of', 'guide', 'run', 'plenty', 'We', 'She', 'My', 'have', 'here', 'number', 'wife', 'are', 'work', 'as', 'lot', 'What', 'Had', 'anyone', 'daily', 'go', 'everyday', 'be', 'They', 'at', 'long', 'she', 'early', 'UK', 'old', 'time', 'any', 'cinema', 'lived', 'today', 'meat', 'in', 'I', 'this', 'a', 'Do', 'has', 'often', 'many', 'did', 'advice', 'there', 'to', 'people']\n","\n","Test probs:\n","[0.00241833 0.00298307 0.9945986 ]\n","[0.00843731 0.01561141 0.97595128]\n","[0.00959419 0.00394488 0.98646093]\n","[4.04878068e-04 4.99425846e-04 9.99095696e-01]\n","[0.63464636 0.22987937 0.13547426]\n","[0.66550866 0.03156084 0.3029305 ]\n","[0.7105137  0.20919083 0.08029547]\n","[0.11611052 0.80008039 0.08380909]\n","[0.3775419  0.16259502 0.45986307]\n","[0.00375657 0.98188839 0.01435504]\n","[3.37985061e-04 9.93850047e-01 5.81196787e-03]\n","[5.60723220e-05 9.98223814e-01 1.72011336e-03]\n","[0.54610042 0.39561314 0.05828644]\n","\n","Test predictions:\n","? <- Do you know who lives here ?\n","? <- What time is it ?\n","? <- Can you tell me where she comes from ?\n","? <- How are you ?\n","+ <- I fill good today\n","+ <- There is a lot of history here\n","+ <- I love programming\n","- <- He told us not to make so much noise\n","? <- We were asked not to park in front of the house\n","- <- I do n't have much time\n","- <- She does n't have any money\n","- <- They did n't have any advice to offer\n","+ <- I am not really sure\n","              precision    recall  f1-score   support\n","\n","           ?       1.00      0.67      0.80         6\n","           +       0.75      1.00      0.86         3\n","           -       0.80      1.00      0.89         4\n","\n","    accuracy                           0.85        13\n","   macro avg       0.85      0.89      0.85        13\n","weighted avg       0.88      0.85      0.84        13\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JOm_ZS3dBRns"},"source":["##### Comparación con sklearn"]},{"cell_type":"code","metadata":{"id":"sqOvA94TJE8Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621281481115,"user_tz":240,"elapsed":1503,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"95bc7c6d-a2ce-46b7-e07e-2fced6e90d06"},"source":["from sklearn.metrics import classification_report\n","\n","X_train2 = pd.DataFrame(data=train_set2)\n","X_test2 = pd.DataFrame(data=test_set2)\n","my_clf2 = MyMultinomialNB(alpha=1)\n","my_clf2.fit(X_train2)\n","print('vocab: ', len(my_clf2.vocab), my_clf2.vocab)\n","print('\\nTest probs:')\n","print('\\n'.join([str(l) for l in my_clf2.predict_proba(X_test2)]))\n","print('\\nTest predictions:')\n","my_y_preds = my_clf2.predict(X_test2)\n","print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_y_preds, X_test2['words'])]))\n","print(classification_report(y_true=X_test2['class_'], y_pred=my_y_preds, target_names=['?', '+', '-']))\n","\n","print('\\nComparing with sklearn')\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline\n","sk_clf2 = Pipeline([('vect', CountVectorizer(lowercase=False, tokenizer=word_tokenize)),\n","                    ('clf', MultinomialNB()),\n","                   ])\n","sk_clf2.fit([' '.join(words) for words in X_train2['words']], X_train2['class_'])\n","print('vocab: ', len(sk_clf2[0].get_feature_names()), sk_clf2[0].get_feature_names())\n","print('same vocab' if sorted(my_clf2.vocab) == sorted(sk_clf2[0].get_feature_names()) else 'distinct vocabs')\n","print('\\nsklearn Test probs:')\n","X_test2_l = [' '.join(words) for words in X_test2['words']]\n","print('\\n'.join([str(l) for l in sk_clf2.predict_proba(X_test2_l)]))\n","print('\\nsklearn Test predictions:')\n","sk_y_preds = sk_clf2.predict(X_test2_l)\n","print('\\n'.join(['{} <- {}'.format(c, s) for c, s in zip(sk_y_preds, X_test2_l)]))\n","print(classification_report(y_true=X_test2['class_'], y_pred=sk_y_preds, target_names=['?', '+', '-']))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["vocab:  109 ['fast', \"'ll\", 'told', 'had', 'my', 'back', 'not', '6am', 'Has', 'came', 'He', 'money', 'questions', 'for', 'children', 'Does', 'they', 'How', 'is', 'you', 'very', 'do', 'much', 'meeting', 'local', 'eat', 'live', 'day', 'make', 'her', 'English', 'the', 'company', 'slowly', 'exercises', 'morning', 'enough', 'dress', 'with', 'and', 'from', 'that', 'offer', \"n't\", 'tour', 'were', 'dating', 'does', '’', '?', 'us', 'useful', 'Have', '61709832145', 'm', 'come', 'France', 'Did', 'phone', 'walks', 'speaks', 'of', 'guide', 'run', 'plenty', 'We', 'She', 'My', 'have', 'here', 'number', 'wife', 'are', 'work', 'as', 'lot', 'What', 'Had', 'anyone', 'daily', 'go', 'everyday', 'be', 'They', 'at', 'long', 'she', 'early', 'UK', 'old', 'time', 'any', 'cinema', 'lived', 'today', 'meat', 'in', 'I', 'this', 'a', 'Do', 'has', 'often', 'many', 'did', 'advice', 'there', 'to', 'people']\n","\n","Test probs:\n","[0.00241833 0.00298307 0.9945986 ]\n","[0.00843731 0.01561141 0.97595128]\n","[0.00959419 0.00394488 0.98646093]\n","[4.04878068e-04 4.99425846e-04 9.99095696e-01]\n","[0.63464636 0.22987937 0.13547426]\n","[0.66550866 0.03156084 0.3029305 ]\n","[0.7105137  0.20919083 0.08029547]\n","[0.11611052 0.80008039 0.08380909]\n","[0.3775419  0.16259502 0.45986307]\n","[0.00375657 0.98188839 0.01435504]\n","[3.37985061e-04 9.93850047e-01 5.81196787e-03]\n","[5.60723220e-05 9.98223814e-01 1.72011336e-03]\n","[0.54610042 0.39561314 0.05828644]\n","\n","Test predictions:\n","? <- Do you know who lives here ?\n","? <- What time is it ?\n","? <- Can you tell me where she comes from ?\n","? <- How are you ?\n","+ <- I fill good today\n","+ <- There is a lot of history here\n","+ <- I love programming\n","- <- He told us not to make so much noise\n","? <- We were asked not to park in front of the house\n","- <- I do n't have much time\n","- <- She does n't have any money\n","- <- They did n't have any advice to offer\n","+ <- I am not really sure\n","              precision    recall  f1-score   support\n","\n","           ?       1.00      0.67      0.80         6\n","           +       0.75      1.00      0.86         3\n","           -       0.80      1.00      0.89         4\n","\n","    accuracy                           0.85        13\n","   macro avg       0.85      0.89      0.85        13\n","weighted avg       0.88      0.85      0.84        13\n","\n","\n","Comparing with sklearn\n","vocab:  109 [\"'ll\", '61709832145', '6am', '?', 'Did', 'Do', 'Does', 'English', 'France', 'Had', 'Has', 'Have', 'He', 'How', 'I', 'My', 'She', 'They', 'UK', 'We', 'What', 'a', 'advice', 'and', 'any', 'anyone', 'are', 'as', 'at', 'back', 'be', 'came', 'children', 'cinema', 'come', 'company', 'daily', 'dating', 'day', 'did', 'do', 'does', 'dress', 'early', 'eat', 'enough', 'everyday', 'exercises', 'fast', 'for', 'from', 'go', 'guide', 'had', 'has', 'have', 'her', 'here', 'in', 'is', 'live', 'lived', 'local', 'long', 'lot', 'm', 'make', 'many', 'meat', 'meeting', 'money', 'morning', 'much', 'my', \"n't\", 'not', 'number', 'of', 'offer', 'often', 'old', 'people', 'phone', 'plenty', 'questions', 'run', 'she', 'slowly', 'speaks', 'that', 'the', 'there', 'they', 'this', 'time', 'to', 'today', 'told', 'tour', 'us', 'useful', 'very', 'walks', 'were', 'wife', 'with', 'work', 'you', '’']\n","same vocab\n","\n","sklearn Test probs:\n","[0.00241833 0.00298307 0.9945986 ]\n","[0.00843731 0.01561141 0.97595128]\n","[0.00959419 0.00394488 0.98646093]\n","[4.04878068e-04 4.99425846e-04 9.99095696e-01]\n","[0.63464636 0.22987937 0.13547426]\n","[0.66550866 0.03156084 0.3029305 ]\n","[0.7105137  0.20919083 0.08029547]\n","[0.11611052 0.80008039 0.08380909]\n","[0.3775419  0.16259502 0.45986307]\n","[0.00375657 0.98188839 0.01435504]\n","[3.37985061e-04 9.93850047e-01 5.81196787e-03]\n","[5.60723220e-05 9.98223814e-01 1.72011336e-03]\n","[0.54610042 0.39561314 0.05828644]\n","\n","sklearn Test predictions:\n","? <- Do you know who lives here ?\n","? <- What time is it ?\n","? <- Can you tell me where she comes from ?\n","? <- How are you ?\n","+ <- I fill good today\n","+ <- There is a lot of history here\n","+ <- I love programming\n","- <- He told us not to make so much noise\n","? <- We were asked not to park in front of the house\n","- <- I do n't have much time\n","- <- She does n't have any money\n","- <- They did n't have any advice to offer\n","+ <- I am not really sure\n","              precision    recall  f1-score   support\n","\n","           ?       1.00      0.67      0.80         6\n","           +       0.75      1.00      0.86         3\n","           -       0.80      1.00      0.89         4\n","\n","    accuracy                           0.85        13\n","   macro avg       0.85      0.89      0.85        13\n","weighted avg       0.88      0.85      0.84        13\n","\n"],"name":"stdout"}]}]}