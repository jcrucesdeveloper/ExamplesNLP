{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tarea 2 - pauta.ipynb","provenance":[{"file_id":"1jZG4e_poENFYDeljHjx9goskyQ3Jy3GU","timestamp":1618284831252}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2obO44rRIDIm"},"source":["# **Tarea 2 - CC6205 Natural Language Processing üìö**\n","\n","**Integrantes:**\n","\n","**Fecha l√≠mite de entrega üìÜ:** Lunes 03 de mayo.\n","\n","**Tiempo estimado de dedicaci√≥n:**"]},{"cell_type":"markdown","metadata":{"id":"Zpupcv6QW2fd"},"source":["Bienvenid@s a la segunda tarea del curso de Natural Language Processing (NLP). En esta tarea estaremos modelando probabil√≠sticamente el lenguaje mediante **Languaje Modeling** y clasificando textos mediante el m√©todo **N√§ive Bayes**. Espec√≠ficamente, la tarea consta de una parte te√≥rica que busca evaluar conceptos vistos en clases sobre los **Language Models** y una parte pr√°ctica donde **programeran a mano** el m√©todo **N√§ive Bayes**. \n","\n","**Instrucciones:**\n","- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n","- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n","- El formato de entrega es este mismo Jupyter Notebook.\n","- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n. \n","- Est√° **PROHIBIDO** usar cualquier librer√≠a que implemente los algoritmos pedidos (Spacy, scikit, etc). S√≥lo se podr√°n utilizar las librer√≠as importadas al inicio de la secci√≥n de pr√°ctica.\n","- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso.\n","\n","Si a√∫n no han visto las clases, se recomienda visitar los links de las referencias.\n","\n","**Referencias:**\n","\n","Slides:\n","    \n","- [Language Models](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/lmslides.pdf) (slides by Michael Collins)\n","- [Text Classification and Naive Bayes](https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf) (slides by Dan Jurafsky)\n","\n","Videos: \n","\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 1](https://www.youtube.com/watch?v=9E2jJ6kcb4Y&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=4)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 2](https://www.youtube.com/watch?v=ZWqbEQXLra0&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=5)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 3](https://www.youtube.com/watch?v=tsumFqwFlaA&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=6)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 4](https://www.youtube.com/watch?v=s3TWdv4sqkg&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=7)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 1](https://www.youtube.com/watch?v=kG9BK9Oy1hU)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 2](https://www.youtube.com/watch?v=Iqte5kKHvzE)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 3](https://www.youtube.com/watch?v=TSJg0_X3Abk)\n","\n","` ` "]},{"cell_type":"markdown","metadata":{"id":"JstKx3TiKcIp"},"source":["---------------------------\n","## Parte 1. Language Modeling (3 pts)\n","\n","En esta parte responder√°n preguntas **te√≥ricas** sobre los Lenguage Models. Para comprender como funcionan muchas de las t√©cnicas que veremos posteriormente durante el curso ser√° muy √∫til que dominen estos modelos y sus fundamentos.\n","\n","Recuerden que los **Lenguage Models** b√°sicamente nos permiten, dado un corpus, estimar un modelo probabilista al que le podemos pasar una oraci√≥n y determinar que tan probable es que esa oraci√≥n haya sido generada. Para esto, tenemos que un modelo de $n$-gramas puede ser definido por una *cadena de M√°rkov* de orden $n-1$.\n","\n","En clases vimos los modelos basados en unigramas, bigramas y trigramas. En esta pregunta trabajaremos con modelos de lenguaje basados en 4-gramas (*cadena de M√°rkov* de tercer orden).\n","\n","**Nota:** Las preguntas deben ser resueltas con desarrollo, pero sin c√≥digo. Por favor usen $\\LaTeX$ para las f√≥rmulas."]},{"cell_type":"markdown","metadata":{"id":"2hwW-07MrRpt"},"source":["\n","### 1.1 (1 pt)\n","\n","Asuma que tenemos calculados los par√°metros $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$ para todas las posibles secuencias de tama√±o 4 que aparecen en un corpus $\\mathcal{C}$. Dado esto, muestre c√≥mo un modelo le asignar√≠a una probabilidad a la frase `una persona corriendo r√°pido`"]},{"cell_type":"markdown","metadata":{"id":"YzlQDAVqWNdX"},"source":["` ` \n","\n","**Respuesta:** \n","\n","\\begin{equation}\n","escriba\\ su\\ respuesta\\ aqu√≠\n","\\end{equation}\n","\n","` `"]},{"cell_type":"markdown","metadata":{"id":"5VUjDx0NrYbg"},"source":["**Respuesta:**\n","\n","\\begin{align}\n","q(\\text{una persona corriendo r√°pido}) = &q(\\text{una}|*,*,*) \\times q(\\text{persona}|*,*,\\text{una}) \\\\\n","&\\times q(\\text{corriendo}|*,\\text{una},\\text{persona}) \\\\\n","&\\times q(\\text{r√°pido}|\\text{una},\\text{persona},\\text{corriendo}) \\\\\n","&\\times q(\\text{STOP}|\\text{persona},\\text{corriendo},\\text{r√°pido})\n","\\end{align}\n"]},{"cell_type":"markdown","metadata":{"id":"lAej_jqtVwm1"},"source":["### 1.2 Estimando las probabilidades (1 pt)"]},{"cell_type":"markdown","metadata":{"id":"gXSFlCIex8kq"},"source":["#### 1.2.1 Modelo simple (0.5 pt)\n","\n","Si hubieses tenido que estimar las probabilidades condicionales (par√°metros del modelo) $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$ a partir de $\\mathcal{C}$, ¬øc√≥mo la definir√≠as siguiendo el principio de m√°xima verosimilitud?"]},{"cell_type":"markdown","metadata":{"id":"WYLKSFjJXDvn"},"source":["` ` \n","\n","**Respuesta:** \n","\n","\\begin{equation}\n","escriba\\ su\\ respuesta\\ aqu√≠\n","\\end{equation}\n","\n","donde ....\n","\n","` `"]},{"cell_type":"markdown","metadata":{"id":"Y8cT1bkOXF4u"},"source":["` `\n","\n","**Respuesta:** \n","\n","\\begin{equation}\n","q(w_i | w_{i-3}, w_{i-2}, w_{i-1}) = \\frac{Count(w_{i-3}, w_{i-2}, w_{i-1}, w_i)}{Count(w_{i-3}, w_{i-2},w_{i-1})},\n","\\end{equation}\n","\n","donde $Count(w_{i-3}, w_{i-2}, w_{i-1}, w_i)$ representa la frecuencia de apariciones de la secuencia de tama√±o 4 $(w_{i-3}, w_{i-2}, w_{i-1}, w_i)$ y $Count(w_{i-3}, w_{i-2}, w_{i-1})$ representa la frecuancia de apariciones de la secuencia de tama√±o 3 $(w_{i-3}, w_{i-2}, w_{i-1})$ dentro del corpus $\\mathcal{C}$.\n","\n","` `"]},{"cell_type":"markdown","metadata":{"id":"bwNkPIXure0C"},"source":["#### 1.2.2 Modelo interpolado (0.5 pt)\n","Muestre c√≥mo se calcular√≠a $q(w_{i} | w_{i-3}, w_{i-2}, w_{i-1})$ usando un modelo que interpola modelos de lenguajes basados en 4-grams, trigramas, bigramas y unigramas. Te fue necesario definir nuevos par√°metros? En caso afirmativo, qu√© los diferencia de los par√°metros del modelo simple y que propiedades deben cumplir?"]},{"cell_type":"markdown","metadata":{"id":"zeLZAd0Tr9ne"},"source":["` ` \n","\n","**Respuesta:** \n","\n","$escriba\\ su\\ respuesta\\ aqu√≠$\n","\n","donde ...\n","\n","` ` \n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZeJI3-C2XAa1"},"source":["` ` \n","\n","**Respuesta:** \n","\n","\\begin{align}\n","q(w_{i} | w_{i-3}, w_{i-2}, w_{i-1}) &= \\lambda_1 \\times q_{ML}(w_{i} | w_{i-3}, w_{i-2}, w_{i-1}) \\times \\lambda_2 \\times q_{ML}(w_{i} | w_{i-2}, w_{i-1})+ \\lambda_3 \\times q_{ML}(w_{i} | w_{i-1}) + \\lambda_4 \\times q_{ML}(w_{i})\\\\\n","&=\\lambda_1 \\times \\frac{Count(w_{i-3}, w_{i-2}, w_{i-1}, w_i)}{Count(w_{i-3}, w_{i-2}, w_{i-1})} \\\\\n","&\\ \\ + \\lambda_2 \\times \\frac{Count(w_{i-2}, w_{i-1}, w_i)}{Count(w_{i-2}, w_{i-1})} \\\\\n","&\\ \\ + \\lambda_3 \\times \\frac{Count(w_{i-1}, w_i)}{Count(w_{i-1})} \\\\\n","&\\ \\ + \\lambda_4~\\times~\\frac{Count(w_i)}{Count()},\n","\\end{align}\n","\n","donde $\\lambda_1 + \\lambda_2 +\\lambda_3 +\\lambda_4 = 1$ con $\\lambda_1, \\lambda_2, \\lambda_3,\\lambda_4 \\geq 0$  y se diferencian de los otros parametros del modelo en que no necesitan estimarse en el entrenamiento del modelo, se pueden tratar como hiper-par√°metros del modelo que debemos definir\n","\n","` ` "]},{"cell_type":"markdown","metadata":{"id":"sHqcRJ7Vr_8x"},"source":["### 1.3 (1 pt)\n","¬øQu√© ventajas tiene el modelo interpolado sobre el modelo de 4-gramas simple?"]},{"cell_type":"markdown","metadata":{"id":"6F5R3Ji7sHjt"},"source":["` ` \n","\n","**Respuesta:** \n","\n","escriba su respuesta aqu√≠\n","\n","` ` "]},{"cell_type":"markdown","metadata":{"id":"wLedQWNJXLZF"},"source":["` ` \n","\n","**Respuesta:**\n","\n","En el modelo de 4-gramas uno debe extraer todos los 4-gramas y trigramas del corpus y contar cuantas veces aparecen. Esto se hace para determinar los par√°metros $q$. Pero ocurre que muchos trigramas y bigramas no aparecen en el corpus, lo que hace que muchos par√°metros $q$ sean cero. Por tanto, la productoria se nos va a cero tambi√©n.\n","\n","Atendiendo a esto, tenemos que los modelos de trigramas se indefinen menos veces que los de 4-gramas, los de bigramas se indefinen menos veces que los de trigramas y los de unigramas se indefinen menos veces que los de bigramas. La idea de un modelo de interpolaci√≥n es combinar los tres modelos en uno solo (mediante una ponderaci√≥n lineal) para suavizar (disminuir) las inefiniciones. Con esto se aumenta la capacidad de generalizaci√≥n del modelo ya que puede entregarnos una probabilidad para un documento nuevo que puede contener 4-gramas nunca vistos.\n","\n","` ` "]},{"cell_type":"markdown","metadata":{"id":"mLPAel_RjFAO"},"source":["### 1.4 (otras posibles preguntas) (1 pt)\n","\n","¬øC√≥mo podemos evaluar los Language Models?\n","\n","¬øCu√°ndo una oraci√≥n es plausible?\n","\n","¬øPor qu√© no modelamos solo la probabilidad de las oraciones que existen en el curpus?\n","\n","¬øQu√© permiten los modelos de Markov de segundo orden?\n","\n","¬øEn qu√© consiste el proceso de padding y para que sirve?\n","\n","¬øQu√© significa $q(\\text{STOP}|\\text{el},\\text{perro})$?"]},{"cell_type":"markdown","metadata":{"id":"rdmB-07ZKfaa"},"source":["-----------------------\n","## Parte 2. Naive Bayes (3 pts)\n","En esta parte programaremos nuestro primer clasificador de documentos. Para esto implementaremos el m√©todo **Naive Bayes Multinomial** usando **Laplace Smothing**.\n","\n","Por favor, documenten su c√≥digo. Escriban que hacen las funciones, que reciben, que entregan, etc. Si en alg√∫n lugar de su c√≥digo hacen algo que creen que debe ser explicado, los comentarios son bienvenidos.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PpjjKnJUvRiA"},"source":["### 2.1 Clase para clasificador (0.5 pt)\n","\n","Programa una clase `MyMultinomialNB` que en su inicializador reciba el parametro `alpha` que representa ...\n","\n","```python\n","class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n","  def __init__(self, ...):\n","    ...\n","```\n","\n","Una llamada de ejemplo para crear un objeto de tu clase es:\n","```python\n","my_clf = MyMultinomialNB(alpha=1)\n","``` \n","lo que debiera crear un clasificador con parametro `alpha` igual a 1."]},{"cell_type":"markdown","metadata":{"id":"ROG50eH0xfxO"},"source":["### 2.2 Entrenamiento del clasificador (1 pt)\n","\n","Programa el entrenamiento de tu clasificador en el m√©todo `fit` de la clase `MyMultinomialNB`. La funci√≥n debiera recibir el par√°metro X que es un `DataFrame` de `pandas` con las columnas `words` y `class_`, donde `words` es una tupla con las palabras asociadas al cada documento y `class_` es el identificador de la clase a la que pertenece cada documento.\n","\n","Para computar el entrenamiento de nuestro clasificador debemos: \n","- extraer el vocabulario,\n","- determinar las probabilidades $p(c_j)$ para cada una de las clases posibles, \n","- determinar las probabilidades $p(w_i|c_j)$ para cada una de las palabras y cada una de las clases usando **Laplace Smothing**.\n","\n","El resultado del metodo `fit` ser√° la misma instancia de nuestro clasificador `self`.\n","\n","```python\n","class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n","  def __init__(self, ...):\n","    ...\n","\n","  def fit(self, X):\n","    ...\n","    return self\n","```\n","\n","**Underflow prevention:** En vez de hacer muchas multiplicacions de `float`s, reempl√°cenlas por sumas de logaritmos para prevenir errores de precisi√≥n. Revisen la diapo 69 de las slides. "]},{"cell_type":"markdown","metadata":{"id":"FNouTCmR2FgY"},"source":["### 2.3 Predicci√≥n (1 pt)\n","\n","Programa la predicci√≥n de tu clasificador en el m√©todo `predict` de la clase `MyMultinomialNB`. Al igual que la funci√≥n `fit`, `predict` debe recibir un `DataFrame` X con valores `None` en la columna `class_` y devolver una lista con las clases que maximizan la probabilidad de Bayes para cada uno de los elmentos de X (filas)."]},{"cell_type":"code","metadata":{"id":"DYFEgTyw2ELL","executionInfo":{"status":"ok","timestamp":1621281480544,"user_tz":240,"elapsed":958,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}}},"source":["# Ac√° implementar√°n las preguntas 2.1, 2.2 y 2.3,\n","# tu c√≥digo debiera comenzar as√≠\n","\n","# importamos algunos paquetes necesarios, puede que necesites otros\n","import numpy as np\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.utils.validation import check_is_fitted\n","\n","class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n","  def __init__(self, alpha=1.0):\n","    # ac√° tu c√≥digo para inicializar el clasificador\n","    ...\n","\n","  def fit(self, X):\n","    # ac√° tu c√≥digo para el entrenamiento del modelo\n","    ...\n","    \n","    return self\n","\n","  def predict(self, X):\n","    # Checkea que fit ha sido ejecutado anteriormente\n","    check_is_fitted(self)\n","\n","    # ac√° tu c√≥digo para computar la predicci√≥n\n","    ...\n","    \n","    return prediction"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"0edu0E7LA3U9","executionInfo":{"status":"ok","timestamp":1621281481113,"user_tz":240,"elapsed":1524,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}}},"source":["import numpy as np\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.utils.validation import check_is_fitted\n","\n","# from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n","# from sklearn.utils.multiclass import unique_labels\n","\n","class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n","  def __init__(self, alpha=1.0):\n","    self.alpha_ = alpha\n","\n","  def fit(self, X):\n","    # Store the classes seen during fit\n","    self.groups = X.groupby('class_')\n","    self.classes_ = list(self.groups['class_'])\n","    self.n_classes = len(self.classes_)\n","\n","    # extract vocabulary\n","    self.vocab = list(set(X['words'].sum()))\n","    self.alpha_V = self.alpha_ * len(self.vocab)\n","    \n","    # compute the count of documents of class c_j \n","    docs_count = self.groups.count()\n","\n","    # the total count of documents in train set\n","    N_docs = X.shape[0]\n","\n","    # for each class (topic) concat all documents in a big doc c_j\n","    big_docs = self.groups.sum()\n","\n","    self.log_p_c, self.count_c_w, self.log_l_c_ls, self.log_p_c_w_ls = [], [], [], []\n","    for c in range(self.n_classes):\n","      # compute P(c_j) = docs_count(C==c_j) / N_docs\n","      self.log_p_c.append(np.log(docs_count['words'][c] / N_docs))\n","\n","      # compute the frecuency of word w_i in the big doc c_j\n","      count_w_ls = {w: self.alpha_ for w in self.vocab}\n","      for t in big_docs['words'][c]:\n","        count_w_ls[t] += 1\n","\n","      # compute the length of the big doc c_j (the sum of frecuency of all words)\n","      log_l_ls = np.log(len(big_docs['words'][c]) + self.alpha_V)\n","      self.log_l_c_ls.append(log_l_ls)\n","\n","      # compute P(w_i|c_j) = count(w_i,c_j) / sum_w(count(w,C_j))\n","      # considering Laplace (add alpha) smoothing (counters were initialized to alpha)\n","      # => P(w_i|c_j) = (count(w_i,c_j) + alpha) / (sum_w(count(w,C_j)) + alpha*|V|)\n","      log_p_w_ls = {w: np.log(count_w_ls[w]) - log_l_ls for w in self.vocab}\n","      self.log_p_c_w_ls.append(log_p_w_ls)\n","\n","    # Return the classifier\n","    return self\n","\n","  def __log_pi(self, c, d):\n","    # implementation of equation\n","    # log(pi_i(P(x_i|c_j) * P(c_j)) = log(pi_i(P(x_i|c_j)) + log(P(c_j)) = sum(log(P(x_i|c_j))) + log(P(c_j))\n","\n","    # considering the words that don't appear in the vocabulary\n","    # return np.sum([self.log_p_c_w_ls[c][w] if w in self.vocab else (np.log(self.alpha_) - self.log_l_c_ls[c]) for w in d]) + self.log_p_c[c]\n","\n","    # without considering the words that don't appear in the vocabulary\n","    return np.sum([self.log_p_c_w_ls[c][w] for w in d if w in self.vocab]) + self.log_p_c[c]\n","\n","  def predict_log_proba(self, X):\n","    # Check is fit had been called\n","    check_is_fitted(self)\n","\n","    log_probs = []\n","    for doc in X['words']:\n","      # compute probability of each doc by using mariginalization\n","      # P(d) = sum_j(P(d,c_j)) = sum_j(P(d|c_j) * P(c_j)) = sum_j(pi_i(P(x_i|c_j) * P(c_j))\n","      log_pi_c = [self.__log_pi(c, doc) for c in range(self.n_classes)]  # log(pi_i(P(x_i|c_j) * P(c_j))\n","      log_p_d = np.log(np.sum(np.exp(log_pi_c)))  # log(sum(exp(log(pi_i(P(x_i|c_j) * P(c_j)))))\n","\n","      # compute Bayes probabilites for each class c_j\n","      # P(d|c_j) * P(c_j) / P(d) = pi_i(P(x_i|c_j)) * P(c_j) / P(d)\n","      log_p_d_c = [log_pi_c[c] - log_p_d for c in range(self.n_classes)]\n","      log_probs.append(log_p_d_c)\n","\n","    return log_probs\n","\n","  def predict_proba(self, X):\n","    return np.exp(self.predict_log_proba(X))\n","\n","  def predict(self, X):\n","    # Check is fit had been called\n","    check_is_fitted(self)\n","\n","    # compute prediction, find the class c that maximize Bayes prob\n","    # it is not nedded to compute the exact probabilities\n","    # => argmax_c P(c) * pi_i(P(x_i|c))\n","    pred = []\n","    for doc in X['words']:\n","      max_log_p, max_c = float('-inf'), -1\n","      for c in range(self.n_classes):\n","        log_pi = self.__log_pi(c, doc)\n","        if log_pi > max_log_p:\n","          max_log_p, max_c = log_pi, c\n","      pred.append(self.classes_[max_c][0])\n","\n","    return pred"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_KOMJ-CS8PRP"},"source":["### 2.4 Probando el clasificador (0.5 pt)"]},{"cell_type":"markdown","metadata":{"id":"hucdz-R7xerG"},"source":["A continuaci√≥n probar√°n el funcionamiento de su clasificador. Para esto, les presentamos un conjunto de documentos de entrenamiento `train_set` divididos en 2 categorias distintas. Ustedes deber√°n primero entrenar su clasificador usando el m√©todo `fit` de su clase y luego, clasificar los documentos del conjunto de prueva `test_set` usando el m√©todo `predict`.\n","\n","**NOTA:** Como pueden ver, los objetos `namedtuple`s tienen dos atributos: `words` donde est√°n las palabras del documento y `class_` donde se guarda la clase de ese documento. Estos objetos son inmutables, lo que quiere decir que si quieren modificar un documento y cambiarle la clase, tienen que crear otro documento. Otra cosa es que son tuplas como cualquier otra, es decir se pueden acceder usando indices como `doc[0]` o `doc[1]`."]},{"cell_type":"code","metadata":{"id":"HLi8PxdV2VQX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621281481114,"user_tz":240,"elapsed":1520,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"14b6417e-ac6c-4cc6-d88d-e6b71c2441b8"},"source":["import pandas as pd\n","\n","from collections import namedtuple\n","document = namedtuple(\n","    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",")\n","\n","train_set = (\n","    document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n","    document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n","    document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n","    document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n","    document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n","    document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=1),\n","    document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n","    document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1)\n",")\n","X_train = pd.DataFrame(data=train_set)\n","print(\"Documentos de entrenamiento\")\n","print(X_train)\n","\n","test_set = (document(words=('w02', 'w09', 'w05', 'w01', 'w06', 'w05', 'w04', 'w00'), class_=None),\n","            document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),\n","            document(words=('w03', 'w03', 'w04', 'w05', 'w01', 'w06', 'w09', 'w02'), class_=None))\n","X_test = pd.DataFrame(data=test_set)\n","print(\"\\nDocumentos de prueba:\")\n","print(X_test)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Documentos de entrenamiento\n","                                           words  class_\n","0            (w03, w01, w02, w06, w02, w08, w07)       0\n","1  (w05, w04, w00, w06, w09, w07, w06, w09, w05)       1\n","2  (w07, w06, w00, w08, w01, w08, w08, w09, w02)       0\n","3            (w08, w09, w02, w06, w05, w08, w07)       1\n","4            (w09, w08, w05, w08, w05, w00, w08)       1\n","5            (w05, w05, w06, w01, w06, w08, w02)       1\n","6            (w04, w03, w07, w05, w04, w00, w02)       0\n","7       (w01, w00, w01, w04, w09, w02, w04, w07)       1\n","\n","Documentos de prueba:\n","                                      words class_\n","0  (w02, w09, w05, w01, w06, w05, w04, w00)   None\n","1  (w02, w09, w06, w01, w05, w04, w03, w03)   None\n","2  (w03, w03, w04, w05, w01, w06, w09, w02)   None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OXHwmOWB-4Aa","executionInfo":{"status":"ok","timestamp":1621281481114,"user_tz":240,"elapsed":1516,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"9b743664-4b11-4e66-a675-0e3d286b0b3c"},"source":["# Ac√° probar√°n su clasificador\n","\n","my_clf = MyMultinomialNB(alpha=1)\n","my_clf.fit(X_train)\n","\n","print('vocab: ', my_clf.vocab)\n","print('\\nTest probs:')\n","print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test)]))\n","print('\\nTest predictions:')\n","print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_clf.predict(X_test), X_test['words'])]))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["vocab:  ['w04', 'w03', 'w06', 'w07', 'w05', 'w00', 'w02', 'w08', 'w09', 'w01']\n","\n","Test probs:\n","[0.06194691 0.93805309]\n","[0.76018101 0.23981899]\n","[0.76018101 0.23981899]\n","\n","Test predictions:\n","1 <- w02 w09 w05 w01 w06 w05 w04 w00\n","0 <- w02 w09 w06 w01 w05 w04 w03 w03\n","0 <- w03 w03 w04 w05 w01 w06 w09 w02\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Js0W9LlXBgjk"},"source":["##### Comparaci√≥n con sklearn"]},{"cell_type":"code","metadata":{"id":"u8Jmgl0KIYaV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621281481114,"user_tz":240,"elapsed":1512,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"cde8bfef-9906-4b60-c1c3-0c56342bfb56"},"source":["X_train = pd.DataFrame(data=train_set)\n","X_test = pd.DataFrame(data=test_set)\n","my_clf = MyMultinomialNB(alpha=1)\n","my_clf.fit(X_train)\n","print('vocab: ', my_clf.vocab)\n","print('\\nTest probs:')\n","print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test)]))\n","print('\\nTest predictions:')\n","print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_clf.predict(X_test), X_test['words'])]))\n","\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline\n","sk_clf = Pipeline([('vect', CountVectorizer()),\n","                    #  ('tfidf', TfidfTransformer()),\n","                     ('clf', MultinomialNB()),\n","                    ])\n","sk_clf.fit([' '.join(words) for words in X_train['words']], X_train['class_'])\n","print('\\nsklearn Test probs:')\n","X_test = [' '.join(words) for words in X_test['words']]\n","print('\\n'.join([str(l) for l in sk_clf.predict_proba(X_test)]))\n","print('\\nsklearn Test predictions:')\n","print('\\n'.join(['{} <- {}'.format(c, s) for c, s in zip(sk_clf.predict(X_test), X_test)]))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["vocab:  ['w04', 'w03', 'w06', 'w07', 'w05', 'w00', 'w02', 'w08', 'w09', 'w01']\n","\n","Test probs:\n","[0.06194691 0.93805309]\n","[0.76018101 0.23981899]\n","[0.76018101 0.23981899]\n","\n","Test predictions:\n","1 <- w02 w09 w05 w01 w06 w05 w04 w00\n","0 <- w02 w09 w06 w01 w05 w04 w03 w03\n","0 <- w03 w03 w04 w05 w01 w06 w09 w02\n","\n","sklearn Test probs:\n","[0.06194691 0.93805309]\n","[0.76018101 0.23981899]\n","[0.76018101 0.23981899]\n","\n","sklearn Test predictions:\n","1 <- w02 w09 w05 w01 w06 w05 w04 w00\n","0 <- w02 w09 w06 w01 w05 w04 w03 w03\n","0 <- w03 w03 w04 w05 w01 w06 w09 w02\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5tDZnmns_1dW"},"source":["#### (opcional) Oraciones reales\n","\n","Aqu√≠ intentaremos entrenar un clasificador para determinar cuando una oracion en ingl√©s es interrogativa, afirmativa o negativa."]},{"cell_type":"code","metadata":{"id":"YCWi3oytd2nf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621281481115,"user_tz":240,"elapsed":1510,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"ff1dcf9d-df14-4b79-f48a-6ff00a5db03e"},"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","document = namedtuple(\n","    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",")\n","\n","train_set2 = [\n","              ['Do you have plenty of time?', '?'],\n","              ['Does she have enough money?','?'],\n","              ['Did they have any useful advice?','?'],\n","              ['What day is today?','?'],\n","              [\"I don't have much time\",'-'],\n","              [\"She doesn't have any money\",'-'],\n","              [\"They didn't have any advice to offer\",'-'],\n","              ['Have you plenty of time?','?'],\n","              ['Has she enough money?','?'],\n","              ['Had they any useful advice?','?'],\n","              [\"I haven't much time\",'-'],\n","              [\"She hasn't any money\",'-'],\n","              [\"He hadn't any advice to offer\",'-'],\n","              ['How are you?','?'],\n","              ['How do you make questions in English?','?'],\n","              ['How long have you lived here?','?'],\n","              ['How often do you go to the cinema?','?'],\n","              ['How much is this dress?','?'],\n","              ['How old are you?','?'],\n","              ['How many people came to the meeting?','?'],\n","              ['I‚Äôm from France','+'],\n","              ['I come from the UK','+'],\n","              ['My phone number is 61709832145','+'],\n","              ['I work as a tour guide for a local tour company','+'],\n","              ['I‚Äôm not dating anyone','-'],\n","              ['I live with my wife and children','+'],\n","              ['I often do morning exercises at 6am','+'],\n","              ['I run everyday','+'],\n","              ['She walks very slowly','+'],\n","              ['They eat a lot of meat daily','+'],\n","              ['We were in France that day', '+'],\n","              ['He speaks very fast', '+'],\n","              ['They told us they came back early', '+'],\n","              [\"I told her I'll be there\", '+']\n","]\n","train_set2 = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in train_set2]\n","X_train2 = pd.DataFrame(data=train_set2)\n","print(\"Documentos de entrenamiento:\")\n","print(X_train2)\n","\n","test_set2 = [\n","             ['Do you know who lives here?','?'],\n","             ['What time is it?','?'],\n","             ['Can you tell me where she comes from?','?'],\n","             ['How are you?','?'],\n","             ['I fill good today', '+'],\n","             ['There is a lot of history here','+'],\n","             ['I love programming','+'],\n","             ['He told us not to make so much noise','+'],  # interesing case\n","             ['We were asked not to park in front of the house','+'],  # interesing case\n","             [\"I don't have much time\",'-'],\n","             [\"She doesn't have any money\",'-'],\n","             [\"They didn't have any advice to offer\",'-'],\n","             ['I am not really sure','+']\n","]\n","test_set2 = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in test_set2]\n","X_test2 = pd.DataFrame(data=test_set2)\n","print(\"\\nDocumentos de prueba:\")\n","print(X_test2)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Documentos de entrenamiento:\n","                                                words class_\n","0                (Do, you, have, plenty, of, time, ?)      ?\n","1                 (Does, she, have, enough, money, ?)      ?\n","2           (Did, they, have, any, useful, advice, ?)      ?\n","3                           (What, day, is, today, ?)      ?\n","4                      (I, do, n't, have, much, time)      -\n","5                  (She, does, n't, have, any, money)      -\n","6      (They, did, n't, have, any, advice, to, offer)      -\n","7                    (Have, you, plenty, of, time, ?)      ?\n","8                        (Has, she, enough, money, ?)      ?\n","9                 (Had, they, any, useful, advice, ?)      ?\n","10                         (I, have, n't, much, time)      -\n","11                        (She, has, n't, any, money)      -\n","12             (He, had, n't, any, advice, to, offer)      -\n","13                                 (How, are, you, ?)      ?\n","14    (How, do, you, make, questions, in, English, ?)      ?\n","15             (How, long, have, you, lived, here, ?)      ?\n","16      (How, often, do, you, go, to, the, cinema, ?)      ?\n","17                    (How, much, is, this, dress, ?)      ?\n","18                            (How, old, are, you, ?)      ?\n","19     (How, many, people, came, to, the, meeting, ?)      ?\n","20                            (I, ‚Äô, m, from, France)      +\n","21                           (I, come, from, the, UK)      +\n","22               (My, phone, number, is, 61709832145)      +\n","23  (I, work, as, a, tour, guide, for, a, local, t...      +\n","24                     (I, ‚Äô, m, not, dating, anyone)      -\n","25           (I, live, with, my, wife, and, children)      +\n","26        (I, often, do, morning, exercises, at, 6am)      +\n","27                                 (I, run, everyday)      +\n","28                         (She, walks, very, slowly)      +\n","29               (They, eat, a, lot, of, meat, daily)      +\n","30                  (We, were, in, France, that, day)      +\n","31                           (He, speaks, very, fast)      +\n","32          (They, told, us, they, came, back, early)      +\n","33                  (I, told, her, I, 'll, be, there)      +\n","\n","Documentos de prueba:\n","                                                words class_\n","0                (Do, you, know, who, lives, here, ?)      ?\n","1                             (What, time, is, it, ?)      ?\n","2    (Can, you, tell, me, where, she, comes, from, ?)      ?\n","3                                  (How, are, you, ?)      ?\n","4                              (I, fill, good, today)      +\n","5              (There, is, a, lot, of, history, here)      +\n","6                              (I, love, programming)      +\n","7      (He, told, us, not, to, make, so, much, noise)      +\n","8   (We, were, asked, not, to, park, in, front, of...      +\n","9                      (I, do, n't, have, much, time)      -\n","10                 (She, does, n't, have, any, money)      -\n","11     (They, did, n't, have, any, advice, to, offer)      -\n","12                         (I, am, not, really, sure)      +\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Wdp22w2ArUl","executionInfo":{"status":"ok","timestamp":1621281481115,"user_tz":240,"elapsed":1506,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"4b265667-3738-4b8e-da63-c5e5003b79c4"},"source":["# Ac√° probar√°n su clasificador\n","\n","from sklearn.metrics import classification_report\n","\n","my_clf2 = MyMultinomialNB(alpha=1)\n","my_clf2.fit(X_train2)\n","print('vocab: ', len(my_clf2.vocab), my_clf2.vocab)\n","print('\\nTest probs:')\n","print('\\n'.join([str(l) for l in my_clf2.predict_proba(X_test2)]))\n","print('\\nTest predictions:')\n","my_y_preds = my_clf2.predict(X_test2)\n","print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_y_preds, X_test2['words'])]))\n","print(classification_report(y_true=X_test2['class_'], y_pred=my_y_preds, target_names=['?', '+', '-']))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["vocab:  109 ['fast', \"'ll\", 'told', 'had', 'my', 'back', 'not', '6am', 'Has', 'came', 'He', 'money', 'questions', 'for', 'children', 'Does', 'they', 'How', 'is', 'you', 'very', 'do', 'much', 'meeting', 'local', 'eat', 'live', 'day', 'make', 'her', 'English', 'the', 'company', 'slowly', 'exercises', 'morning', 'enough', 'dress', 'with', 'and', 'from', 'that', 'offer', \"n't\", 'tour', 'were', 'dating', 'does', '‚Äô', '?', 'us', 'useful', 'Have', '61709832145', 'm', 'come', 'France', 'Did', 'phone', 'walks', 'speaks', 'of', 'guide', 'run', 'plenty', 'We', 'She', 'My', 'have', 'here', 'number', 'wife', 'are', 'work', 'as', 'lot', 'What', 'Had', 'anyone', 'daily', 'go', 'everyday', 'be', 'They', 'at', 'long', 'she', 'early', 'UK', 'old', 'time', 'any', 'cinema', 'lived', 'today', 'meat', 'in', 'I', 'this', 'a', 'Do', 'has', 'often', 'many', 'did', 'advice', 'there', 'to', 'people']\n","\n","Test probs:\n","[0.00241833 0.00298307 0.9945986 ]\n","[0.00843731 0.01561141 0.97595128]\n","[0.00959419 0.00394488 0.98646093]\n","[4.04878068e-04 4.99425846e-04 9.99095696e-01]\n","[0.63464636 0.22987937 0.13547426]\n","[0.66550866 0.03156084 0.3029305 ]\n","[0.7105137  0.20919083 0.08029547]\n","[0.11611052 0.80008039 0.08380909]\n","[0.3775419  0.16259502 0.45986307]\n","[0.00375657 0.98188839 0.01435504]\n","[3.37985061e-04 9.93850047e-01 5.81196787e-03]\n","[5.60723220e-05 9.98223814e-01 1.72011336e-03]\n","[0.54610042 0.39561314 0.05828644]\n","\n","Test predictions:\n","? <- Do you know who lives here ?\n","? <- What time is it ?\n","? <- Can you tell me where she comes from ?\n","? <- How are you ?\n","+ <- I fill good today\n","+ <- There is a lot of history here\n","+ <- I love programming\n","- <- He told us not to make so much noise\n","? <- We were asked not to park in front of the house\n","- <- I do n't have much time\n","- <- She does n't have any money\n","- <- They did n't have any advice to offer\n","+ <- I am not really sure\n","              precision    recall  f1-score   support\n","\n","           ?       1.00      0.67      0.80         6\n","           +       0.75      1.00      0.86         3\n","           -       0.80      1.00      0.89         4\n","\n","    accuracy                           0.85        13\n","   macro avg       0.85      0.89      0.85        13\n","weighted avg       0.88      0.85      0.84        13\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JOm_ZS3dBRns"},"source":["##### Comparaci√≥n con sklearn"]},{"cell_type":"code","metadata":{"id":"sqOvA94TJE8Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621281481115,"user_tz":240,"elapsed":1503,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"95bc7c6d-a2ce-46b7-e07e-2fced6e90d06"},"source":["from sklearn.metrics import classification_report\n","\n","X_train2 = pd.DataFrame(data=train_set2)\n","X_test2 = pd.DataFrame(data=test_set2)\n","my_clf2 = MyMultinomialNB(alpha=1)\n","my_clf2.fit(X_train2)\n","print('vocab: ', len(my_clf2.vocab), my_clf2.vocab)\n","print('\\nTest probs:')\n","print('\\n'.join([str(l) for l in my_clf2.predict_proba(X_test2)]))\n","print('\\nTest predictions:')\n","my_y_preds = my_clf2.predict(X_test2)\n","print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_y_preds, X_test2['words'])]))\n","print(classification_report(y_true=X_test2['class_'], y_pred=my_y_preds, target_names=['?', '+', '-']))\n","\n","print('\\nComparing with sklearn')\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline\n","sk_clf2 = Pipeline([('vect', CountVectorizer(lowercase=False, tokenizer=word_tokenize)),\n","                    ('clf', MultinomialNB()),\n","                   ])\n","sk_clf2.fit([' '.join(words) for words in X_train2['words']], X_train2['class_'])\n","print('vocab: ', len(sk_clf2[0].get_feature_names()), sk_clf2[0].get_feature_names())\n","print('same vocab' if sorted(my_clf2.vocab) == sorted(sk_clf2[0].get_feature_names()) else 'distinct vocabs')\n","print('\\nsklearn Test probs:')\n","X_test2_l = [' '.join(words) for words in X_test2['words']]\n","print('\\n'.join([str(l) for l in sk_clf2.predict_proba(X_test2_l)]))\n","print('\\nsklearn Test predictions:')\n","sk_y_preds = sk_clf2.predict(X_test2_l)\n","print('\\n'.join(['{} <- {}'.format(c, s) for c, s in zip(sk_y_preds, X_test2_l)]))\n","print(classification_report(y_true=X_test2['class_'], y_pred=sk_y_preds, target_names=['?', '+', '-']))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["vocab:  109 ['fast', \"'ll\", 'told', 'had', 'my', 'back', 'not', '6am', 'Has', 'came', 'He', 'money', 'questions', 'for', 'children', 'Does', 'they', 'How', 'is', 'you', 'very', 'do', 'much', 'meeting', 'local', 'eat', 'live', 'day', 'make', 'her', 'English', 'the', 'company', 'slowly', 'exercises', 'morning', 'enough', 'dress', 'with', 'and', 'from', 'that', 'offer', \"n't\", 'tour', 'were', 'dating', 'does', '‚Äô', '?', 'us', 'useful', 'Have', '61709832145', 'm', 'come', 'France', 'Did', 'phone', 'walks', 'speaks', 'of', 'guide', 'run', 'plenty', 'We', 'She', 'My', 'have', 'here', 'number', 'wife', 'are', 'work', 'as', 'lot', 'What', 'Had', 'anyone', 'daily', 'go', 'everyday', 'be', 'They', 'at', 'long', 'she', 'early', 'UK', 'old', 'time', 'any', 'cinema', 'lived', 'today', 'meat', 'in', 'I', 'this', 'a', 'Do', 'has', 'often', 'many', 'did', 'advice', 'there', 'to', 'people']\n","\n","Test probs:\n","[0.00241833 0.00298307 0.9945986 ]\n","[0.00843731 0.01561141 0.97595128]\n","[0.00959419 0.00394488 0.98646093]\n","[4.04878068e-04 4.99425846e-04 9.99095696e-01]\n","[0.63464636 0.22987937 0.13547426]\n","[0.66550866 0.03156084 0.3029305 ]\n","[0.7105137  0.20919083 0.08029547]\n","[0.11611052 0.80008039 0.08380909]\n","[0.3775419  0.16259502 0.45986307]\n","[0.00375657 0.98188839 0.01435504]\n","[3.37985061e-04 9.93850047e-01 5.81196787e-03]\n","[5.60723220e-05 9.98223814e-01 1.72011336e-03]\n","[0.54610042 0.39561314 0.05828644]\n","\n","Test predictions:\n","? <- Do you know who lives here ?\n","? <- What time is it ?\n","? <- Can you tell me where she comes from ?\n","? <- How are you ?\n","+ <- I fill good today\n","+ <- There is a lot of history here\n","+ <- I love programming\n","- <- He told us not to make so much noise\n","? <- We were asked not to park in front of the house\n","- <- I do n't have much time\n","- <- She does n't have any money\n","- <- They did n't have any advice to offer\n","+ <- I am not really sure\n","              precision    recall  f1-score   support\n","\n","           ?       1.00      0.67      0.80         6\n","           +       0.75      1.00      0.86         3\n","           -       0.80      1.00      0.89         4\n","\n","    accuracy                           0.85        13\n","   macro avg       0.85      0.89      0.85        13\n","weighted avg       0.88      0.85      0.84        13\n","\n","\n","Comparing with sklearn\n","vocab:  109 [\"'ll\", '61709832145', '6am', '?', 'Did', 'Do', 'Does', 'English', 'France', 'Had', 'Has', 'Have', 'He', 'How', 'I', 'My', 'She', 'They', 'UK', 'We', 'What', 'a', 'advice', 'and', 'any', 'anyone', 'are', 'as', 'at', 'back', 'be', 'came', 'children', 'cinema', 'come', 'company', 'daily', 'dating', 'day', 'did', 'do', 'does', 'dress', 'early', 'eat', 'enough', 'everyday', 'exercises', 'fast', 'for', 'from', 'go', 'guide', 'had', 'has', 'have', 'her', 'here', 'in', 'is', 'live', 'lived', 'local', 'long', 'lot', 'm', 'make', 'many', 'meat', 'meeting', 'money', 'morning', 'much', 'my', \"n't\", 'not', 'number', 'of', 'offer', 'often', 'old', 'people', 'phone', 'plenty', 'questions', 'run', 'she', 'slowly', 'speaks', 'that', 'the', 'there', 'they', 'this', 'time', 'to', 'today', 'told', 'tour', 'us', 'useful', 'very', 'walks', 'were', 'wife', 'with', 'work', 'you', '‚Äô']\n","same vocab\n","\n","sklearn Test probs:\n","[0.00241833 0.00298307 0.9945986 ]\n","[0.00843731 0.01561141 0.97595128]\n","[0.00959419 0.00394488 0.98646093]\n","[4.04878068e-04 4.99425846e-04 9.99095696e-01]\n","[0.63464636 0.22987937 0.13547426]\n","[0.66550866 0.03156084 0.3029305 ]\n","[0.7105137  0.20919083 0.08029547]\n","[0.11611052 0.80008039 0.08380909]\n","[0.3775419  0.16259502 0.45986307]\n","[0.00375657 0.98188839 0.01435504]\n","[3.37985061e-04 9.93850047e-01 5.81196787e-03]\n","[5.60723220e-05 9.98223814e-01 1.72011336e-03]\n","[0.54610042 0.39561314 0.05828644]\n","\n","sklearn Test predictions:\n","? <- Do you know who lives here ?\n","? <- What time is it ?\n","? <- Can you tell me where she comes from ?\n","? <- How are you ?\n","+ <- I fill good today\n","+ <- There is a lot of history here\n","+ <- I love programming\n","- <- He told us not to make so much noise\n","? <- We were asked not to park in front of the house\n","- <- I do n't have much time\n","- <- She does n't have any money\n","- <- They did n't have any advice to offer\n","+ <- I am not really sure\n","              precision    recall  f1-score   support\n","\n","           ?       1.00      0.67      0.80         6\n","           +       0.75      1.00      0.86         3\n","           -       0.80      1.00      0.89         4\n","\n","    accuracy                           0.85        13\n","   macro avg       0.85      0.89      0.85        13\n","weighted avg       0.88      0.85      0.84        13\n","\n"],"name":"stdout"}]}]}