{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tarea_3_Word_Embeddings.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ckbt7VPDhBwb"},"source":["# **Tarea 3 - Word Embeddings 游닄**\n","\n","**Integrantes:**\n","\n","**Fecha l칤mite de entrega 游늱:** 24 de mayo.\n","\n","**Tiempo estimado de dedicaci칩n:**"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-19T18:30:18.109327Z","start_time":"2020-03-19T18:30:18.103344Z"},"id":"q5CSRY4oNCHK"},"source":["\n","**Instrucciones:**\n","- El ejercicio consiste en:\n","    - Responder preguntas relativas a los contenidos vistos en los v칤deos y slides de las clases. \n","    - Entrenar Word2Vec y FastText sobre un peque침o corpus.\n","    - Evaluar los embeddings obtenidos en una tarea de clasificaci칩n.\n","- La tarea se realiza en grupos de **m치ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n","- La entrega es a trav칠s de u-cursos a m치s tardar el d칤a estipulado arriba. No se aceptan atrasos.\n","- El formato de entrega es este mismo Jupyter Notebook.\n","- Al momento de la revisi칩n tu c칩digo ser치 ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci칩n. \n","- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav칠s del canal de Discord del curso. \n","\n","\n","**Referencias**\n","\n","V칤deos: \n","\n","- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n","- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n","- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"]},{"cell_type":"markdown","metadata":{"id":"G4wYf0vgnbTv"},"source":["## **Preguntas te칩ricas 游늿 (3 puntos).** ##\n","Para estas preguntas no es necesario implementar c칩digo, pero pueden utilizar pseudo c칩digo."]},{"cell_type":"markdown","metadata":{"id":"B5hUG6-8ngoK"},"source":["### **Parte 1: Modelos Lineales (1.5 ptos)**"]},{"cell_type":"markdown","metadata":{"id":"5yRvZbhsoi8f"},"source":["Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categor칤as: pol칤tica, deporte, negocios y otros. "]},{"cell_type":"markdown","metadata":{"id":"irsqBVmCnx3M"},"source":["**Pregunta 1**: Dise침e un modelo lineal capaz de clasificar un documento seg칰n estas categor칤as donde el output sea un vector con una distribuci칩n de probabilidad con la pertenencia a cada clase. \n","\n","Especifique: representaci칩n de los documentos de entrada, par치metros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y funci칩n de p칠rdida escogida. **(0.75 puntos)**\n","\n","**Respuesta**: \n"]},{"cell_type":"markdown","metadata":{"id":"G5FaWqBVvL90"},"source":["**Pregunta 2**: Explique c칩mo funciona el proceso de entrenamiento en este tipo de modelos y su evaluaci칩n. **(0.75 puntos)**\n","\n","**Respuesta**: "]},{"cell_type":"markdown","metadata":{"id":"XkK7pc54njZq"},"source":["### **Parte 2: Redes Neuronales (1.5 ptos)** "]},{"cell_type":"markdown","metadata":{"id":"VUbJjlj_9AFC"},"source":["Supongamos que tenemos la siguiente red neuronal."]},{"cell_type":"markdown","metadata":{"id":"obUfuOYB_TOC"},"source":["![image.png](https://drive.google.com/uc?export=view&id=1fFTjtMvH6MY8o42_vj010y8eTuCVb5a3)"]},{"cell_type":"markdown","metadata":{"id":"s2z-8zKW0_6q"},"source":["**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matem치tica. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en funci칩n del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n","\n","Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.75 Puntos)**\n","\n","**Respuesta**: \n","\n","Formula:\n","$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) =$\n","\n","Dimensiones: \n","\n","**Pregunta 2**: Explique qu칠 es backpropagation. 쮺uales ser칤an los par치metros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n","\n","**Respuesta**:\n","\n","**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.5 puntos)**\n","\n","**Respuesta**:"]},{"cell_type":"markdown","metadata":{"id":"ocS_vQhR1gcU"},"source":["## **Preguntas pr치cticas 游눹 (3 puntos).** ##"]},{"cell_type":"markdown","metadata":{"id":"Ol82nJ0FnmcP"},"source":["### **Parte 3: Word Embeddings**"]},{"cell_type":"markdown","metadata":{"id":"OgmeSFqKLpFL"},"source":["En la auxiliar 2 aprendieron como entrenar Word2Vec utilizando gensim. El objetivo de esta parte es comparar los embeddings obtenidos con dos modelos diferentes: Word2Vec y [FastText](https://radimrehurek.com/gensim/models/fasttext.html) (utilizen size=200 en FastText) entrenados en el mismo dataset de di치logos de los Simpson. "]},{"cell_type":"code","metadata":{"id":"ecCvnryeQiG7"},"source":["import re  \n","import pandas as pd \n","from time import time  \n","from collections import defaultdict \n","import string \n","import multiprocessing\n","import os\n","import gensim\n","import sklearn\n","from sklearn import linear_model\n","from collections import Counter\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n","\n","# word2vec\n","from gensim.models import Word2Vec, KeyedVectors, FastText\n","from gensim.models.phrases import Phrases, Phraser\n","from sklearn.model_selection import train_test_split\n","import logging\n","\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","logger = logging.getLogger(__name__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tZgN06q4QPi3"},"source":["Utilizando el dataset adjunto con la tarea:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eY3kmg4onnsu","outputId":"d3525a54-0c10-401e-b3e2-9c6e9e714a2c"},"source":["data_file = \"dialogue-lines-of-the-simpsons.zip\"\n","df = pd.read_csv(data_file)\n","stopwords = pd.read_csv(\n","    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",").values\n","stopwords = Counter(stopwords.flatten().tolist())\n","df = df.dropna().reset_index(drop=True) # Quitar filas vacias"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-05-05 17:58:59,568 : INFO : NumExpr defaulting to 2 threads.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"VAg5a5bmWk3T"},"source":["**Pregunta 1**: Ayud치ndose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec y FastText sobre el dataset anterior. **(1 punto)** (Hint, le puede servir explorar un poco los datos)"]},{"cell_type":"markdown","metadata":{"id":"MWw2fXFRXe5Y"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"Bvwplz7yTNcr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Lr8U5wOTNcr"},"source":["**Pregunta 2**: Encuentre las palabras mas similares a las siguientes: Lisa, Bart, Homer, Marge. C칰al es la diferencia entre ambos resultados? Por qu칠 ocurre esto? Intente comparar ahora Liisa en ambos modelos (doble i). Cuando escoger칤a uno vs el otro? **(0.5 puntos)**"]},{"cell_type":"markdown","metadata":{"id":"yMLyGffVTNcs"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"w6RvJGpbTNcs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IRCB-jqgTNcs"},"source":["### **Parte 4: Aplicar embeddings para clasificar**"]},{"cell_type":"markdown","metadata":{"id":"zlqzlJRSTNcs"},"source":["Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n","\n","Para esto ocuparemos el lexic칩n AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotaci칩n es positiva y un -1 si es negativa."]},{"cell_type":"code","metadata":{"id":"CMskFDmHTNcs"},"source":["AFINN = 'AFINN_full.csv'\n","df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uaKl8hsCTNcs"},"source":["Hint: Para w2v son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendr치n una representaci칩n en AFINN. Pueden utilizar esta funci칩n auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."]},{"cell_type":"code","metadata":{"id":"tWSSuctiTNcs"},"source":["def try_apply(model,word):\n","    try:\n","        aux = model[word]\n","        return True\n","    except KeyError:\n","        #logger.error('Word {} not in dictionary'.format(word))\n","        return False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LrVPeEzgTNcs"},"source":["**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representaci칩n en embedding que acabamos de calcular (con ambos modelos). \n","\n","Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n","\n","Para ambos modelos, separar train y test de acuerdo a la siguiente funci칩n. **(0.75 puntos)**"]},{"cell_type":"code","metadata":{"id":"0Bkt26BwTNcs"},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iDcq5czXTNct"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"upAn_eT4TNct"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kDKe4gA3TNct"},"source":["**Pregunta 2**: Entrenar una regresi칩n log칤stica (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qu칠 se obtienen estos resultados? C칩mo los mejorar칤as? **(0.75 puntos)**"]},{"cell_type":"markdown","metadata":{"id":"hJMzq_dETNct"},"source":["**Respuesta**:"]},{"cell_type":"markdown","metadata":{"id":"izppruGQTNct"},"source":["# Bonus: +0.25 puntos en cualquier pregunta"]},{"cell_type":"markdown","metadata":{"id":"YW0aeK2KTNct"},"source":["**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset m치s grande y obtener mejores resultados. Les puede servir [칠sta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."]},{"cell_type":"markdown","metadata":{"id":"qvHcVS3sTNct"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"MSc8p-T8TNcu"},"source":[""],"execution_count":null,"outputs":[]}]}