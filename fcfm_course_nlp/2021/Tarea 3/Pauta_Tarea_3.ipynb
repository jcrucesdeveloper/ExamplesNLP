{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ckbt7VPDhBwb"},"source":["# **Tarea 3 - Word Embeddings üìö**\n","\n","**Integrantes:**\n","\n","**Fecha l√≠mite de entrega üìÜ:** Martes 06 de Julio.\n","\n","**Tiempo estimado de dedicaci√≥n:**"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-19T18:30:18.109327Z","start_time":"2020-03-19T18:30:18.103344Z"},"id":"q5CSRY4oNCHK"},"source":["\n","**Instrucciones:**\n","- El ejercicio consiste en:\n","    - Responder preguntas relativas a los contenidos vistos en los v√≠deos y slides de las clases. \n","    - Entrenar Word2Vec y FastText sobre un peque√±o corpus.\n","    - Evaluar los embeddings obtenidos en una tarea de clasificaci√≥n.\n","- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n","- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n","- El formato de entrega es este mismo Jupyter Notebook.\n","- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n. \n","- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso. \n","\n","\n","**Referencias**\n","\n","V√≠deos: \n","\n","- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n","- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n","- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"]},{"cell_type":"markdown","metadata":{"id":"G4wYf0vgnbTv"},"source":["## **Preguntas te√≥ricas üìï (3 puntos).** ##\n","Para estas preguntas no es necesario implementar c√≥digo, pero pueden utilizar pseudo c√≥digo."]},{"cell_type":"markdown","metadata":{"id":"B5hUG6-8ngoK"},"source":["### **Parte 1: Modelos Lineales (1.5 ptos)**"]},{"cell_type":"markdown","metadata":{"id":"5yRvZbhsoi8f"},"source":["Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categor√≠as: pol√≠tica, deporte, negocios y otros. "]},{"cell_type":"markdown","metadata":{"id":"irsqBVmCnx3M"},"source":["**Pregunta 1**: Dise√±e un modelo lineal capaz de clasificar un documento seg√∫n estas categor√≠as donde el output sea un vector con una distribuci√≥n de probabilidad con la pertenencia a cada clase. \n","\n","Especifique: representaci√≥n de los documentos de entrada, par√°metros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y funci√≥n de p√©rdida escogida. **(0.75 puntos)**\n","\n","**Respuesta**: Representaci√≥n escogida del documento de entrada: Bag of words por ejemplo\n","\n","Par√°metros del modelo: Matriz de pesos W en donde la columna 1 tengan mayor importancia las palabras de pol√≠tica, columna 2 sobre deporte, columna 3 sobre negocios y columna 4 el resto\n","\n","Transformaciones necesarias: Aplicar softmax al vector de output del modelo.\n","\n","Funci√≥n de p√©rdida escogida: Cross-entropy es el ideal, pero pueden escoger cualquier funci√≥n de perdida multiclase que alcanze el m√≠nimo cuando las predicciones son correctas\n"]},{"cell_type":"markdown","metadata":{"id":"G5FaWqBVvL90"},"source":["**Pregunta 2**: Explique c√≥mo funciona el proceso de entrenamiento en este tipo de modelos y su evaluaci√≥n. **(0.75 puntos)**\n","\n","**Respuesta**: El objetivo del entrenamiento es minimizar la loss del modelo. Se van tuneando los par√°metros de la matriz W hasta encontrar uno que minimice la loss del modelo. La evaluaci√≥n se hace sobre datos que no se han observado para comprobar generalizaci√≥n del modelo."]},{"cell_type":"markdown","metadata":{"id":"XkK7pc54njZq"},"source":["### **Parte 2: Redes Neuronales (1.5 ptos)** "]},{"cell_type":"markdown","metadata":{"id":"VUbJjlj_9AFC"},"source":["Supongamos que tenemos la siguiente red neuronal."]},{"cell_type":"markdown","metadata":{"id":"obUfuOYB_TOC"},"source":["![image.png](https://drive.google.com/uc?export=view&id=1fFTjtMvH6MY8o42_vj010y8eTuCVb5a3)"]},{"cell_type":"markdown","metadata":{"id":"s2z-8zKW0_6q"},"source":["**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matem√°tica. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en funci√≥n del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n","\n","Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.75 Puntos)**\n","\n","**Respuesta**: \n","\n","Formula:\n","$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x})= h(f(g(\\vec{x}W¬π+\\vec{b}¬π)W¬≤+\\vec{b}¬≤)W¬≥+\\vec{b}¬≥)W‚Å¥+\\vec{b}‚Å¥$ (1 punto)\n","\n","Dimensiones: (2 puntos)\n","- $\\vec{x}$: 3\n","- $W¬π$: 3x2\n","- $\\vec{b}¬π$: 2\n","- $W¬≤$: 2x3\n","- $\\vec{b}¬≤$: 3\n","- $W¬≥:$ 3x1\n","- $\\vec{b}¬≥:$ 1\n","- $W‚Å¥:$ 1x4\n","- $\\vec{b}‚Å¥:$ 4\n","\n","**Pregunta 2**: Explique qu√© es backpropagation. ¬øCuales ser√≠an los par√°metros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n","\n","**Respuesta**: \"Backpropagation es una t√©cnica eficiente para evaluar el gradiente de una loss function L en una red neuronal feed-forward con respecto a todos sus par√°metros\" (Cita de clases). Los par√°metros serian de $W¬π,\\vec{b}¬π$ a $W‚Å¥,\\vec{b}‚Å¥$\n","\n","**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.5 puntos)**\n","\n","**Respuesta**: Los 4 pasos son:\n","\n","- Aplicar el vector x y propagarlo por toda la red\n","- Evaluar $\\delta$ para todas las unidades ocultas\n","- Propagar los $\\delta$ desde el final al inicio de la red\n","- Ocupar la formula de $\\frac{\\partial L}{\\partial W}$\n","\n","En la red anterior se necesitan las derivadas: $f', g',h'$"]},{"cell_type":"markdown","metadata":{"id":"ocS_vQhR1gcU"},"source":["## **Preguntas pr√°cticas üíª (3 puntos).** ##"]},{"cell_type":"markdown","metadata":{"id":"Ol82nJ0FnmcP"},"source":["### **Parte 3: Word Embeddings**"]},{"cell_type":"markdown","metadata":{"id":"OgmeSFqKLpFL"},"source":["En la auxiliar 2 aprendieron como entrenar Word2Vec utilizando gensim. El objetivo de esta parte es comparar los embeddings obtenidos con dos modelos diferentes: Word2Vec y [FastText](https://radimrehurek.com/gensim/models/fasttext.html) (utilizen size=200 en FastText) entrenados en el mismo dataset de di√°logos de los Simpson. "]},{"cell_type":"code","metadata":{"id":"ecCvnryeQiG7"},"source":["import re  \n","import pandas as pd \n","from time import time  \n","from collections import defaultdict \n","import string \n","import multiprocessing\n","import os\n","import gensim\n","import sklearn\n","from sklearn import linear_model\n","from collections import Counter\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n","\n","# word2vec\n","from gensim.models import Word2Vec, KeyedVectors, FastText\n","from gensim.models.phrases import Phrases, Phraser\n","from sklearn.model_selection import train_test_split\n","import logging\n","\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","logger = logging.getLogger(__name__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tZgN06q4QPi3"},"source":["Utilizando el dataset adjunto con la tarea:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eY3kmg4onnsu","outputId":"d3525a54-0c10-401e-b3e2-9c6e9e714a2c"},"source":["data_file = \"dialogue-lines-of-the-simpsons.zip\"\n","df = pd.read_csv(data_file)\n","stopwords = pd.read_csv(\n","    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",").values\n","stopwords = Counter(stopwords.flatten().tolist())\n","df = df.dropna().reset_index(drop=True) # Quitar filas vacias"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-05-05 17:58:59,568 : INFO : NumExpr defaulting to 2 threads.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"VAg5a5bmWk3T"},"source":["**Pregunta 1**: Ayud√°ndose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec y FastText sobre el dataset anterior. **(1 punto)** (Hint, le puede servir explorar un poco los datos)"]},{"cell_type":"markdown","metadata":{"id":"MWw2fXFRXe5Y"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"Bvwplz7yTNcr"},"source":["punctuation = string.punctuation + \"¬´¬ª‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî\"\n","def simple_tokenizer(doc, lower=False):\n","    if lower:\n","        tokenized_doc = doc.translate(str.maketrans(\n","            '', '', punctuation)).lower().split()\n","\n","    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n","\n","    tokenized_doc = [\n","        token for token in tokenized_doc if token.lower() not in stopwords\n","    ]\n","    return tokenized_doc\n","content = df['spoken_words']\n","cleaned_content = [simple_tokenizer(doc) for doc in content.values]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_mNOqfl0Ajin"},"source":["w2v_model = Word2Vec(min_count=10,\n","                      window=4,\n","                      size=200,\n","                      sample=6e-5,\n","                      alpha=0.03,\n","                      min_alpha=0.0007,\n","                      negative=20,\n","                      workers=multiprocessing.cpu_count())\n","ft_model = FastText(size=200, window=3, min_count=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdu4xlZ6Alu5"},"source":["w2v_model.build_vocab(sentences, progress_per=10000)\n","ft_model.build_vocab(sentences, progress_per=10000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FjJ7yi-0AnaR"},"source":["t = time()\n","w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=15, report_delay=10)\n","print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n","if not os.path.exists('./pretrained_models'):\n","    os.mkdir('./pretrained_models')\n","w2v_model.save('./pretrained_models/w2v.model')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CsU36VjLAo2y"},"source":["t = time()\n","ft_model.train(sentences, total_examples=ft_model.corpus_count, epochs=15, report_delay=10)\n","print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n","if not os.path.exists('./pretrained_models'):\n","    os.mkdir('./pretrained_models')\n","ft_model.save('./pretrained_models/ft.model')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Lr8U5wOTNcr"},"source":["**Pregunta 2**: Encuentre las palabras mas similares a las siguientes: Lisa, Bart, Homer, Marge. C√∫al es la diferencia entre ambos resultados? Por qu√© ocurre esto? Intente comparar ahora Liisa en ambos modelos (doble i). Cuando escoger√≠a uno vs el otro? **(0.5 puntos)**"]},{"cell_type":"markdown","metadata":{"id":"yMLyGffVTNcs"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"w6RvJGpbTNcs"},"source":["w2v_model.wv.most_similar(positive=[\"Lisa\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bAHsds_5Artb"},"source":["ft_model.wv.most_similar(positive=[\"Lisa\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EBASS4yrAuCq"},"source":["w2v_model.wv.most_similar(positive=[\"Bart\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GPP8VVJlAuSy"},"source":["ft_model.wv.most_similar(positive=[\"Bart\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_CQalj6A0DO"},"source":["w2v_model.wv.most_similar(positive=[\"Homer\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h6B7zV40A0Re"},"source":["ft_model.wv.most_similar(positive=['Homer'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xMWK_OCMA0e9"},"source":["w2v_model.wv.most_similar(positive=[\"Marge\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZuXm2nXIA3Ul"},"source":["ft_model.wv.most_similar(positive=[\"Marge\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"utwaAGADA3l5"},"source":["w2v_model.wv.most_similar(positive=[\"Liisa\"]) # Es normal el siguiente error!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zfuwx7q1BA65"},"source":["ft_model.wv.most_similar(positive=[\"Liisa\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ez5EFAP_Bd3m"},"source":["W2V encuentra las palabras del vocabulario que est√°n mas cerca mientras que FastText busca los ngramas. FastText funciona mejor que W2V cuando buscamos palabras que est√°n fuera del vocabulario.\n"]},{"cell_type":"markdown","metadata":{"id":"IRCB-jqgTNcs"},"source":["### **Parte 4: Aplicar embeddings para clasificar**"]},{"cell_type":"markdown","metadata":{"id":"zlqzlJRSTNcs"},"source":["Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n","\n","Para esto ocuparemos el lexic√≥n AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotaci√≥n es positiva y un -1 si es negativa."]},{"cell_type":"code","metadata":{"id":"CMskFDmHTNcs"},"source":["AFINN = 'AFINN_full.csv'\n","df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uaKl8hsCTNcs"},"source":["Hint: Para w2v son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendr√°n una representaci√≥n en AFINN. Pueden utilizar esta funci√≥n auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."]},{"cell_type":"code","metadata":{"id":"tWSSuctiTNcs"},"source":["def try_apply(model,word):\n","    try:\n","        aux = model[word]\n","        return True\n","    except KeyError:\n","        #logger.error('Word {} not in dictionary'.format(word))\n","        return False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LrVPeEzgTNcs"},"source":["**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representaci√≥n en embedding que acabamos de calcular (con ambos modelos). \n","\n","Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n","\n","Para ambos modelos, separar train y test de acuerdo a la siguiente funci√≥n. **(0.75 puntos)**"]},{"cell_type":"code","metadata":{"id":"0Bkt26BwTNcs"},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iDcq5czXTNct"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"upAn_eT4TNct"},"source":["df_afinn = df_afinn[df_afinn[0].apply(lambda x: try_apply(ft_model.wv,x))]\n","df_afinn[0] = df_afinn[0].apply(lambda x: ft_model.wv[x])\n","X = np.stack(df_afinn[0].values)\n","y = df_afinn[1].values\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kDKe4gA3TNct"},"source":["**Pregunta 2**: Entrenar una regresi√≥n log√≠stica (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qu√© se obtienen estos resultados? C√≥mo los mejorar√≠as? **(0.75 puntos)**"]},{"cell_type":"markdown","metadata":{"id":"hJMzq_dETNct"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"z_hvlmOHBJoG"},"source":["reg = linear_model.LogisticRegression(penalty='l2', solver='liblinear', C=1)\n","reg.fit(X_train, y_train)\n","y_pred = reg.predict(X_test)\n","acc = accuracy_score(y_test, y_pred)\n","pre = precision_score(y_test, y_pred)\n","rec = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","conf = confusion_matrix(y_test, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKzZJUAtBMLF"},"source":["logger.info(\"The accuracy is {0}\".format(acc))\n","logger.info(\"The precision is {0}\".format(pre))\n","logger.info(\"The recall is {0}\".format(rec))\n","logger.info(\"The f1 score is {0}\".format(f1))\n","logger.info(\"The confusion matrix:\\n{0}\".format(conf))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oHEw2yynBNx1"},"source":["df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)\n","df_afinn = df_afinn[df_afinn[0].apply(lambda x: try_apply(w2v_model,x))]\n","df_afinn[0] = df_afinn[0].apply(lambda x: w2v_model[x])\n","X = np.stack(df_afinn[0].values)\n","y = df_afinn[1].values\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)\n","reg = linear_model.LogisticRegression(penalty='l2', solver='liblinear', C=1)\n","reg.fit(X_train, y_train)\n","y_pred = reg.predict(X_test)\n","acc = accuracy_score(y_test, y_pred)\n","pre = precision_score(y_test, y_pred)\n","rec = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","conf = confusion_matrix(y_test, y_pred)\n","logger.info(\"The accuracy is {0}\".format(acc))\n","logger.info(\"The precision is {0}\".format(pre))\n","logger.info(\"The recall is {0}\".format(rec))\n","logger.info(\"The f1 score is {0}\".format(f1))\n","logger.info(\"The confusion matrix:\\n{0}\".format(conf))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cRAz0bDZBZRj"},"source":["Estos resultados son p√©simos. Hay 2 razones posibles: El dataset es muy peque√±o c√≥mo para que estos modelos logren aprender bien las relaciones entre las palabras o puede que con los di√°logos de los Simpson no se obtengan buenos embeddings para clasificar palabras por sentimiento. Se podr√≠a mejorar a√±adiendo mas datos, por ejemplo los subt√≠tulos de las pel√≠culas o simplemente buscar otro dataset m√°s grande."]},{"cell_type":"markdown","metadata":{"id":"izppruGQTNct"},"source":["# Bonus: +0.25 puntos en cualquier pregunta"]},{"cell_type":"markdown","metadata":{"id":"YW0aeK2KTNct"},"source":["**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset m√°s grande y obtener mejores resultados. Les puede servir [√©sta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."]},{"cell_type":"markdown","metadata":{"id":"qvHcVS3sTNct"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"MSc8p-T8TNcu"},"source":["df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)\n","import gensim.downloader as api\n","model = api.load(\"glove-wiki-gigaword-300\") \n","df_afinn = df_afinn[df_afinn[0].apply(lambda x: try_apply(model,x))]\n","df_afinn[0] = df_afinn[0].apply(lambda x: model[x])\n","X = np.stack(df_afinn[0].values)\n","y = df_afinn[1].values\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)\n","reg = linear_model.LogisticRegression(penalty='l2', solver='liblinear', C=1)\n","reg.fit(X_train, y_train)\n","y_pred = reg.predict(X_test)\n","acc = accuracy_score(y_test, y_pred)\n","pre = precision_score(y_test, y_pred)\n","rec = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","conf = confusion_matrix(y_test, y_pred)\n","logger.info(\"The accuracy is {0}\".format(acc))\n","logger.info(\"The precision is {0}\".format(pre))\n","logger.info(\"The recall is {0}\".format(rec))\n","logger.info(\"The f1 score is {0}\".format(f1))\n","logger.info(\"The ROC AUC score is {0}\".format(roc))\n","logger.info(\"The confusion matrix:\\n{0}\".format(conf))\n","logger.info(\"The kappa is {0}\".format(kappa))\n","logger.info(\"Summary report:\\n{0}\".format(class_rep))"],"execution_count":null,"outputs":[]}]}