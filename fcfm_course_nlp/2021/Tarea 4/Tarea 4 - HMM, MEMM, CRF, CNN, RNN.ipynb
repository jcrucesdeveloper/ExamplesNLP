{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tarea 4 - HMM, MEMM, CRF, CNN, RNN.ipynb","provenance":[{"file_id":"12LcpdRWmbyBZI5mX4a_oN-acdJUhSzef","timestamp":1622493646585}],"collapsed_sections":["ClRAHR95Y8aB","tj1V_sAzZCHY"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"X4lL5hGw07yP"},"source":["# **Tarea 4 - CC6205 Natural Language Processing üìö**\n","\n","**Integrantes:**\n","\n","**Fecha l√≠mite de entrega üìÜ:** Martes 22 de junio.\n","\n","**Tiempo estimado de dedicaci√≥n:**"]},{"cell_type":"markdown","metadata":{"id":"P6jB5fLGMCaI"},"source":["Bienvenid@s a la cuarta tarea del curso de Natural Language Processing (NLP). \n","En esta tarea estaremos tratando el problema de **tagging** (generaci√≥n de secuencias de etiquetas del mismo largo que la secuencia de input), el uso de **Convolutional Neural Networks** y **Recurrent Neural Networks**, e implementaremos una red usando PyTorch. \n","\n","Usen $\\LaTeX$ para las f√≥rmulas matem√°ticas. En la parte de programaci√≥n pueden usar lo que quieran, pero la [Axiliar 3](https://www.youtube.com/watch?v=r1pt9v_vuy8&list=PLwITEwTGVF6jXRu6l55W4OedqDiLgU9OM&index=2&t=5s) les puede ser de *gran ayuda*.\n","\n","**Instrucciones:**\n","- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n","- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n","- El formato de entrega es este mismo Jupyter Notebook.\n","- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n.\n","- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso.\n","\n","Si a√∫n no han visto las clases, se recomienda visitar los links de las referencias.\n","\n","**Referencias:**\n","\n","- [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/Tjgb-yQOg54), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)       \n","- [MEMMs and CRFs](slides/NLP-CRF.pdf) | ([tex source file](slides/NLP-CRF.tex)), [notes 1](http://www.cs.columbia.edu/~mcollins/crf.pdf), [notes 2](http://www.cs.columbia.edu/~mcollins/fb.pdf), [video 1](https://youtu.be/qlI-4lSUDkg), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/ZpUwDy6o28Y)\n","- [Convolutional Neural Networks](slides/NLP-CNN.pdf) | ([tex source file](slides/NLP-CNN.tex)), [video](https://youtu.be/lLZW5Fn40r8)\n","- [Recurrent Neural Networks](slides/NLP-RNN.pdf) | ([tex source file](slides/NLP-RNN.tex)), [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)"]},{"cell_type":"markdown","metadata":{"id":"bWXD3D7RYKJ-"},"source":["# Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMM) and Conditional Random Field(CRF)\n","\n","### Pregunta 1 (1 pt)\n","Para un problema de POS tagging se define el conjunto de etiquetas $S = \\{ \\text{DET}, \\text{NOUN}, \\text{VERB}, \\text{ADP} \\}$ y se tiene un Hidden Markov Model con los siguientes par√°metros estimados a partir de un corpus de entrenamiento:\n","\n","\\begin{equation}\n","\\begin{split}\n","q(\\text{NOUN}| \\text{ VERB}, \\text{DET}) &= 0.3 \\\\\n","q(\\text{NOUN}|\\ w, \\text{DET}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n","q(\\text{DET}| \\text{ VERB}, \\text{NOUN}) &= 0.4 \\\\\n","q(\\text{DET}|\\ w, \\text{NOUN}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n","e(the|\\text{ DET}) &= 0.5 \\\\\n","e(pasta|\\text{ NOUN}) &= 0.6\n","\\end{split}\n","\\end{equation}\n","\n","Luego para la oraci√≥n: `the man is pouring sauce on the pasta`, se tiene una tabla de programaci√≥n din√°mica con los siguientes valores:\n","\\begin{equation}\n","\\begin{split}\n","\\pi(7,\\text{DET},\\text{DET})&=0.1\\\\\n","\\pi(7,\\text{NOUN},\\text{DET})&=0.2\\\\\n","\\pi(7,\\text{VERB},\\text{DET})&=0.01\\\\\n","\\pi(7,\\text{ADP},\\text{DET})&=0.5\n","\\end{split}\n","\\end{equation}\n","\n","Con esta informaci√≥n, calcule el valor de $\\pi(8,\\text{DET},\\text{NOUN})$. Puede dejar el resultado expresado como una fracci√≥n.\n"]},{"cell_type":"markdown","metadata":{"id":"5UvNmJMvi83q"},"source":["**Respuesta**\n","\n","\\begin{equation}\n","escriba\\ su\\ respuesta\\ aqu√≠\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"oiwJb_vmkKLZ"},"source":["### Pregunta 2 (0.5 pts)\n","Comente  sobre las similitudes o diferencias entre los HMMs, MEMMs y CRFs. Para esto, responda las siguientes preguntas.\n","\n","#### 2.1. ¬øPara qu√© tipo de tarea sirven? D√© dos ejemplo de este tipo de tarea y descr√≠balos brevemente. (0.1 pts)\n","\n","**Respuesta:** Escriba su respuesta aqu√≠\n","\n","#### 2.2. ¬øQu√© modelos usan features? ¬øQu√© ventajas conlleva esto? (0.1 pts)\n","\n","**Respuesta:** Escriba su respuesta aqu√≠\n","\n","#### 2.3. ¬øC√≥mo maneja cada uno de los modelos las palabras con baja frecuencia en el set de train? (0.1 pts)\n","\n","**Respuesta:** Escriba su respuesta aqu√≠\n","\n","#### 2.4. ¬øQu√© le permite a los CRF realizar decisiones globales? ¬øQu√© diferencia con respecto a los MEMMs permite lograr esto? ¬øPor qu√© los HMMs tampoco son capaces de tomar decisiones globales? (0.1 pts)\n","\n","**Respuesta:** Escriba su respuesta aqu√≠\n","\n","#### 2.5 Dado una secuencia de $x_1, ..., x_m$ ¬øCu√°ntas posibles secuencias de etiquetas se pueden generar para un conjunto de etiquetas $S$ con $|S|=k$ ? ¬øAnalizarlas todas ser√≠a computacionalmente tratable? (0.1 pts)\n","\n","**Respuesta:** Escriba su respuesta aqu√≠"]},{"cell_type":"markdown","metadata":{"id":"ClRAHR95Y8aB"},"source":["# Convolutional Neural Networks\n","### Pregunta 3 (1 pt)\n","\n","Considere la frase $w_{1..7}=$ `El agua moja y el fuego quema` $=[El, agua, moja, y, el, fuego, quema]$.\n","\n","La siguiente matriz de embeddings, donde la i-√©sima fila corresponde al vector de embedding de la i-√©sima palabra, ordenadas seg√∫n aparecen en la frase. (vectores de largo 2).\n","\\begin{equation}\n","E = \\begin{pmatrix}\n","2 & 2\\\\\n","0 & -2\\\\\n","0 & 1\\\\\n","-2 & 1\\\\\n","1 & 0\\\\\n","-1 & 1\\\\\n","1 & 1\n","\\end{pmatrix}\n","\\end{equation}\n","\n","Los siguientes 3 filtros\n","\\begin{equation}\n","U = \\begin{pmatrix}\n","-1 & 1 & 0\\\\\n","1 & 1 & 0\\\\\n","0 & 0 & -1\\\\\n","1 & -1 & -1\\\\\n","-1 & -1 & 1\\\\\n","1 & 0 & -1\n","\\end{pmatrix}\n","\\end{equation}\n","\n","Y la funci√≥n de activaci√≥n\n","\\begin{equation}\n","tanh = \\frac{e^{2x} - 1}{e^{2x} + 1}\n","\\end{equation}\n","\n","Usando estos param√°tros escriba los pasos para calcular la representaci√≥n (vector) resultante de aplicar la operaci√≥n de convoluci√≥n (sin padding) + max pooling. ¬øDe qu√© tama√±o ser√≠a la ventana que debemos usar?\n"]},{"cell_type":"markdown","metadata":{"id":"SlQ30Arkq0u4"},"source":["**Respuesta**\n","\n","\\begin{equation}\n","escriba\\ su\\ respuesta\\ aqu√≠\n","\\end{equation}"]},{"cell_type":"code","metadata":{"id":"FZdqJSNZKEBq"},"source":["# puedes comprobar tus calculos con algunas lineas de c√≥digo python\n","\n","import numpy as np\n","\n","U = np.array([[-1, 1, 0],\n","              [1,1,0],\n","              [0,0,-1],\n","              [1,-1,-1],\n","              [-1,-1,1],\n","              [1,0,-1]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tj1V_sAzZCHY"},"source":["# Recurrent Neural Networks\n","### Pregunta 4 (1 pt)\n","Usando los embeddings de dos dimensiones de la pregunta anteror, la oraci√≥n `el fuego quema` la podemos representar por una secuencia de vectores $(\\vec{x}_1,\\vec{x}_2,\\vec{x}_3)$, con $\\vec{x}_i \\in \\mathbb{R}^{d_x}$ y $d_x=2$.\n","\n","Tenemos una red recurrente *Elman* definidad como: \n","\\begin{equation}\n","\\begin{split}\n","\\vec{s}_i &= R_{SRNN}\\left (\\vec{x}_i, \\vec{s}_{i-1}\\right ) = g \\left (\\vec{s}_{i-1}W^s + \\vec{x}_i W^x + \\vec{b}\\right ) \\\\\n","\\vec{y}_i &= O_{SRNN}\\left(\\vec{s}_i\\right) = \\vec{s}_i \\\\\n","\\end{split}\n","\\end{equation}\n","donde\n","\\begin{equation}\n","\\vec{s}_i, \\vec{y}_i \\in \\mathbb{R}^{d_s}, \\quad W^x \\in \\mathbb{R}^{d_x \\times d_s}, \\quad W^s \\in \\mathbb{R}^{d_s \\times d_s}, \\quad \\vec{b} \\in \\mathbb{R}^{d_s},\n","\\end{equation}\n","y los vectores de estado $s_i$ son de tres dimensiones, $ds= 3$.\n","\n","Sea\n","\\begin{equation}\n","\\begin{split}\n","\\vec{s}_0 &= [0,0,0]\\\\\n","W^x &= \\begin{pmatrix}\n","0 &  0 & 1\\\\\n","1 & -1 & 0\n","\\end{pmatrix} \\\\\n","W^s &= \\begin{pmatrix}\n","1 & 0 &  1\\\\\n","0 & 1 & -1\\\\\n","1 & 1 &  1\n","\\end{pmatrix} \\\\\n","\\vec{b} &= [0, 0, 0] \\\\\n","g(x) &= ReLu(x) = max(0, x)\n","\\end{split}\n","\\end{equation}\n","\n","<br>\n","\n","Calcule manualmente los valores de los vectores $\\vec{s}_1, \\vec{s}_2,\\vec{s}_3$ y de $\\vec{y}_1, \\vec{y}_2,\\vec{y}_3$."]},{"cell_type":"markdown","metadata":{"id":"7M7sqIQV-Q3a"},"source":["**Respuesta**\n","\n","\\begin{equation}\n","escriba\\ su\\ respuesta\\ aqu√≠\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"W4rAT6ELxRZW"},"source":["### Pregunta 5 (0.5 pts)\n","¬øDe qu√© forma las RNN y las CNN logran aprender representaciones espec√≠ficas\n","para la tarea objetivo? Compare la forma en que las RNN y las CNN aprenden con los modelos que usan *features* dise√±adas manualmente."]},{"cell_type":"markdown","metadata":{"id":"b6AXbQSgA_t8"},"source":["**Respuesta**\n","\n","escriba su respuesta aqu√≠"]},{"cell_type":"markdown","metadata":{"id":"qxQIuO8axTUa"},"source":["# Redes neuronales con PyTorch\n","### Pregunta 6 (2 pts)\n","En esta parte van a tener que implementar una red neuronal Feed Forward. Adem√°s, deber√°n entrenar el modelo usando uno de los datasets de TorchText. En la secci√≥n de la respuesta hay un esqueleto de lo que deben hacer, deber√°n completar los metodos del modelo e implementar la parte asociada al entrenamiento. Como les mencionamos en la [Auxiliar 3](https://www.youtube.com/watch?v=r1pt9v_vuy8&list=PLwITEwTGVF6jXRu6l55W4OedqDiLgU9OM&index=2&t=5s), el proceso de entrenamiento es bastante est√°ndar, as√≠ que se pueden guiar en gran medida por los ejemplos que ah√≠ mostramos y los que vamos a ver en las pr√≥ximas auxiliares.\n","\n","#### 6.1 Capa Convolucional (Opcional)\n","Agregue a la arquitectura una capa convolucional. Para esto puede registrar el parametro $U$ en la red y realizar el computo de la convoluci√≥n en el metodo forward de la red, o puede usar la clase [`torch.nn.Conv1d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#conv1d) de `torch`.\n"]},{"cell_type":"code","metadata":{"id":"LVKEaQXZ3eGl","colab":{"base_uri":"https://localhost:8080/","height":819},"executionInfo":{"status":"ok","timestamp":1624417172812,"user_tz":240,"elapsed":430115,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"26433b80-9873-4b76-e189-b0a86e4b3689"},"source":["# %%capture\n","# Nos aseguramos que instalemos la version anterior de torchtext 0.9.0\n","!pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install torchtext==0.9.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.8.0+cu111\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 834.1MB 2.5MB/s eta 0:07:39tcmalloc: large alloc 1147494400 bytes == 0x5624669b4000 @  0x7f9a5118e615 0x56242d28ccdc 0x56242d36c52a 0x56242d28fafd 0x56242d380fed 0x56242d303988 0x56242d2fe4ae 0x56242d2913ea 0x56242d3037f0 0x56242d2fe4ae 0x56242d2913ea 0x56242d30032a 0x56242d381e36 0x56242d2ff853 0x56242d381e36 0x56242d2ff853 0x56242d381e36 0x56242d2ff853 0x56242d381e36 0x56242d4043e1 0x56242d3646a9 0x56242d2cfcc4 0x56242d290559 0x56242d3044f8 0x56242d29130a 0x56242d2ff3b5 0x56242d2fe7ad 0x56242d2913ea 0x56242d2ff3b5 0x56242d29130a 0x56242d2ff3b5\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 1055.7MB 1.3MB/s eta 0:12:21tcmalloc: large alloc 1434370048 bytes == 0x5624ab00a000 @  0x7f9a5118e615 0x56242d28ccdc 0x56242d36c52a 0x56242d28fafd 0x56242d380fed 0x56242d303988 0x56242d2fe4ae 0x56242d2913ea 0x56242d3037f0 0x56242d2fe4ae 0x56242d2913ea 0x56242d30032a 0x56242d381e36 0x56242d2ff853 0x56242d381e36 0x56242d2ff853 0x56242d381e36 0x56242d2ff853 0x56242d381e36 0x56242d4043e1 0x56242d3646a9 0x56242d2cfcc4 0x56242d290559 0x56242d3044f8 0x56242d29130a 0x56242d2ff3b5 0x56242d2fe7ad 0x56242d2913ea 0x56242d2ff3b5 0x56242d29130a 0x56242d2ff3b5\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 1336.2MB 1.2MB/s eta 0:08:54tcmalloc: large alloc 1792966656 bytes == 0x56242fe3c000 @  0x7f9a5118e615 0x56242d28ccdc 0x56242d36c52a 0x56242d28fafd 0x56242d380fed 0x56242d303988 0x56242d2fe4ae 0x56242d2913ea 0x56242d3037f0 0x56242d2fe4ae 0x56242d2913ea 0x56242d30032a 0x56242d381e36 0x56242d2ff853 0x56242d381e36 0x56242d2ff853 0x56242d381e36 0x56242d2ff853 0x56242d381e36 0x56242d4043e1 0x56242d3646a9 0x56242d2cfcc4 0x56242d290559 0x56242d3044f8 0x56242d29130a 0x56242d2ff3b5 0x56242d2fe7ad 0x56242d2913ea 0x56242d2ff3b5 0x56242d29130a 0x56242d2ff3b5\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1691.1MB 1.4MB/s eta 0:03:36tcmalloc: large alloc 2241208320 bytes == 0x56249ac24000 @  0x7f9a5118e615 0x56242d28ccdc 0x56242d36c52a 0x56242d28fafd 0x56242d380fed 0x56242d303988 0x56242d2fe4ae 0x56242d2913ea 0x56242d3037f0 0x56242d2fe4ae 0x56242d2913ea 0x56242d30032a 0x56242d381e36 0x56242d2ff853 0x56242d381e36 0x56242d2ff853 0x56242d381e36 0x56242d2ff853 0x56242d381e36 0x56242d4043e1 0x56242d3646a9 0x56242d2cfcc4 0x56242d290559 0x56242d3044f8 0x56242d29130a 0x56242d2ff3b5 0x56242d2fe7ad 0x56242d2913ea 0x56242d2ff3b5 0x56242d29130a 0x56242d2ff3b5\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1982.2MB 1.2MB/s eta 0:00:01tcmalloc: large alloc 1982251008 bytes == 0x562520586000 @  0x7f9a5118d1e7 0x56242d2c2f37 0x56242d28ccdc 0x56242d36c52a 0x56242d28fafd 0x56242d380fed 0x56242d303988 0x56242d2fe4ae 0x56242d2913ea 0x56242d2ff60e 0x56242d2fe4ae 0x56242d2913ea 0x56242d2ff60e 0x56242d2fe4ae 0x56242d2913ea 0x56242d2ff60e 0x56242d2fe4ae 0x56242d2913ea 0x56242d2ff60e 0x56242d2fe4ae 0x56242d2913ea 0x56242d2ff60e 0x56242d29130a 0x56242d2ff60e 0x56242d2fe4ae 0x56242d2913ea 0x56242d30032a 0x56242d2fe4ae 0x56242d2913ea 0x56242d30032a 0x56242d2fe4ae\n","tcmalloc: large alloc 2477817856 bytes == 0x56260acb0000 @  0x7f9a5118e615 0x56242d28ccdc 0x56242d36c52a 0x56242d28fafd 0x56242d380fed 0x56242d303988 0x56242d2fe4ae 0x56242d2913ea 0x56242d2ff60e 0x56242d2fe4ae 0x56242d2913ea 0x56242d2ff60e 0x56242d2fe4ae 0x56242d2913ea 0x56242d2ff60e 0x56242d2fe4ae 0x56242d2913ea 0x56242d2ff60e 0x56242d2fe4ae 0x56242d2913ea 0x56242d2ff60e 0x56242d29130a 0x56242d2ff60e 0x56242d2fe4ae 0x56242d2913ea 0x56242d30032a 0x56242d2fe4ae 0x56242d2913ea 0x56242d30032a 0x56242d2fe4ae 0x56242d291a81\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1982.2MB 3.6kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (1.19.5)\n","\u001b[31mERROR: torchvision 0.10.0+cu102 has requirement torch==1.9.0, but you'll have torch 1.8.0+cu111 which is incompatible.\u001b[0m\n","\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.8.0+cu111 which is incompatible.\u001b[0m\n","Installing collected packages: torch\n","  Found existing installation: torch 1.9.0+cu102\n","    Uninstalling torch-1.9.0+cu102:\n","      Successfully uninstalled torch-1.9.0+cu102\n","Successfully installed torch-1.8.0+cu111\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Collecting torchtext==0.9.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/50/84184d6230686e230c464f0dd4ff32eada2756b4a0b9cefec68b88d1d580/torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.1MB 2.4MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.41.1)\n","Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (1.8.0+cu111)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->torchtext==0.9.0) (3.7.4.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n","Installing collected packages: torchtext\n","  Found existing installation: torchtext 0.10.0\n","    Uninstalling torchtext-0.10.0:\n","      Successfully uninstalled torchtext-0.10.0\n","Successfully installed torchtext-0.9.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torchtext"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"dJ-wrzFO5mCC"},"source":["# Trabajaremos con el dataset AG_NEWS de torchtext\n","# https://pytorch.org/text/stable/datasets.html#ag-news\n","import os\n","import torch\n","from random import choice\n","from torchtext.datasets import AG_NEWS\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","os.makedirs(\"data\", exist_ok=True)\n","train_dataset, test_dataset = AG_NEWS(root=\"data\")\n","\n","# Informacion relevante del dataset\n","tokenizer = get_tokenizer(\"basic_english\")\n","vocab = build_vocab_from_iterator(tokenizer(x[1]) for x in train_dataset)\n","num_classes = 4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IXngUm9HxKvA"},"source":["# De aca para abajo viene su respuesta, completen las funciones en la red\n","# y luego entrenen el modelo y evaluenlo usando los dataset que acaban de\n","# cargar\n","import torch\n","import torch.nn as nn\n","\n","\n","class CNNClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim=32, num_classes=10, \n","                 use_cnn=False, cnn_pool_channels=24, cnn_kernel_size=3):\n","        # Aca deben registrar sus parametros. A lo menos necesitan\n","        # una capa de embedding y un MLP basico (una capa lineal + softmax)\n","        ...\n","\n","    def forward(self, *args): # Reemplacen el *args por sus argumentos\n","        # Ac√° debe programar la pasada hacia adelante\n","        ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPHyqgId96ra"},"source":["# El resto de su respuesta. Ac√° deben programar el entrenamiento de la red\n","\n","import sys\n","from torch.optim import SGD\n","from torch.utils.data import DataLoader\n","\n","\n","def generate_batch(batch):\n","  # hint: si definen la capa de embedding del modelo usando nn.EmbeddingBag,\n","  # les puede ayudar computar offsets para cada elemento del batch\n","  ...\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","model = CNNClassifier(...).to(device)\n","optimizer = SGD(...)\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","# Ac√° deben programar el entrenamiento de la red.\n","...\n","\n","# Intenten superar un Accuracy de 90% en el conjunto de test"],"execution_count":null,"outputs":[]}]}