{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tarea 4 - pauta.ipynb","provenance":[{"file_id":"1KzvykC6jqYIdE9LXFQMIyhBZXcl4KmH6","timestamp":1622526705773}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"UwaDuQqCOyLJ"},"source":["# **Tarea 4 - CC6205 Natural Language Processing üìö**\n","\n","**Integrantes:**\n","\n","**Fecha l√≠mite de entrega üìÜ:** Martes 22 de junio.\n","\n","**Tiempo estimado de dedicaci√≥n:**"]},{"cell_type":"markdown","metadata":{"id":"X4lL5hGw07yP"},"source":["Bienvenid@s a la cuarta tarea del curso de Natural Language Processing (NLP). \n","En esta tarea estaremos tratando el problema de **tagging** (generaci√≥n de secuencias de etiquetas del mismo largo que la secuencia de input), el uso de **Convolutional Neural Networks** y **Recurrent Neural Networks**, e implementaremos una red usando PyTorch. \n","\n","Usen $\\LaTeX$ para las f√≥rmulas matem√°ticas. En la parte de programaci√≥n pueden usar lo que quieran, pero la [Axiliar 3](https://www.youtube.com/watch?v=r1pt9v_vuy8&list=PLwITEwTGVF6jXRu6l55W4OedqDiLgU9OM&index=2&t=5s) les puede ser de *gran ayuda*.\n","\n","**Instrucciones:**\n","- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n","- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n","- El formato de entrega es este mismo Jupyter Notebook.\n","- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n.\n","- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso.\n","\n","Si a√∫n no han visto las clases, se recomienda visitar los links de las referencias.\n","\n","**Referencias:**\n","\n","- [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/Tjgb-yQOg54), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)       \n","- [MEMMs and CRFs](slides/NLP-CRF.pdf) | ([tex source file](slides/NLP-CRF.tex)), [notes 1](http://www.cs.columbia.edu/~mcollins/crf.pdf), [notes 2](http://www.cs.columbia.edu/~mcollins/fb.pdf), [video 1](https://youtu.be/qlI-4lSUDkg), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/ZpUwDy6o28Y)\n","- [Convolutional Neural Networks](slides/NLP-CNN.pdf) | ([tex source file](slides/NLP-CNN.tex)), [video](https://youtu.be/lLZW5Fn40r8)\n","- [Recurrent Neural Networks](slides/NLP-RNN.pdf) | ([tex source file](slides/NLP-RNN.tex)), [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)"]},{"cell_type":"markdown","metadata":{"id":"bWXD3D7RYKJ-"},"source":["# Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMM) and Conditional Random Field(CRF)\n","\n","### Pregunta 1 (1 pt)\n","Para un problema de POS tagging se define el conjunto de etiquetas $S = \\{ \\text{DET}, \\text{NOUN}, \\text{VERB}, \\text{ADP} \\}$ y se tiene un Hidden Markov Model con los siguientes par√°metros estimados a partir de un corpus de entrenamiento:\n","\n","\\begin{equation}\n","\\begin{split}\n","q(\\text{NOUN}| \\text{ VERB}, \\text{DET}) &= 0.3 \\\\\n","q(\\text{NOUN}|\\ w, \\text{DET}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n","q(\\text{DET}| \\text{ VERB}, \\text{NOUN}) &= 0.4 \\\\\n","q(\\text{DET}|\\ w, \\text{NOUN}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n","e(the|\\text{ DET}) &= 0.5 \\\\\n","e(pasta|\\text{ NOUN}) &= 0.6\n","\\end{split}\n","\\end{equation}\n","\n","Luego para la oraci√≥n: `the man is pouring sauce on the pasta`, se tiene una tabla de programaci√≥n din√°mica con los siguientes valores:\n","\\begin{equation}\n","\\begin{split}\n","\\pi(7,\\text{DET},\\text{DET})&=0.1\\\\\n","\\pi(7,\\text{NOUN},\\text{DET})&=0.2\\\\\n","\\pi(7,\\text{VERB},\\text{DET})&=0.01\\\\\n","\\pi(7,\\text{ADP},\\text{DET})&=0.5\n","\\end{split}\n","\\end{equation}\n","\n","Con esta informaci√≥n, calcule el valor de $\\pi(8,\\text{DET},\\text{NOUN})$. Puede dejar el resultado expresado como una fracci√≥n.\n"]},{"cell_type":"markdown","metadata":{"id":"5EzgysW9kGi-"},"source":["**Respuesta**\n","\\begin{equation}\n","\\begin{split}\n","\\pi(8,\\text{DET},\\text{NOUN}) &= \\max_{w\\in S_{6}} \\big(\\pi(7, w, \\text{DET}) \\times q(\\text{NOUN}|w,\\text{DET}) \\times e(\\text{pasta}|\\text{NOUN})\\big)\\\\ \n","&= \\pi(7, \\text{VERB}, \\text{DET}) \\times q(\\text{NOUN}|\\text{VERB},\\text{DET}) \\times e(\\text{pasta}|\\text{NOUN})\\\\\n","&= 0.01 \\times 0.3 \\times 0.6 \\\\\n","&= 0.0018\n","\\end{split}\n","\\end{equation}\n"]},{"cell_type":"markdown","metadata":{"id":"oiwJb_vmkKLZ"},"source":["### Pregunta 2 (0.5 pts)\n","Comente  sobre las similitudes o diferencias entre los HMMs, MEMMs y CRFs. Para esto, responda las siguientes preguntas.\n","\n","#### 2.1. ¬øPara qu√© tipo de tarea sirven? D√© dos ejemplo de este tipo de tarea y descr√≠balos brevemente. (0.1 pts)\n","\n","**Respuesta:** Escriba su respuesta aqu√≠\n","\n","#### 2.2. ¬øQu√© modelos usan features? ¬øQu√© ventajas conlleva esto? (0.1 pts)\n","\n","**Respuesta:** Escriba su respuesta aqu√≠\n","\n","#### 2.3. ¬øC√≥mo maneja cada uno de los modelos las palabras con baja frecuencia en el set de train? (0.1 pts)\n","\n","**Respuesta:** Escriba su respuesta aqu√≠\n","\n","#### 2.4. ¬øQu√© le permite a los CRF realizar decisiones globales? ¬øQu√© diferencia con respecto a los MEMMs permite lograr esto? ¬øPor qu√© los HMMs tampoco son capaces de tomar decisiones globales? (0.1 pts)\n","\n","**Respuesta:** Escriba su respuesta aqu√≠\n","\n","#### 2.5 Dado una secuencia de $x_1, ..., x_m$ ¬øCu√°ntas posibles secuencias de etiquetas se pueden generar para un conjunto de etiquetas $S$ con $|S|=k$ ? ¬øAnalizarlas todas ser√≠a computacionalmente tratable? (0.1 pts)\n","\n","**Respuesta:** Escriba su respuesta aqu√≠"]},{"cell_type":"markdown","metadata":{"id":"P9h5ow8OWF7y"},"source":["**Respuestas**\n","#### Problemas para los que sirven:\n","Problemas que producen como salida una secuencia de etiquetas, a partir de una secuencia de entrada. Estos problemas con conocidos como *sequence labeling* y dos ejemplos de problemas de este tipo son:\n","- Named Entity Recognition (NER): Tiene como objetivo localizar y clasificar en categor√≠as predefinidas, como personas, organizaciones, lugares, expresiones de tiempo y cantidades, las entidades encontradas en el texto de input.\n","- Part-of-Speech (POS) Tagging: El objetivo es generar un secuencia de etiquetas que representan la categor√≠a gramatical de cada una de las palabras del texto de input.\n","\n","#### Modelos que usan features, ventajas:\n","Los modelos que usan features son MEMMs y CRF. Son modelos discriminativos mientras que los HMMs son modelos generativos. Los modelos discriminativos modelan directamente la probabilidad condicional utilizando un modelo parametrizado, minetras que los generativos utilizan solo la probabilidad conjunta.\n","\n","#### Manejo de las palabras con baja frecuencia:\n","En lose HMMs se reemplazan las palabras poco frecuentes por categor√≠as. Para esto, se divide el vocabulario en dos subconjuntos mediante un umbral de frecuencia: uno con las palabras frecuentes y otro con las poco frecuentes. Luego se convierten las poco frecuentes a un conjunto fijo de categor√≠as, por ejemplo, las fechas, los numeros y los nombres.\n","En la fase de testing, las palabras desconocidas las podemos llevar a estas categorias en vez de solo usar el token **unk**.\n","Esto trae como ventaja una mejora en las propiedades de generalizacion del modelo.\n","\n","En los MEMMs lo que tenemos son funciones parametrizadas para los cuales se aprenden pesos. Dado esto, el input se trata como un vector de caracteristicas (features). \n","Los inputs de los MEMMs incluyen la etiqueta. Esto tiene como ventaja que perimite codificar relaciones m√°s directas entre la etiqueta y el input. \n","Estas relaciones se determinan mediante el uso de $\\phi(\\cdot, \\cdot)$\n","\n","#### Decisiones globales de los CRFs, diferencias con respecto a los HMMs y MEMMs:\n","En los CRFs la normalizaci√≥n es global, mientras que en los MEMMs la normalizaci√≥n en local. Esto hace que entrenar una MEMMs es menos complicado que entrenar los CRFs.\n","\n","La gran ventaja de CRFs sobre MEMMs es que puede resolver un problema que se llama label bias. Este problema se da porque para algunas configuraciones de las etiquetas las MEMMs pueden ignorar informacion importante del contexto al realizar decisiones independientes en cada paso. Esto puede probocar una asignaci√≥n erronea de la etiqueta."]},{"cell_type":"markdown","metadata":{"id":"ClRAHR95Y8aB"},"source":["# Convolutional Neural Networks\n","### Pregunta 3 (1 pt)\n","\n","Considere la frase $w_{1..7}=$ `El agua moja y el fuego quema` $=[El, agua, moja, y, el, fuego, quema]$.\n","\n","La siguiente matriz de embeddings, donde la i-√©sima fila corresponde al vector de embedding de la i-√©sima palabra, ordenadas seg√∫n aparecen en la frase. (vectores de largo 2).\n","\\begin{equation}\n","E = \\begin{pmatrix}\n","2 & 2\\\\\n","0 & -2\\\\\n","0 & 1\\\\\n","-2 & 1\\\\\n","1 & 0\\\\\n","-1 & 1\\\\\n","1 & 1\n","\\end{pmatrix}\n","\\end{equation}\n","\n","Los siguientes 3 filtros\n","\\begin{equation}\n","U = \\begin{pmatrix}\n","-1 & 1 & 0\\\\\n","1 & 1 & 0\\\\\n","0 & 0 & -1\\\\\n","1 & -1 & -1\\\\\n","-1 & -1 & 1\\\\\n","1 & 0 & -1\n","\\end{pmatrix}\n","\\end{equation}\n","\n","Y la funci√≥n de activaci√≥n\n","\\begin{equation}\n","tanh = \\frac{e^{2x} - 1}{e^{2x} + 1}\n","\\end{equation}\n","\n","Usando estos param√°tros escriba los pasos para calcular la representaci√≥n (vector) resultante de aplicar la operaci√≥n de convoluci√≥n (sin padding) + max pooling. ¬øDe qu√© tama√±o ser√≠a la ventana que debemos usar?"]},{"cell_type":"markdown","metadata":{"id":"SlQ30Arkq0u4"},"source":["**Respuesta**\n","Dado que la matriz de filtro tiene 6 filas y los embedigs son vectores de largo 2, concluimos que la ventana es de tama√±o 3. Por tanto estamos usando n-gramas de 3 palabras.\n","\n","\\begin{equation}\n","\\begin{split}\n","w_{1..3} &=[El, agua, moja] &= \\big(2,2,0,-2,0,1\\big)\\\\\n","w_{2..4} &=[agua, moja, y]  &= \\big(0,-2,0,1,-2,1\\big)\\\\\n","w_{3..5} &=[moja, y, el]    &= \\big(0,1,-2,1,1,0\\big)\\\\\n","w_{4..6} &=[y, el, fuego]   &= \\big(-2,1,1,0,-1,1\\big)\\\\\n","w_{5..7} &=[el, fuego, quema] &= \\big(1,0,-1,1,1,1\\big)\\\\\n","\\end{split}\n","\\end{equation}\n","\n","\\\n","\n","\\begin{equation}\n","\\begin{split}\n","[[-1  6  1]\n"," [ 2 -1 -4]\n"," [ 1 -1  2]\n"," [ 5  0 -3]\n"," [ 0 -1  0]]\n","u_{1..3} &=\\text{tanh}(w_{1..3} \\cdot U) &= \\text{tanh}\\big((-1,6,1)\\big)&=(-0.76159416&,  0.99998771&,  0.76159416&)\\\\\n","u_{2..4} &=\\text{tanh}(w_{2..4} \\cdot U) &= \\text{tanh}\\big((1,-1,-4)\\big)&=(0.96402758&, -0.76159416&, -0.9993293&)\\\\\n","u_{3..5} &=\\text{tanh}(w_{3..5} \\cdot U) &= \\text{tanh}\\big((1,-1,2)\\big)&=(0.76159416&, -0.76159416&,  0.96402758&)\\\\\n","u_{4..6} &=\\text{tanh}(w_{4..6} \\cdot U) &= \\text{tanh}\\big((5,0,-3)\\big)&=(0.9999092&,   0.0&,         -0.99505475&)\\\\\n","u_{5..7} &=\\text{tanh}(w_{5..7} \\cdot U) &= \\text{tanh}\\big((0,-1,0)\\big)&=(0.0&,         -0.76159416&,  0.0        &)\\\\\n","\\end{split}\n","\\end{equation}\n","\n","\\\n","\n","\\begin{equation}\n","\\begin{split}\n","Y &=\\text{max-pool}\\big((u_{1..3},u_{2..4},u_{3..5},u_{4..6},u_{5..7})\\big) \\\\\n","  &=(0.9999092,  0.99998771, 0.96402758)\\\\\n","\\end{split}\n","\\end{equation}"]},{"cell_type":"code","metadata":{"id":"m63_M8MQFLDB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626814284408,"user_tz":240,"elapsed":141,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"cef9d622-2a89-4605-ae95-3088b4667df0"},"source":["import numpy as np\n","\n","U = np.array([[-1, 1, 0],\n","              [1,1,0],\n","              [0,0,-1],\n","              [1,-1,-1],\n","              [-1,-1,1],\n","              [1,0,-1]])\n","W = np.array([[2,2,0,-2,0,1],\n","              [0,-2,0,1,-2,1],\n","              [0,1,-2,1,1,0],\n","              [-2,1,1,0,-1,1],\n","              [1,0,-1,1,1,1]])\n","\n","H = np.matmul(W, U)\n","H1 = np.tanh(H)\n","\n","print(H)\n","print(H1)\n","print('result: ', np.max(H1, axis=0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[-1  6  1]\n"," [ 2 -1 -4]\n"," [ 1 -1  2]\n"," [ 5  0 -3]\n"," [ 0 -1  0]]\n","[[-0.76159416  0.99998771  0.76159416]\n"," [ 0.96402758 -0.76159416 -0.9993293 ]\n"," [ 0.76159416 -0.76159416  0.96402758]\n"," [ 0.9999092   0.         -0.99505475]\n"," [ 0.         -0.76159416  0.        ]]\n","result:  [0.9999092  0.99998771 0.96402758]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U0et78Z4oKIq"},"source":["# Recurrent Neural Networks\n","### Pregunta 4 (1 pt)\n","Usando los embeddings de dos dimensiones de la pregunta anteror, la oraci√≥n `el fuego quema` la podemos representar por una secuencia de vectores $(\\vec{x}_1,\\vec{x}_2,\\vec{x}_3)$, con $\\vec{x}_i \\in \\mathbb{R}^{d_x}$ y $d_x=2$.\n","\n","Tenemos una red recurrente *Elman* definidad como: \n","\\begin{equation}\n","\\begin{split}\n","\\vec{s}_i &= R_{SRNN}\\left (\\vec{x}_i, \\vec{s}_{i-1}\\right ) = g \\left (\\vec{s}_{i-1}W^s + \\vec{x}_i W^x + \\vec{b}\\right ) \\\\\n","\\vec{y}_i &= O_{SRNN}\\left(\\vec{s}_i\\right) = \\vec{s}_i \\\\\n","\\end{split}\n","\\end{equation}\n","donde\n","\\begin{equation}\n","\\vec{s}_i, \\vec{y}_i \\in \\mathbb{R}^{d_s}, \\quad W^x \\in \\mathbb{R}^{d_x \\times d_s}, \\quad W^s \\in \\mathbb{R}^{d_s \\times d_s}, \\quad \\vec{b} \\in \\mathbb{R}^{d_s},\n","\\end{equation}\n","y los vectores de estado $s_i$ son de tres dimensiones, $ds= 3$.\n","\n","Sea\n","\\begin{equation}\n","\\begin{split}\n","\\vec{s}_0 &= [0,0,0]\\\\\n","W^x &= \\begin{pmatrix}\n","0 &  0 & 1\\\\\n","1 & -1 & 0\n","\\end{pmatrix} \\\\\n","W^s &= \\begin{pmatrix}\n","1 & 0 &  1\\\\\n","0 & 1 & -1\\\\\n","1 & 1 &  1\n","\\end{pmatrix} \\\\\n","\\vec{b} &= [0, 0, 0] \\\\\n","g(x) &= ReLu(x) = max(0, x)\n","\\end{split}\n","\\end{equation}\n","\n","<br>\n","\n","Calcule manualmente los valores de los vectores $\\vec{s}_1, \\vec{s}_2,\\vec{s}_3$ y de $\\vec{y}_1, \\vec{y}_2,\\vec{y}_3$."]},{"cell_type":"markdown","metadata":{"id":"fim2W8JioPhL"},"source":["**Respuesta**\n","\n","\\begin{split}\n","    \\vec{s}_1 &= \\text{ReLu}(\\vec{s_0}W^s + \\vec{x_0}W^x+\\vec{b}) \\\\\n","              &= \\text{ReLu}([0,0,0]\n","              \\begin{pmatrix}\n","                1 & 0 &  1\\\\\n","                0 & 1 & -1\\\\\n","                1 & 1 &  1\n","              \\end{pmatrix}   + [1,0]\n","              \\begin{pmatrix}\n","                0 &  0 & 1\\\\\n","                1 & -1 & 0\n","              \\end{pmatrix} + [0,0,0]) \\\\\n","              &= \\text{ReLu}([0,0,1]) \\\\\n","              &= [0,0,1]\n","\\end{split}\n","\n","\\\n","\n","\\begin{split}\n","    \\vec{s}_2 &= \\text{ReLu}(\\vec{s_1}W^s + \\vec{x_2}W^x+\\vec{b}) \\\\\n","              &= \\text{ReLu}([0,0,1]\n","              \\begin{pmatrix}\n","                1 & 0 &  1\\\\\n","                0 & 1 & -1\\\\\n","                1 & 1 &  1\n","              \\end{pmatrix}   + [-1,1]\n","              \\begin{pmatrix}\n","                0 &  0 & 1\\\\\n","                1 & -1 & 0\n","              \\end{pmatrix}+[0,0,0]) \\\\ \n","              &= \\text{ReLu}([2,0,0]) \\\\\n","              &= [2,0,0] \\\\\n","\\end{split}\n","\n","\\\n","\n","\\begin{split}\n","    \\vec{s}_3 &= \\text{ReLu}(\\vec{s_2}W^s + \\vec{x_3}W^x+\\vec{b}) \\\\\n","              &= \\text{ReLu}([2,0,0]\n","              \\begin{pmatrix}\n","                1 & 0 &  1\\\\\n","                0 & 1 & -1\\\\\n","                1 & 1 &  1\n","              \\end{pmatrix}   + [1,1]\n","              \\begin{pmatrix}\n","                0 &  0 & 1\\\\\n","                1 & -1 & 0\n","              \\end{pmatrix}+[0,0,0])  \\\\\n","              &= \\text{ReLu}([3,-1,3]) \\\\\n","              &= [3,0,3]\n","\\end{split}"]},{"cell_type":"code","metadata":{"id":"EfcOlD0IOdgJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624457041388,"user_tz":240,"elapsed":6,"user":{"displayName":"Jes√∫s P√©rez","photoUrl":"","userId":"03422465056396107574"}},"outputId":"f7d0cc14-b971-404e-b2e9-cc8b1f7e9d67"},"source":["import numpy as np\n","x  = np.array([[1,0], [-1,1], [1,1]])\n","s = np.array([0,0,0])\n","Ws = np.array([[1,0,1],[0,1,-1],[1,1,1]])\n","Wx = np.array([[0,0,1],[1,-1,0]])\n","b = np.array([0,0,0])\n","for xi in x: \n","  print('\\nstep for: ', xi)\n","  print(np.matmul(s,Ws))\n","  print(np.matmul(xi,Wx))\n","  s = np.matmul(s,Ws) + np.matmul(xi,Wx) + b\n","  print(s)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","step for:  [1 0]\n","[0 0 0]\n","[0 0 1]\n","[0 0 1]\n","\n","step for:  [-1  1]\n","[1 1 1]\n","[ 1 -1 -1]\n","[2 0 0]\n","\n","step for:  [1 1]\n","[2 0 2]\n","[ 1 -1  1]\n","[ 3 -1  3]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W4rAT6ELxRZW"},"source":["### Pregunta 5 (0.5 pts)\n","¬øDe qu√© forma las RNN y las CNN logran aprender representaciones espec√≠ficas\n","para la tarea objetivo? Compare la forma en que las RNN y las CNN aprenden con los modelos que usan *features* dise√±adas manualmente."]},{"cell_type":"markdown","metadata":{"id":"b6AXbQSgA_t8"},"source":["**Respuesta**\n","\n","Las RNN preservan la estructura a lo largo de la sequencia, codificando los vectores de la secuencia (embeddings) en un solo vector. En el caso de las tareas de sequence labeling, podemos usar los vectores de salida temporales (para cada uno de las posibles subsecuencias prefijo del texto), pas√°ndolos por un modelo que los clasifique seg√∫n el label.\n","\n","Las RNN y las CNN no necesitan que se dise√±en los features manualmente. Ellas son capaces de aprender los features directamente a partir de las propiedades estad√≠sticas de la secuencia."]},{"cell_type":"markdown","metadata":{"id":"qxQIuO8axTUa"},"source":["# Redes neuronales con PyTorch\n","### Pregunta 6 (2 pts)\n","En esta parte van a tener que implementar una red neuronal Feed Forward. Adem√°s, deber√°n entrenar el modelo usando uno de los datasets de TorchText. En la secci√≥n de la respuesta hay un esqueleto de lo que deben hacer, deber√°n completar los metodos del modelo e implementar la parte asociada al entrenamiento. Como les mencionamos en la [Auxiliar 3](https://www.youtube.com/watch?v=r1pt9v_vuy8&list=PLwITEwTGVF6jXRu6l55W4OedqDiLgU9OM&index=2&t=5s), el proceso de entrenamiento es bastante est√°ndar, as√≠ que se pueden guiar en gran medida por los ejemplos que ah√≠ mostramos y los que vamos a ver en las pr√≥ximas auxiliares.\n","\n","#### 6.1 Capa Convolucional (Opcional)\n","Agregue a la arquitectura una capa convolucional. Para esto puede registrar el parametro $U$ en la red y realizar el computo de la convoluci√≥n en el metodo forward de la red, o puede usar la clase [`torch.nn.Conv1d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#conv1d) de `torch`.\n"]},{"cell_type":"code","metadata":{"id":"LVKEaQXZ3eGl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626798465840,"user_tz":240,"elapsed":300280,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"84453e0c-ad67-43a2-d857-7f5976f3ce49"},"source":["# %%capture\n","# Nos aseguramos que torchtext este en la ultima version\n","!pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install torchtext==0.9.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.8.0+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 834.1 MB 1.4 MB/s eta 0:14:08tcmalloc: large alloc 1147494400 bytes == 0x5620cc43c000 @  0x7f7e6b107615 0x56209254c02c 0x56209262c17a 0x56209254ee4d 0x562092640c0d 0x5620925c30d8 0x5620925bdc35 0x56209255073a 0x5620925c2f40 0x5620925bdc35 0x56209255073a 0x5620925bf93b 0x562092641a56 0x5620925befb3 0x562092641a56 0x5620925befb3 0x562092641a56 0x5620925befb3 0x562092550b99 0x562092593e79 0x56209254f7b2 0x5620925c2e65 0x5620925bdc35 0x56209255073a 0x5620925bf93b 0x5620925bdc35 0x56209255073a 0x5620925beb0e 0x56209255065a 0x5620925bed67 0x5620925bdc35\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 1055.7 MB 1.2 MB/s eta 0:12:25tcmalloc: large alloc 1434370048 bytes == 0x562110a92000 @  0x7f7e6b107615 0x56209254c02c 0x56209262c17a 0x56209254ee4d 0x562092640c0d 0x5620925c30d8 0x5620925bdc35 0x56209255073a 0x5620925c2f40 0x5620925bdc35 0x56209255073a 0x5620925bf93b 0x562092641a56 0x5620925befb3 0x562092641a56 0x5620925befb3 0x562092641a56 0x5620925befb3 0x562092550b99 0x562092593e79 0x56209254f7b2 0x5620925c2e65 0x5620925bdc35 0x56209255073a 0x5620925bf93b 0x5620925bdc35 0x56209255073a 0x5620925beb0e 0x56209255065a 0x5620925bed67 0x5620925bdc35\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 1336.2 MB 1.3 MB/s eta 0:08:05tcmalloc: large alloc 1792966656 bytes == 0x5620958c4000 @  0x7f7e6b107615 0x56209254c02c 0x56209262c17a 0x56209254ee4d 0x562092640c0d 0x5620925c30d8 0x5620925bdc35 0x56209255073a 0x5620925c2f40 0x5620925bdc35 0x56209255073a 0x5620925bf93b 0x562092641a56 0x5620925befb3 0x562092641a56 0x5620925befb3 0x562092641a56 0x5620925befb3 0x562092550b99 0x562092593e79 0x56209254f7b2 0x5620925c2e65 0x5620925bdc35 0x56209255073a 0x5620925bf93b 0x5620925bdc35 0x56209255073a 0x5620925beb0e 0x56209255065a 0x5620925bed67 0x5620925bdc35\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1691.1 MB 1.3 MB/s eta 0:03:53tcmalloc: large alloc 2241208320 bytes == 0x5621006ac000 @  0x7f7e6b107615 0x56209254c02c 0x56209262c17a 0x56209254ee4d 0x562092640c0d 0x5620925c30d8 0x5620925bdc35 0x56209255073a 0x5620925c2f40 0x5620925bdc35 0x56209255073a 0x5620925bf93b 0x562092641a56 0x5620925befb3 0x562092641a56 0x5620925befb3 0x562092641a56 0x5620925befb3 0x562092550b99 0x562092593e79 0x56209254f7b2 0x5620925c2e65 0x5620925bdc35 0x56209255073a 0x5620925bf93b 0x5620925bdc35 0x56209255073a 0x5620925beb0e 0x56209255065a 0x5620925bed67 0x5620925bdc35\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1982.2 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1982251008 bytes == 0x56218600e000 @  0x7f7e6b1061e7 0x562092581ae7 0x56209254c02c 0x56209262c17a 0x56209254ee4d 0x562092640c0d 0x5620925c30d8 0x5620925bdc35 0x56209255073a 0x5620925bed67 0x5620925bdc35 0x56209255073a 0x5620925bed67 0x5620925bdc35 0x56209255073a 0x5620925bed67 0x5620925bdc35 0x56209255073a 0x5620925bed67 0x5620925bdc35 0x56209255073a 0x5620925bed67 0x56209255065a 0x5620925bed67 0x5620925bdc35 0x56209255073a 0x5620925bf93b 0x5620925bdc35 0x56209255073a 0x5620925bf93b 0x5620925bdc35\n","tcmalloc: large alloc 2477817856 bytes == 0x5621fc27a000 @  0x7f7e6b107615 0x56209254c02c 0x56209262c17a 0x56209254ee4d 0x562092640c0d 0x5620925c30d8 0x5620925bdc35 0x56209255073a 0x5620925bed67 0x5620925bdc35 0x56209255073a 0x5620925bed67 0x5620925bdc35 0x56209255073a 0x5620925bed67 0x5620925bdc35 0x56209255073a 0x5620925bed67 0x5620925bdc35 0x56209255073a 0x5620925bed67 0x56209255065a 0x5620925bed67 0x5620925bdc35 0x56209255073a 0x5620925bf93b 0x5620925bdc35 0x56209255073a 0x5620925bf93b 0x5620925bdc35 0x562092550dd1\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1982.2 MB 2.2 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (1.19.5)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.9.0+cu102\n","    Uninstalling torch-1.9.0+cu102:\n","      Successfully uninstalled torch-1.9.0+cu102\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu111 which is incompatible.\n","torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.0+cu111 which is incompatible.\u001b[0m\n","Successfully installed torch-1.8.0+cu111\n","Collecting torchtext==0.9.0\n","  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.1 MB 7.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (1.19.5)\n","Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (1.8.0+cu111)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->torchtext==0.9.0) (3.7.4.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n","Installing collected packages: torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.10.0\n","    Uninstalling torchtext-0.10.0:\n","      Successfully uninstalled torchtext-0.10.0\n","Successfully installed torchtext-0.9.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dJ-wrzFO5mCC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626798476300,"user_tz":240,"elapsed":10465,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"1eef1c14-cffc-453a-9269-698e33abea75"},"source":["# Trabajaremos con el dataset AG_NEWS de torchtext\n","# https://pytorch.org/text/stable/datasets.html#ag-news\n","import os\n","import torch\n","from random import choice\n","from torchtext.datasets import AG_NEWS\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","os.makedirs(\"data\", exist_ok=True)\n","train_dataset, test_dataset = AG_NEWS(root=\"data\", split=('train', 'test'))\n","train_list = list(train_dataset)\n","test_list = list(test_dataset)\n","\n","# Informacion relevante del dataset\n","tokenizer = get_tokenizer(\"basic_english\")\n","vocab = build_vocab_from_iterator(tokenizer(x[1]) for x in train_list)\n","num_classes = 4"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train.csv: 29.5MB [00:00, 90.7MB/s]\n","test.csv: 1.86MB [00:00, 26.0MB/s]                  \n","120000lines [00:03, 33396.20lines/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"IXngUm9HxKvA"},"source":["# De aca para abajo viene su respuesta, completen las funciones en la red\n","# y luego entrenen el modelo y evaluenlo usando los dataset que acaban de\n","# cargar\n","import torch\n","import torch.nn as nn\n","from itertools import zip_longest\n","\n","class CNNClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim=32, num_classes=10, \n","                 use_cnn=False, cnn_pool_channels=24, cnn_kernel_size=3):\n","      super().__init__()\n","      self.use_cnn = use_cnn\n","      \n","      if use_cnn:\n","        # capa de embedding\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","\n","        # capa de convoluci√≥n\n","        self.conv = nn.Conv1d(\n","            in_channels=1,\n","            out_channels=cnn_pool_channels,\n","            kernel_size=cnn_kernel_size * embed_dim,\n","            stride=embed_dim,\n","        )\n","\n","        fc_in_size = cnn_pool_channels\n","      else:\n","        # capa de embedding, en este caso usamos EmbeddingBag \n","        # por lo que necesitaremos los offsets del batch en el forwards\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n","\n","        fc_in_size = embed_dim\n","\n","      # capa lineal\n","      self.fc = nn.Linear(fc_in_size, num_classes)\n","\n","      self.init_weights()\n","\n","    def init_weights(self):\n","      initrange = 0.5\n","      self.embedding.weight.data.uniform_(-initrange, initrange)\n","      self.fc.weight.data.uniform_(-initrange, initrange)\n","      self.fc.bias.data.zero_()\n","\n","    def forward(self, text, offsets):\n","      if self.use_cnn:\n","        # preparamos el input de la capa de embeddings a partir de text y offsets\n","        # (N x longest_text)\n","        text = torch.tensor(\n","            list(\n","                zip(\n","                    *zip_longest(\n","                        *([text[o:offsets[i+1]] for i, o in enumerate(offsets[:-1])] + [text[offsets[-1]:len(texts)]]), \n","                        fillvalue=vocab[\"<pad>\"]\n","                    )\n","                )\n","            )\n","        ).to(text.device)\n","\n","        # (N x longest_text x embed_dim)\n","        h = self.embedding(text)\n","\n","        # (N x pool_channels)\n","        h = h.view(h.size(0), 1, -1)\n","        h = torch.relu(self.conv(h))\n","        h = h.mean(dim=2)\n","      else:\n","        # (N x embed_dim)\n","        h = self.embedding(text, offsets)\n","\n","      # (N x num_classes)\n","      return self.fc(h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPHyqgId96ra","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626806891910,"user_tz":240,"elapsed":7509286,"user":{"displayName":"Jesus Perez-Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrUM8PO172vMI675unxlqAs-_g-U_LvdtWc9_taw=s64","userId":"14968406805419001169"}},"outputId":"ae9983c2-ecd8-4e3e-fd5b-9c40eedde1f9"},"source":["# El resto de su respuesta\n","# Aca deben programar el entrenamiento de la red\n","\n","import sys\n","from torch.optim import SGD, lr_scheduler\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","\n","def generate_batch(batch):\n","  label = torch.tensor([entry[0]-1 for entry in batch])\n","  texts = [tokenizer(entry[1]) for entry in batch]\n","  offsets = [0] + [len(text) for text in texts]\n","  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","  big_text = torch.cat([torch.tensor([vocab.stoi[t] for t in text]) for text in texts])\n","  return big_text, offsets, label\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","BATCH_SIZE = 16\n","TEST_BATCH_SIZE = BATCH_SIZE * 5\n","LR = 1e-1\n","\n","model = CNNClassifier(len(vocab), num_classes=num_classes, use_cnn=True).to(device)\n","optimizer = SGD(model.parameters(), lr=LR)\n","criterion = nn.CrossEntropyLoss().to(device)\n","scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda epoch: .9 ** (epoch // 10)])\n","\n","split_size = {'train': len(train_list), 'test': len(test_list)}\n","\n","# train_dataset, test_dataset = AG_NEWS(root=\"data\")\n","for epoch in range(1, 80):\n","  train_loader = DataLoader(train_list, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n","  test_loader = DataLoader(test_list, batch_size=TEST_BATCH_SIZE, collate_fn=generate_batch)\n","  loaders = {'train': train_loader, 'test': test_loader}\n","  for phase in ['train', 'test']:\n","    if phase == 'train':\n","      model.train()\n","    else:\n","      model.eval()\n","\n","    total_acc, total_loss = 0, 0\n","    for i, (texts, offsets, cls) in enumerate(loaders[phase]):\n","      texts = texts.to(device)\n","      offsets = offsets.to(device)\n","      cls = cls.to(device)\n","\n","      optimizer.zero_grad()\n","      with torch.set_grad_enabled(phase == 'train'):\n","        output = model(texts, offsets)\n","        loss = criterion(output, cls)\n","        total_loss += loss.item()\n","        if phase == 'train':\n","          loss.backward()\n","          optimizer.step()\n","      \n","      acc = (output.argmax(1) == cls).sum().item()\n","      total_acc += acc\n","\n","      sys.stdout.write('\\rEpoch: {0:03d}\\t Phase: {1} Iter: {2:03d}/{3:03d}\\t iter-Acc: {4:.3f}%\\t iter-Loss: {5:.3f}'.format(epoch, phase, i, len(loaders[phase]), acc/len(offsets)*100, loss.item()))\n","\n","    if phase == 'train':\n","      scheduler.step()\n","    print('\\n {0}\\tAvg. Acc: {1:.3f}%\\t Avg. Loss: {2:.3f}'.format(phase, total_acc/split_size[phase]*100, total_loss/split_size[phase]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n","Epoch: 001\t Phase: train Iter: 7499/7500\t iter-Acc: 93.750%\t iter-Loss: 0.225\n"," train\tAvg. Acc: 59.071%\t Avg. Loss: 0.059\n","Epoch: 001\t Phase: test Iter: 094/095\t iter-Acc: 81.250%\t iter-Loss: 0.439\n"," test\tAvg. Acc: 79.224%\t Avg. Loss: 0.007\n","Epoch: 002\t Phase: train Iter: 7499/7500\t iter-Acc: 93.750%\t iter-Loss: 0.147\n"," train\tAvg. Acc: 84.485%\t Avg. Loss: 0.028\n","Epoch: 002\t Phase: test Iter: 094/095\t iter-Acc: 88.750%\t iter-Loss: 0.309\n"," test\tAvg. Acc: 85.368%\t Avg. Loss: 0.006\n","Epoch: 003\t Phase: train Iter: 7499/7500\t iter-Acc: 93.750%\t iter-Loss: 0.111\n"," train\tAvg. Acc: 87.889%\t Avg. Loss: 0.023\n","Epoch: 003\t Phase: test Iter: 094/095\t iter-Acc: 91.250%\t iter-Loss: 0.279\n"," test\tAvg. Acc: 87.092%\t Avg. Loss: 0.005\n","Epoch: 004\t Phase: train Iter: 7499/7500\t iter-Acc: 93.750%\t iter-Loss: 0.088\n"," train\tAvg. Acc: 89.330%\t Avg. Loss: 0.020\n","Epoch: 004\t Phase: test Iter: 094/095\t iter-Acc: 91.250%\t iter-Loss: 0.268\n"," test\tAvg. Acc: 88.303%\t Avg. Loss: 0.005\n","Epoch: 005\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.070\n"," train\tAvg. Acc: 90.148%\t Avg. Loss: 0.018\n","Epoch: 005\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.263\n"," test\tAvg. Acc: 88.605%\t Avg. Loss: 0.004\n","Epoch: 006\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.056\n"," train\tAvg. Acc: 90.776%\t Avg. Loss: 0.017\n","Epoch: 006\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.259\n"," test\tAvg. Acc: 88.961%\t Avg. Loss: 0.004\n","Epoch: 007\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.048\n"," train\tAvg. Acc: 91.332%\t Avg. Loss: 0.016\n","Epoch: 007\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.256\n"," test\tAvg. Acc: 89.276%\t Avg. Loss: 0.004\n","Epoch: 008\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.040\n"," train\tAvg. Acc: 91.752%\t Avg. Loss: 0.015\n","Epoch: 008\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.253\n"," test\tAvg. Acc: 89.513%\t Avg. Loss: 0.004\n","Epoch: 009\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.035\n"," train\tAvg. Acc: 92.165%\t Avg. Loss: 0.015\n","Epoch: 009\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.250\n"," test\tAvg. Acc: 89.579%\t Avg. Loss: 0.004\n","Epoch: 010\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.029\n"," train\tAvg. Acc: 92.529%\t Avg. Loss: 0.014\n","Epoch: 010\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.248\n"," test\tAvg. Acc: 89.711%\t Avg. Loss: 0.004\n","Epoch: 011\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.025\n"," train\tAvg. Acc: 92.888%\t Avg. Loss: 0.013\n","Epoch: 011\t Phase: test Iter: 094/095\t iter-Acc: 96.250%\t iter-Loss: 0.248\n"," test\tAvg. Acc: 89.803%\t Avg. Loss: 0.004\n","Epoch: 012\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.021\n"," train\tAvg. Acc: 93.143%\t Avg. Loss: 0.013\n","Epoch: 012\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.248\n"," test\tAvg. Acc: 89.842%\t Avg. Loss: 0.004\n","Epoch: 013\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.019\n"," train\tAvg. Acc: 93.403%\t Avg. Loss: 0.012\n","Epoch: 013\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.248\n"," test\tAvg. Acc: 89.934%\t Avg. Loss: 0.004\n","Epoch: 014\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.016\n"," train\tAvg. Acc: 93.683%\t Avg. Loss: 0.012\n","Epoch: 014\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.250\n"," test\tAvg. Acc: 90.013%\t Avg. Loss: 0.004\n","Epoch: 015\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.014\n"," train\tAvg. Acc: 93.903%\t Avg. Loss: 0.012\n","Epoch: 015\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.251\n"," test\tAvg. Acc: 90.105%\t Avg. Loss: 0.004\n","Epoch: 016\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.012\n"," train\tAvg. Acc: 94.147%\t Avg. Loss: 0.011\n","Epoch: 016\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.253\n"," test\tAvg. Acc: 90.158%\t Avg. Loss: 0.004\n","Epoch: 017\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.010\n"," train\tAvg. Acc: 94.390%\t Avg. Loss: 0.011\n","Epoch: 017\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.255\n"," test\tAvg. Acc: 90.158%\t Avg. Loss: 0.004\n","Epoch: 018\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.009\n"," train\tAvg. Acc: 94.624%\t Avg. Loss: 0.010\n","Epoch: 018\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.256\n"," test\tAvg. Acc: 90.250%\t Avg. Loss: 0.004\n","Epoch: 019\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.007\n"," train\tAvg. Acc: 94.827%\t Avg. Loss: 0.010\n","Epoch: 019\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.257\n"," test\tAvg. Acc: 90.145%\t Avg. Loss: 0.004\n","Epoch: 020\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.006\n"," train\tAvg. Acc: 95.028%\t Avg. Loss: 0.010\n","Epoch: 020\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.258\n"," test\tAvg. Acc: 90.105%\t Avg. Loss: 0.004\n","Epoch: 021\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.005\n"," train\tAvg. Acc: 95.276%\t Avg. Loss: 0.009\n","Epoch: 021\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.260\n"," test\tAvg. Acc: 90.250%\t Avg. Loss: 0.004\n","Epoch: 022\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.005\n"," train\tAvg. Acc: 95.467%\t Avg. Loss: 0.009\n","Epoch: 022\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.263\n"," test\tAvg. Acc: 90.329%\t Avg. Loss: 0.004\n","Epoch: 023\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.004\n"," train\tAvg. Acc: 95.647%\t Avg. Loss: 0.009\n","Epoch: 023\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.265\n"," test\tAvg. Acc: 90.237%\t Avg. Loss: 0.004\n","Epoch: 024\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.003\n"," train\tAvg. Acc: 95.820%\t Avg. Loss: 0.008\n","Epoch: 024\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.268\n"," test\tAvg. Acc: 90.237%\t Avg. Loss: 0.004\n","Epoch: 025\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.003\n"," train\tAvg. Acc: 95.993%\t Avg. Loss: 0.008\n","Epoch: 025\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.269\n"," test\tAvg. Acc: 90.263%\t Avg. Loss: 0.004\n","Epoch: 026\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.002\n"," train\tAvg. Acc: 96.130%\t Avg. Loss: 0.008\n","Epoch: 026\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.271\n"," test\tAvg. Acc: 90.237%\t Avg. Loss: 0.004\n","Epoch: 027\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.002\n"," train\tAvg. Acc: 96.281%\t Avg. Loss: 0.007\n","Epoch: 027\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.274\n"," test\tAvg. Acc: 90.158%\t Avg. Loss: 0.004\n","Epoch: 028\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.002\n"," train\tAvg. Acc: 96.439%\t Avg. Loss: 0.007\n","Epoch: 028\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.276\n"," test\tAvg. Acc: 90.171%\t Avg. Loss: 0.004\n","Epoch: 029\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.002\n"," train\tAvg. Acc: 96.602%\t Avg. Loss: 0.007\n","Epoch: 029\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.278\n"," test\tAvg. Acc: 90.171%\t Avg. Loss: 0.004\n","Epoch: 030\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 96.750%\t Avg. Loss: 0.006\n","Epoch: 030\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.280\n"," test\tAvg. Acc: 90.132%\t Avg. Loss: 0.004\n","Epoch: 031\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 96.944%\t Avg. Loss: 0.006\n","Epoch: 031\t Phase: test Iter: 094/095\t iter-Acc: 96.250%\t iter-Loss: 0.284\n"," test\tAvg. Acc: 90.145%\t Avg. Loss: 0.004\n","Epoch: 032\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 97.080%\t Avg. Loss: 0.006\n","Epoch: 032\t Phase: test Iter: 094/095\t iter-Acc: 96.250%\t iter-Loss: 0.287\n"," test\tAvg. Acc: 90.092%\t Avg. Loss: 0.004\n","Epoch: 033\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 97.228%\t Avg. Loss: 0.006\n","Epoch: 033\t Phase: test Iter: 094/095\t iter-Acc: 96.250%\t iter-Loss: 0.289\n"," test\tAvg. Acc: 90.039%\t Avg. Loss: 0.004\n","Epoch: 034\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 97.357%\t Avg. Loss: 0.005\n","Epoch: 034\t Phase: test Iter: 094/095\t iter-Acc: 96.250%\t iter-Loss: 0.292\n"," test\tAvg. Acc: 90.026%\t Avg. Loss: 0.004\n","Epoch: 035\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 97.493%\t Avg. Loss: 0.005\n","Epoch: 035\t Phase: test Iter: 094/095\t iter-Acc: 96.250%\t iter-Loss: 0.294\n"," test\tAvg. Acc: 89.987%\t Avg. Loss: 0.004\n","Epoch: 036\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 97.597%\t Avg. Loss: 0.005\n","Epoch: 036\t Phase: test Iter: 094/095\t iter-Acc: 96.250%\t iter-Loss: 0.297\n"," test\tAvg. Acc: 89.974%\t Avg. Loss: 0.004\n","Epoch: 037\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 97.720%\t Avg. Loss: 0.005\n","Epoch: 037\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.300\n"," test\tAvg. Acc: 89.868%\t Avg. Loss: 0.004\n","Epoch: 038\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 97.840%\t Avg. Loss: 0.005\n","Epoch: 038\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.303\n"," test\tAvg. Acc: 89.763%\t Avg. Loss: 0.005\n","Epoch: 039\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 97.962%\t Avg. Loss: 0.004\n","Epoch: 039\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.307\n"," test\tAvg. Acc: 89.684%\t Avg. Loss: 0.005\n","Epoch: 040\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 98.072%\t Avg. Loss: 0.004\n","Epoch: 040\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.311\n"," test\tAvg. Acc: 89.645%\t Avg. Loss: 0.005\n","Epoch: 041\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 98.216%\t Avg. Loss: 0.004\n","Epoch: 041\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.317\n"," test\tAvg. Acc: 89.579%\t Avg. Loss: 0.005\n","Epoch: 042\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 98.311%\t Avg. Loss: 0.004\n","Epoch: 042\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.321\n"," test\tAvg. Acc: 89.526%\t Avg. Loss: 0.005\n","Epoch: 043\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 98.407%\t Avg. Loss: 0.004\n","Epoch: 043\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.327\n"," test\tAvg. Acc: 89.579%\t Avg. Loss: 0.005\n","Epoch: 044\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 98.493%\t Avg. Loss: 0.003\n","Epoch: 044\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.332\n"," test\tAvg. Acc: 89.632%\t Avg. Loss: 0.005\n","Epoch: 045\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 98.577%\t Avg. Loss: 0.003\n","Epoch: 045\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.337\n"," test\tAvg. Acc: 89.645%\t Avg. Loss: 0.005\n","Epoch: 046\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 98.639%\t Avg. Loss: 0.003\n","Epoch: 046\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.342\n"," test\tAvg. Acc: 89.632%\t Avg. Loss: 0.005\n","Epoch: 047\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 98.725%\t Avg. Loss: 0.003\n","Epoch: 047\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.347\n"," test\tAvg. Acc: 89.618%\t Avg. Loss: 0.005\n","Epoch: 048\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 98.809%\t Avg. Loss: 0.003\n","Epoch: 048\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.353\n"," test\tAvg. Acc: 89.526%\t Avg. Loss: 0.005\n","Epoch: 049\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 98.872%\t Avg. Loss: 0.003\n","Epoch: 049\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.359\n"," test\tAvg. Acc: 89.605%\t Avg. Loss: 0.005\n","Epoch: 050\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 98.938%\t Avg. Loss: 0.003\n","Epoch: 050\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.367\n"," test\tAvg. Acc: 89.632%\t Avg. Loss: 0.005\n","Epoch: 051\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.017%\t Avg. Loss: 0.002\n","Epoch: 051\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.378\n"," test\tAvg. Acc: 89.618%\t Avg. Loss: 0.006\n","Epoch: 052\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.072%\t Avg. Loss: 0.002\n","Epoch: 052\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.385\n"," test\tAvg. Acc: 89.605%\t Avg. Loss: 0.006\n","Epoch: 053\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.130%\t Avg. Loss: 0.002\n","Epoch: 053\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.392\n"," test\tAvg. Acc: 89.632%\t Avg. Loss: 0.006\n","Epoch: 054\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.184%\t Avg. Loss: 0.002\n","Epoch: 054\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.399\n"," test\tAvg. Acc: 89.592%\t Avg. Loss: 0.006\n","Epoch: 055\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.230%\t Avg. Loss: 0.002\n","Epoch: 055\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.406\n"," test\tAvg. Acc: 89.579%\t Avg. Loss: 0.006\n","Epoch: 056\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.268%\t Avg. Loss: 0.002\n","Epoch: 056\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.413\n"," test\tAvg. Acc: 89.566%\t Avg. Loss: 0.006\n","Epoch: 057\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.310%\t Avg. Loss: 0.002\n","Epoch: 057\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.421\n"," test\tAvg. Acc: 89.566%\t Avg. Loss: 0.006\n","Epoch: 058\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.331%\t Avg. Loss: 0.002\n","Epoch: 058\t Phase: test Iter: 094/095\t iter-Acc: 95.000%\t iter-Loss: 0.429\n"," test\tAvg. Acc: 89.526%\t Avg. Loss: 0.006\n","Epoch: 059\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.363%\t Avg. Loss: 0.002\n","Epoch: 059\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.437\n"," test\tAvg. Acc: 89.539%\t Avg. Loss: 0.006\n","Epoch: 060\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.392%\t Avg. Loss: 0.002\n","Epoch: 060\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.448\n"," test\tAvg. Acc: 89.579%\t Avg. Loss: 0.006\n","Epoch: 061\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.439%\t Avg. Loss: 0.002\n","Epoch: 061\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.460\n"," test\tAvg. Acc: 89.513%\t Avg. Loss: 0.006\n","Epoch: 062\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.468%\t Avg. Loss: 0.001\n","Epoch: 062\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.468\n"," test\tAvg. Acc: 89.474%\t Avg. Loss: 0.006\n","Epoch: 063\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.497%\t Avg. Loss: 0.001\n","Epoch: 063\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.476\n"," test\tAvg. Acc: 89.474%\t Avg. Loss: 0.006\n","Epoch: 064\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.520%\t Avg. Loss: 0.001\n","Epoch: 064\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.485\n"," test\tAvg. Acc: 89.500%\t Avg. Loss: 0.006\n","Epoch: 065\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.543%\t Avg. Loss: 0.001\n","Epoch: 065\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.493\n"," test\tAvg. Acc: 89.526%\t Avg. Loss: 0.007\n","Epoch: 066\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.558%\t Avg. Loss: 0.001\n","Epoch: 066\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.502\n"," test\tAvg. Acc: 89.461%\t Avg. Loss: 0.007\n","Epoch: 067\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.577%\t Avg. Loss: 0.001\n","Epoch: 067\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.510\n"," test\tAvg. Acc: 89.434%\t Avg. Loss: 0.007\n","Epoch: 068\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 99.589%\t Avg. Loss: 0.001\n","Epoch: 068\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.517\n"," test\tAvg. Acc: 89.447%\t Avg. Loss: 0.007\n","Epoch: 069\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 99.606%\t Avg. Loss: 0.001\n","Epoch: 069\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.525\n"," test\tAvg. Acc: 89.421%\t Avg. Loss: 0.007\n","Epoch: 070\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.000\n"," train\tAvg. Acc: 99.618%\t Avg. Loss: 0.001\n","Epoch: 070\t Phase: test Iter: 094/095\t iter-Acc: 93.750%\t iter-Loss: 0.534\n"," test\tAvg. Acc: 89.342%\t Avg. Loss: 0.007\n","Epoch: 071\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 99.640%\t Avg. Loss: 0.001\n","Epoch: 071\t Phase: test Iter: 094/095\t iter-Acc: 92.500%\t iter-Loss: 0.542\n"," test\tAvg. Acc: 89.316%\t Avg. Loss: 0.007\n","Epoch: 072\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 99.657%\t Avg. Loss: 0.001\n","Epoch: 072\t Phase: test Iter: 094/095\t iter-Acc: 92.500%\t iter-Loss: 0.550\n"," test\tAvg. Acc: 89.276%\t Avg. Loss: 0.007\n","Epoch: 073\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 99.678%\t Avg. Loss: 0.001\n","Epoch: 073\t Phase: test Iter: 094/095\t iter-Acc: 92.500%\t iter-Loss: 0.557\n"," test\tAvg. Acc: 89.276%\t Avg. Loss: 0.007\n","Epoch: 074\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 99.686%\t Avg. Loss: 0.001\n","Epoch: 074\t Phase: test Iter: 094/095\t iter-Acc: 92.500%\t iter-Loss: 0.564\n"," test\tAvg. Acc: 89.250%\t Avg. Loss: 0.007\n","Epoch: 075\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 99.693%\t Avg. Loss: 0.001\n","Epoch: 075\t Phase: test Iter: 094/095\t iter-Acc: 92.500%\t iter-Loss: 0.570\n"," test\tAvg. Acc: 89.263%\t Avg. Loss: 0.007\n","Epoch: 076\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 99.697%\t Avg. Loss: 0.001\n","Epoch: 076\t Phase: test Iter: 094/095\t iter-Acc: 92.500%\t iter-Loss: 0.578\n"," test\tAvg. Acc: 89.237%\t Avg. Loss: 0.007\n","Epoch: 077\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 99.706%\t Avg. Loss: 0.001\n","Epoch: 077\t Phase: test Iter: 094/095\t iter-Acc: 92.500%\t iter-Loss: 0.585\n"," test\tAvg. Acc: 89.237%\t Avg. Loss: 0.007\n","Epoch: 078\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 99.716%\t Avg. Loss: 0.001\n","Epoch: 078\t Phase: test Iter: 094/095\t iter-Acc: 92.500%\t iter-Loss: 0.589\n"," test\tAvg. Acc: 89.211%\t Avg. Loss: 0.007\n","Epoch: 079\t Phase: train Iter: 7499/7500\t iter-Acc: 100.000%\t iter-Loss: 0.001\n"," train\tAvg. Acc: 99.721%\t Avg. Loss: 0.001\n","Epoch: 079\t Phase: test Iter: 094/095\t iter-Acc: 92.500%\t iter-Loss: 0.597\n"," test\tAvg. Acc: 89.197%\t Avg. Loss: 0.007\n"],"name":"stdout"}]}]}