{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pauta_Tarea5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYExzNuwx-Ke"
      },
      "source": [
        "# **Tarea 5 - Transformers y BERT ðŸ“š**\n",
        "\n",
        "**Integrantes:**\n",
        "\n",
        "**Fecha lÃ­mite de entrega ðŸ“†:** Martes 06 de Julio.\n",
        "\n",
        "**Tiempo estimado de dedicaciÃ³n:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-19T18:30:18.109327Z",
          "start_time": "2020-03-19T18:30:18.103344Z"
        },
        "id": "q5CSRY4oNCHK"
      },
      "source": [
        "## Instrucciones\n",
        "\n",
        "- El ejercicio consiste en:\n",
        "    - Responder preguntas relativas a los contenidos vistos en los vÃ­deos y slides de las clases. \n",
        "    - Utilizar la librerÃ­a Transformers\n",
        "- La tarea se realiza en grupos de **mÃ¡ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a travÃ©s de u-cursos a mÃ¡s tardar el dÃ­a estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo Jupyter Notebook.\n",
        "- Al momento de la revisiÃ³n tu cÃ³digo serÃ¡ ejecutado. Por favor verifica que tu entrega no tenga errores de compilaciÃ³n. \n",
        "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a travÃ©s del canal de Discord del curso. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4wYf0vgnbTv"
      },
      "source": [
        "## **Preguntas teÃ³ricas ðŸ“• (3 puntos).** ##\n",
        "Para estas preguntas no es necesario implementar cÃ³digo, pero pueden utilizar pseudo cÃ³digo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcnN6ZlOzwLu"
      },
      "source": [
        "### **Parte 1: Arquitecturas de Redes Neuronales**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irsqBVmCnx3M"
      },
      "source": [
        "**Pregunta 1**: \n",
        "\n",
        "Explique el principal problema de las redes Elman recurrentes. Explique cada compuerta de las redes LSTM y la GRU.  **(0.75 puntos)**\n",
        "\n",
        "**Respuesta**:\n",
        "\n",
        "El problema de las redes Elman (o S-RNN) es el problema del vanishing gradient (o gradiente desvaneciente). Este problema se da porque los gradientes de las primeras capas tienden a 0 y puede hacer que estas capas no aprendan nada. **(0.25 ptos)**\n",
        "\n",
        "Para la LSTM: **(0.25 ptos)**\n",
        "Cada compuerta es una combinaciÃ³n lineal entre el input $\\vec x_j$ y el estado anterior $\\vec h_{j-1}$ pasados por una tanh.\n",
        "- $\\vec i$: Que cosas me importan del input\n",
        "- $\\vec f$: Que cosas olvido de $\\vec c_j$\n",
        "- $\\vec o$: Que es lo que me importa para producir un output\n",
        "\n",
        "Para la GRU: **(0.25 puntos)**\n",
        "- $\\vec r$: Se utiliza para controlar el acceso al estado previo y proponer un candidato $\\vec{\\tilde s_j}$\n",
        "- $\\vec z$: Genera una interpolaciÃ³n entre el estado anterior y el candidato de $\\vec r$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5FaWqBVvL90"
      },
      "source": [
        "**Pregunta 2**: \n",
        "\n",
        "Explique cuales son las diferencias entre las tres arquitecturas de sequence to sequence vistas en clases (Encoder-Decoder con RNN, Encoder-Decoder con RNN y Attention, y el Transformer) **(0.75 ptos)**\n",
        "\n",
        "**Respuesta**: \n",
        "\n",
        "Attention ayuda al Decoder a aprender a resaltar las partes importantes del Encoder. El Transformer permite una paralelizaciÃ³n superior a las RNN's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9q0SrZS8CqX"
      },
      "source": [
        "**Pregunta 3**: \n",
        "\n",
        "Â¿CÃºal es el principal diferencia entre los Embeddings contextualizados (por ejemplo BERT) vs. los Embeddings estÃ¡ticos (por ejemplo Word2Vec)? **(0.75 ptos)**\n",
        "\n",
        "**Respuesta**: \n",
        "\n",
        "Los embeddings contextualizados le dan una representaciÃ³n a cada palabra que depende del contexto en donde aparece. Una palabra tiene tantas representaciones como contextos. Las palabras tienen solo una representaciÃ³n en los Embeddings estaticos, la cual fue aprendida durante el entrenamiento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX0C2x5j8DNR"
      },
      "source": [
        "**Pregunta 4**: \n",
        "\n",
        "Explique en que tareas y las arquitecturas con las que se entrenan ELMO y BERT **(0.75 ptos)**\n",
        "\n",
        "**Respuesta**: \n",
        "- ELMO es un Language Model usando RNN.  \n",
        "- BERT hace masked language modeling (eliminan una palabra de una oraciÃ³n y debe adivinarla) y Next Sentence Prediction (Una oraciÃ³n A se le dan dos posibles opciones para continuar y BERT debe predecir cÃºal es, B o C, en la cual una de las dos es la continuaciÃ³n mÃ¡s factible) usando un Transformer Encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocS_vQhR1gcU"
      },
      "source": [
        "## **Preguntas prÃ¡cticas ðŸ’» (3 puntos).** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol82nJ0FnmcP"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W28UEwvGYTbg"
      },
      "source": [
        "Lo primero es instalar las librerÃ­as necesarias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X4Gbx7wYWDD"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertForNextSentencePrediction, BertForMaskedLM, BertForQuestionAnswering\n",
        "import torch"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIQo-VciYz2V"
      },
      "source": [
        "Para las preguntas que siguen, utilizaremos distintas variantes de BERT disponibles en la librerÃ­a transformers. [AquÃ­](https://huggingface.co/transformers/model_doc/bert.html) pueden encontrar toda la documentaciÃ³n necesaria. El modelo pre-entrenado a utilizar es \"bert-base-uncased\" (salvo para question answering)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4_WHbz8bXx2"
      },
      "source": [
        "BERT es un modelo de lenguaje que fue entrenado exhaustivamente sobre dos tareas: 1) Next sentence prediction. 2) Masked language modeling. Veremos ejemplos de esto en la Auxiliar del jueves 16/07/20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyMb4YZRMYkm"
      },
      "source": [
        "#### **BertForNextSentencePrediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt6CbBtyadRb"
      },
      "source": [
        "**Pregunta 1 (1 pto en total):**  Utilizando el modelo BertForNextSentencePrediction de la librerÃ­a transformers, muestre cual de las 2 oraciones es **mÃ¡s probable** que sea una continuaciÃ³n de la primera. Para esto defina la funciÃ³n $oracion\\_mas\\_probable$, que recibe el inicio de una frase, las alternativas para continuar esta frase y retorna un string indicando cual de las dos oraciones es mÃ¡s probable **(0.25 puntos cada una)**.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "Initial: \"The sky is blue.\"\\\n",
        "A: \"This is due to the shorter wavelength of blue light.\"\\\n",
        "B: \"Chile is one of the world's greatest economies.\"\n",
        "\n",
        "DeberÃ­a retornar \"La oraciÃ³n que continÃºa mÃ¡s probable es A\", justificÃ¡ndolo con la evaluaciÃ³n de BERT.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOX0bwser8OM"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoBKxPt-mz-e"
      },
      "source": [
        "# Funciones auxiliares:\n",
        "def oracion_mas_probable(first,sentA,sentB):\n",
        "  encodingA = tokenizer(first, sentA, return_tensors='pt')\n",
        "  encodingB = tokenizer(first, sentB, return_tensors='pt')\n",
        "  outputA = model(**encodingA, labels=torch.LongTensor([1]))\n",
        "  outputB = model(**encodingB, labels=torch.LongTensor([1]))\n",
        "  logitsA = outputA.logits\n",
        "  logitsB = outputB.logits\n",
        "  #Nota logits[0,0] entrega el score que la oracion si sea la siguiente (que tan True)\n",
        "  #logits[0,1] entrega el score de que la oracion no sea la siguiente (que tan False)\n",
        "  # Se puede aplicar una SoftMax sobre estos resultados para que sean probabilidades\n",
        "  # Pero no es necesario.\n",
        "  if logitsA[0, 0] > logitsB[0, 0] and logitsA[0, 1] < logitsB[0, 1]:\n",
        "    print(\"La oraciÃ³n mÃ¡s probable es A\")\n",
        "  elif logitsB[0, 0] > logitsA[0, 0] and logitsB[0, 1] < logitsA[0, 1]:\n",
        "    print(\"La oraciÃ³n mÃ¡s probable es B\")\n",
        "  else:\n",
        "    print(\"No estÃ¡ claro cual es mÃ¡s probable\")"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goXIGaief8Bi"
      },
      "source": [
        "1.1)\n",
        "Initial: \"My cat is fluffy.\"\\\n",
        "A: \"My dog has a curling tail.\"\\\n",
        "B: \"A song can make or ruin a personâ€™s day if they let it get to them.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zea5pybzf9x1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c365e02-f18f-4668-fd9e-7eba2ab3da51"
      },
      "source": [
        "initial = \"My cat is fluffy.\"\n",
        "sentenceA = \"My dog has a curly tail.\"\n",
        "sentenceB = \"A song can make or ruin a personâ€™s day if they let it get to them.\"\n",
        "oracion_mas_probable(initial,sentenceA,sentenceB)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La oraciÃ³n mÃ¡s probable es A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHd7UzXpgCY-"
      },
      "source": [
        "1.2)\n",
        "Initial: \"The Big Apple is famous worldwide.\"\\\n",
        "A: \"You can add cinnamon for the perfect combination.\"\\\n",
        "B: \"It is America's largest city.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7XbCBDmgCJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d22751-1372-4fe6-8e8b-aafe0b7cf18b"
      },
      "source": [
        "initial = \"The Big Apple is famous worldwide.\"\n",
        "sentenceA = \"You can add cinnamon for the perfect combination.\"\n",
        "sentenceB = \"It is America's largest city.\"\n",
        "oracion_mas_probable(initial,sentenceA,sentenceB)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La oraciÃ³n mÃ¡s probable es B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1Y7oxM7d0jM"
      },
      "source": [
        "1.3)\n",
        "Initial: \"Roses are red.\"\\\n",
        "A: \"Violets are blue.\"\\\n",
        "B: \"Fertilize them regularly for impressive flowers.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6t1QxlOf-O7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a431a5-c04e-415e-d55d-815502a6bb22"
      },
      "source": [
        "initial = \"Roses are red.\"\n",
        "sentenceA = \"Violets are blue.\"\n",
        "sentenceB = \"Fertilize them regularly for impressive flowers.\"\n",
        "oracion_mas_probable(initial,sentenceA,sentenceB)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La oraciÃ³n mÃ¡s probable es A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgC5BcoDiIWV"
      },
      "source": [
        "1.4)\n",
        "Initial: \"I play videogames the whole day.\"\\\n",
        "A: \"They make me happy.\"\\\n",
        "B: \"They make me rage.\"\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJK7PhmwiItA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "065139a4-e77b-4094-e5b1-7c69248489b0"
      },
      "source": [
        "initial = \"I play videogames the whole day.\"\n",
        "sentenceA = \"They make me happy.\"\n",
        "sentenceB = \"They make me rage.\"\n",
        "oracion_mas_probable(initial,sentenceA,sentenceB)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La oraciÃ³n mÃ¡s probable es A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ArRo_zMdtr"
      },
      "source": [
        "#### **BertForMaskedLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCN8XM7qihx1"
      },
      "source": [
        "**Pregunta 2 (1 pto en total):**  Ahora utilizaremos BertForMaskedLM para **predecir una palabra oculta** en una oraciÃ³n. **(0.25 puntos cada una)**\\\n",
        "Por ejemplo:\\\n",
        "BERT input: \"I want to _ a new car.\"\\\n",
        "BERT prediction: \"buy\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZwFxbJOv_aW"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSGkdSokv-3G"
      },
      "source": [
        "# Funcion auxiliar\n",
        "def palabra_mas_probable(sentence):\n",
        "  tokenized_text = tokenizer.tokenize(sentence)\n",
        "  masked_index = tokenized_text.index('[MASK]')\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_ids = [0] * len(tokenized_text)\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "  predictions = model(tokens_tensor, segments_tensors)\n",
        "  predicted_index = torch.argmax(predictions[0][0][masked_index]).item()\n",
        "  predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "  print(predicted_token)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gua8HPhfkYOb"
      },
      "source": [
        "2.1)\n",
        "BERT input: \"[CLS] I love [MASK] . [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRFJnURk7ut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e07c0d9-4be1-448e-ffb3-414aa3fc2b03"
      },
      "source": [
        "sent = \"[CLS] I love [MASK] . [SEP]\"\n",
        "palabra_mas_probable(sent)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7npsRd9Nk8hi"
      },
      "source": [
        "2.2)\n",
        "BERT input: \"[CLS] I hear that Karen is very [MASK] . [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW7CosmKk87e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b777b887-fb02-4523-fbfc-91c627f216ad"
      },
      "source": [
        "sent = \"[CLS] I heard that Karen is very [MASK] . [SEP]\"\n",
        "palabra_mas_probable(sent)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "upset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SqzWh8Wk9TD"
      },
      "source": [
        "2.3)\n",
        "BERT input: \"[CLS] She had the gift of being able to [MASK] . [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6YFd1Xpk9qd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41eade35-52b7-4394-da53-b7b2d1d2a244"
      },
      "source": [
        "sent = \"[CLS] She had the gift of being able to [MASK] . [SEP]\"\n",
        "palabra_mas_probable(sent)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoV_5suNk-1X"
      },
      "source": [
        "2.4)\n",
        "BERT input: \"[CLS] It's not often you find a [MASK] on the street. [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8ayI5VIk_Hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9387e3e4-23fe-4bf3-bb41-1f817acd9ee9"
      },
      "source": [
        "sent = \"[CLS] It's not often you find an [MASK] on the circus . [SEP]\"\n",
        "palabra_mas_probable(sent)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "expert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJFPpgoYMim5"
      },
      "source": [
        "#### **BertForQuestionAnswering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmCSmftALYbA"
      },
      "source": [
        "**Pregunta 3 (1 pto):**  Utilizando el modelo BertForQuestionAnswering pre-entrenado con 'bert-large-uncased-whole-word-masking-finetuned-squad', **extraiga la respuesta** a cada una de las siguientes 4 preguntas y su contexto. **(0.25 puntos cada una)**. Recuerde cambiar el tokenizer para que coincida con el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UhKkKpyToFf"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFejTyj98XDC"
      },
      "source": [
        "def entregar_respuesta(qst, cntxt):\n",
        "  inputs = tokenizer(qst, cntxt, return_tensors='pt')\n",
        "  start_positions = torch.tensor([1])\n",
        "  end_positions = torch.tensor([3])\n",
        "\n",
        "  outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
        "  start_scores = outputs.start_logits\n",
        "  end_scores = outputs.end_logits\n",
        "  all_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].tolist())\n",
        "  answer = all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]\n",
        "\n",
        "  print(' '.join(answer))"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7zizotdKVIF"
      },
      "source": [
        "3.1)\n",
        "\n",
        "Pregunta: \"When was the Battle of Iquique?\"\n",
        "\n",
        "Contexto: \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_TrDijrKnQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c684489-b02d-4b02-95a3-d81d267402d1"
      },
      "source": [
        "q = \"When was the Battle of Iquique?\"\n",
        "c = \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\"\n",
        "entregar_respuesta(q, c)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21 may 1879\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0au9XCqNB2TY"
      },
      "source": [
        "3.2)\n",
        "\n",
        "Pregunta: \"Who won the Battle of Iquique?\"\n",
        "\n",
        "Contexto: \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DlTUMxAB_0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b618a1c2-6d5d-40ac-b942-590b32c7366b"
      },
      "source": [
        "q = \"Who won the Battle of Iquique?\"\n",
        "c = \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\"\n",
        "entregar_respuesta(q, c)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "peruvian\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryRajxniCKbp"
      },
      "source": [
        "3.3)\n",
        "\n",
        "Pregunta: \"Who introduced peephole connections to LSTM networks?\"\n",
        "Contexto: \"What Iâ€™ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but itâ€™s worth mentioning some of them. One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding â€œpeephole connections.â€ This means that we let the gate layers look at the cell state.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1rT9kgLCKuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d08c2b-64b6-4751-d53f-39977e539841"
      },
      "source": [
        "q = \"Who introduced peephole connections to LSTM networks?\"\n",
        "c = \"What Iâ€™ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but itâ€™s worth mentioning some of them. One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding â€œpeephole connections.â€ This means that we let the gate layers look at the cell state.\"\n",
        "entregar_respuesta(q, c)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ge ##rs & sc ##hmi ##dh ##uber\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqn0vNvKCLAq"
      },
      "source": [
        "3.4)\n",
        "\n",
        "Pregunta: \"When is the cat most active?\"\n",
        "\n",
        "Contexto: \"The cat is similar in anatomy to the other felid species: it has a strong flexible body, quick reflexes, sharp teeth and retractable claws adapted to killing small prey. Its night vision and sense of smell are well developed. Cat communication includes vocalizations like meowing, purring, trilling, hissing, growling and grunting as well as cat-specific body language. It is a solitary hunter but a social species. It can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small mammals. It is a predator that is most active at dawn and dusk. It secretes and perceives pheromones.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5n37FwOCLTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62137bda-b9f1-4e34-befd-11bbd1ca6f02"
      },
      "source": [
        "q = \"When is the cat most active?\"\n",
        "c = \"The cat is similar in anatomy to the other felid species: it has a strong flexible body, quick reflexes, sharp teeth and retractable claws adapted to killing small prey. Its night vision and sense of smell are well developed. Cat communication includes vocalizations like meowing, purring, trilling, hissing, growling and grunting as well as cat-specific body language. It is a solitary hunter but a social species. It can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small mammals. It is a predator that is most active at dawn and dusk. It secretes and perceives pheromones.\"\n",
        "entregar_respuesta(q, c)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dawn and dusk\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}